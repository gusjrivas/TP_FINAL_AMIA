{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio teórico\n",
    "\n",
    "Sea una red neuronal de dos capas, la primera de 3 neuronas y la segunda de 1 con los parámetros inicializados con los siguientes valores:\n",
    "$$\n",
    "w^{(1)} =\n",
    "\\begin{pmatrix}\n",
    "0.1 & -0.5 \\\\\n",
    "-0.3 & -0.9 \\\\\n",
    "0.8 & 0.02\n",
    "\\end{pmatrix},\n",
    "b^{(1)} = \\begin{pmatrix}\n",
    "0.1 \\\\\n",
    "0.5 \\\\\n",
    "0.8\n",
    "\\end{pmatrix},\n",
    "w^{(2)} =\n",
    "\\begin{pmatrix}\n",
    "-0.4 & 0.2 & -0.5\n",
    "\\end{pmatrix},\n",
    "b^{(2)} = 0.7\n",
    "$$\n",
    "\n",
    "y donde cada capa calcula su salida vía\n",
    "\n",
    "$$\n",
    "y^{(i)} = \\sigma (w^{(i)} \\cdot x^{(i)}+b^{(i)})\n",
    "$$\n",
    "\n",
    "donde $\\sigma (z) = \\frac{1}{1+e^{-z}}$ es la función sigmoidea .\n",
    "\n",
    "\\\\\n",
    "Dada la observación $x=\\begin{pmatrix}\n",
    "1.8 \\\\\n",
    "-3.4\n",
    "\\end{pmatrix}$, $y=5$ y la función de costo $J(\\theta)=\\frac{1}{2}(\\hat{y}_\\theta-y)^2$, calcular las derivadas de J respecto de cada parámetro $w^{(1)}$, $w^{(2)}$, $b^{(1)}$, $b^{(2)}$.\n",
    "\n",
    "*Nota: Con una sigmoidea a la salida jamás va a poder estimar el 5 \"pedido\", pero eso no afecta al mecanismo de backpropagation!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Propagación hacia adelante (Forward Propagation)\n",
    "\n",
    "Primero, calculamos las salidas de la red neuronal para la observación dada \\( x \\):\n",
    "\n",
    "$$ \n",
    "z^{(1)} = w^{(1)} \\cdot x + b^{(1)} \n",
    "$$\n",
    "\n",
    "$$ \n",
    "= \\begin{pmatrix} \n",
    "0.1 & -0.5 \\\\ \n",
    "-0.3 & -0.9 \\\\ \n",
    "0.8 & 0.02 \n",
    "\\end{pmatrix} \\cdot \\begin{pmatrix} \n",
    "1.8 \\\\ \n",
    "-3.4 \n",
    "\\end{pmatrix} + \\begin{pmatrix} \n",
    "0.1 \\\\ \n",
    "0.5 \\\\ \n",
    "0.8 \n",
    "\\end{pmatrix} \n",
    "$$\n",
    "\n",
    "$$ \n",
    "= \\begin{pmatrix} \n",
    "(0.1 \\times 1.8) + (-0.5 \\times -3.4) \\\\ \n",
    "(-0.3 \\times 1.8) + (-0.9 \\times -3.4) \\\\ \n",
    "(0.8 \\times 1.8) + (0.02 \\times -3.4) \n",
    "\\end{pmatrix} + \\begin{pmatrix} \n",
    "0.1 \\\\ \n",
    "0.5 \\\\ \n",
    "0.8 \n",
    "\\end{pmatrix} \n",
    "$$\n",
    "\n",
    "$$ \n",
    "= \\begin{pmatrix} \n",
    "1.88 \\\\ \n",
    "2.52 \\\\ \n",
    "1.372 \n",
    "\\end{pmatrix} + \\begin{pmatrix} \n",
    "0.1 \\\\ \n",
    "0.5 \\\\ \n",
    "0.8 \n",
    "\\end{pmatrix} \n",
    "$$\n",
    "\n",
    "$$ \n",
    "= \\begin{pmatrix} \n",
    "1.98 \\\\ \n",
    "3.02 \\\\ \n",
    "2.172 \n",
    "\\end{pmatrix} \n",
    "$$\n",
    "\n",
    "Aplicamos la función sigmoidea $ \\sigma(z) = \\frac{1}{1 + e^{-z}} $ a cada elemento de $z^{(1)} $:\n",
    "\n",
    "$$ \n",
    "y^{(1)} = \\sigma(z^{(1)}) = \\begin{pmatrix} \n",
    "\\sigma(1.98) \\\\ \n",
    "\\sigma(3.02) \\\\ \n",
    "\\sigma(2.172) \n",
    "\\end{pmatrix} \n",
    "$$\n",
    "\n",
    "$$ \n",
    "\\sigma(1.98) = \\frac{1}{1 + e^{-1.98}} \\approx \\frac{1}{1 + 0.137} \\approx 0.8797 \n",
    "$$\n",
    "\n",
    "$$ \n",
    "\\sigma(3.02) = \\frac{1}{1 + e^{-3.02}} \\approx \\frac{1}{1 + 0.049} \\approx 0.9534 \n",
    "$$\n",
    "\n",
    "$$ \n",
    "\\sigma(2.172) = \\frac{1}{1 + e^{-2.172}} \\approx \\frac{1}{1 + 0.114} \\approx 0.8977 \n",
    "$$\n",
    "\n",
    "$$ \n",
    "y^{(1)} \\approx \\begin{pmatrix} \n",
    "0.8797 \\\\ \n",
    "0.9534 \\\\ \n",
    "0.8977 \n",
    "\\end{pmatrix} \n",
    "$$\n",
    "\n",
    "Para la capa de salida:\n",
    "\n",
    "$$ \n",
    "z^{(2)} = w^{(2)} \\cdot y^{(1)} + b^{(2)} \n",
    "$$\n",
    "\n",
    "$$ \n",
    "= \\begin{pmatrix} \n",
    "-0.4 & 0.2 & -0.5 \n",
    "\\end{pmatrix} \\cdot \\begin{pmatrix} \n",
    "0.8797 \\\\ \n",
    "0.9534 \\\\ \n",
    "0.8977 \n",
    "\\end{pmatrix} + 0.7 \n",
    "$$\n",
    "\n",
    "$$ \n",
    "w^{(2)} \\cdot y^{(1)} = (-0.4 \\times 0.8797) + (0.2 \\times 0.9534) + (-0.5 \\times 0.8977) = -0.35188 + 0.19068 - 0.44885 = -0.61005 \n",
    "$$\n",
    "\n",
    "$$ \n",
    "z^{(2)} = -0.61005 + 0.7 = 0.08995 \n",
    "$$\n",
    "\n",
    "Aplicamos la función sigmoidea:\n",
    "\n",
    "$$ \n",
    "\\hat{y} = \\sigma(z^{(2)}) = \\sigma(0.08995) \n",
    "$$\n",
    "\n",
    "$$ \n",
    "\\sigma(0.08995) = \\frac{1}{1 + e^{-0.08995}} \\approx \\frac{1}{1 + 0.914} \\approx 0.5225 \n",
    "$$\n",
    "\n",
    "## Cálculo del error (Error Calculation)\n",
    "\n",
    "Utilizando la observación \\( x \\) y el valor objetivo \\( y = 5 \\), calculamos el error:\n",
    "\n",
    "$$ \n",
    "J(\\theta) = \\frac{1}{2} (\\hat{y} - y)^2 \n",
    "$$\n",
    "\n",
    "$$ \n",
    "J(\\theta) = \\frac{1}{2} (0.5225 - 5)^2 = \\frac{1}{2} (-4.4775)^2 = \\frac{1}{2} \\times 20.0431 = 10.02155 \n",
    "$$\n",
    "\n",
    "## Retropropagación (Backpropagation)\n",
    "\n",
    "Para la capa de salida:\n",
    "\n",
    "$$ \n",
    "\\delta^{(2)} = \\frac{\\partial J}{\\partial z^{(2)}} = (\\hat{y} - y) \\cdot \\sigma'(z^{(2)}) \n",
    "$$\n",
    "\n",
    "$$ \n",
    "\\sigma'(z) = \\sigma(z) \\cdot (1 - \\sigma(z)) \n",
    "$$\n",
    "\n",
    "$$ \n",
    "\\sigma'(0.08995) = 0.5225 \\cdot (1 - 0.5225) \\approx 0.2495 \n",
    "$$\n",
    "\n",
    "$$ \n",
    "\\delta^{(2)} = (0.5225 - 5) \\cdot 0.2495 \\approx -4.4775 \\cdot 0.2495 \\approx -1.1174 \n",
    "$$\n",
    "\n",
    "Gradientes para $w^{(2)}$ y $b^{(2)}$:\n",
    "\n",
    "$$ \n",
    "\\frac{\\partial J}{\\partial w^{(2)}} = \\delta^{(2)} \\cdot (y^{(1)})^T \n",
    "$$\n",
    "\n",
    "$$ \n",
    "\\frac{\\partial J}{\\partial w^{(2)}} = -1.1174 \\cdot \\begin{pmatrix} \n",
    "0.8797 \\\\ \n",
    "0.9534 \\\\ \n",
    "0.8977 \n",
    "\\end{pmatrix} \n",
    "$$\n",
    "\n",
    "$$ \n",
    "= \\begin{pmatrix} \n",
    "-1.1174 \\times 0.8797 \\\\ \n",
    "-1.1174 \\times 0.9534 \\\\ \n",
    "-1.1174 \\times 0.8977 \n",
    "\\end{pmatrix} \n",
    "$$\n",
    "\n",
    "$$ \n",
    "= \\begin{pmatrix} \n",
    "-0.9834 \\\\ \n",
    "-1.0644 \\\\ \n",
    "-1.0024 \n",
    "\\end{pmatrix} \n",
    "$$\n",
    "\n",
    "$$ \n",
    "\\frac{\\partial J}{\\partial b^{(2)}} = \\delta^{(2)} = -1.1174 \n",
    "$$\n",
    "\n",
    "Para la capa oculta:\n",
    "\n",
    "$$ \n",
    "\\delta^{(1)} = (w^{(2)})^T \\cdot \\delta^{(2)} \\cdot \\sigma'(z^{(1)}) \n",
    "$$\n",
    "\n",
    "$$ \n",
    "\\sigma'(z^{(1)}) = y^{(1)} \\cdot (1 - y^{(1)}) \n",
    "$$\n",
    "\n",
    "$$ \n",
    "\\sigma'(1.98) = 0.8797 \\cdot (1 - 0.8797) \\approx 0.1060 \n",
    "$$\n",
    "\n",
    "$$ \n",
    "\\sigma'(3.02) = 0.9534 \\cdot (1 - 0.9534) \\approx 0.0445 \n",
    "$$\n",
    "\n",
    "$$ \n",
    "\\sigma'(2.172) = 0.8977 \\cdot (1 - 0.8977) \\approx 0.0921 \n",
    "$$\n",
    "\n",
    "$$ \n",
    "\\sigma'(z^{(1)}) = \\begin{pmatrix} \n",
    "0.1060 \\\\ \n",
    "0.0445 \\\\ \n",
    "0.0921 \n",
    "\\end{pmatrix} \n",
    "$$\n",
    "\n",
    "$$ \n",
    "(w^{(2)})^T \\cdot \\delta^{(2)} = \\begin{pmatrix} \n",
    "-0.4 \\\\ \n",
    "0.2 \\\\ \n",
    "-0.5 \n",
    "\\end{pmatrix} \\cdot -1.1174 \n",
    "$$\n",
    "\n",
    "$$ \n",
    "= \\begin{pmatrix} \n",
    "0.44696 \\\\ \n",
    "-0.22348 \\\\ \n",
    "0.5587 \n",
    "\\end{pmatrix} \n",
    "$$\n",
    "\n",
    "$$ \n",
    "\\delta^{(1)} = \\begin{pmatrix} \n",
    "0.44696 \\\\ \n",
    "-0.22348 \\\\ \n",
    "0.5587 \n",
    "\\end{pmatrix} \\cdot \\begin{pmatrix} \n",
    "0.1060 \\\\ \n",
    "0.0445 \\\\ \n",
    "0.0921 \n",
    "\\end{pmatrix} \n",
    "$$\n",
    "\n",
    "$$ \n",
    "= \\begin{pmatrix} \n",
    "0.44696 \\times 0.1060 \\\\ \n",
    "-0.22348 \\times 0.0445 \\\\ \n",
    "0.5587 \\times 0.0921 \n",
    "\\end{pmatrix} \n",
    "$$\n",
    "\n",
    "$$ \n",
    "= \\begin{pmatrix} \n",
    "0.0474 \\\\ \n",
    "-0.0099 \\\\ \n",
    "0.0514 \n",
    "\\end{pmatrix} \n",
    "$$\n",
    "\n",
    "Gradientes para $w^{(1)}$ y $b^{(1)}$:\n",
    "\n",
    "$$ \n",
    "\\frac{\\partial J}{\\partial w^{(1)}} = \\delta^{(1)} \\cdot x^T \n",
    "$$\n",
    "\n",
    "$$ \n",
    "\\frac{\\partial J}{\\partial w^{(1)}} = \\begin{pmatrix} \n",
    "0.0474 \\\\ \n",
    "-0.0099 \\\\ \n",
    "0.0514 \n",
    "\\end{pmatrix} \\cdot \\begin{pmatrix} \n",
    "1.8 & -3.4 \n",
    "\\end{pmatrix} \n",
    "$$\n",
    "\n",
    "$$ \n",
    "= \\begin{pmatrix} \n",
    "0.0474 \\times 1.8 & 0.0474 \\times -3.4 \\\\ \n",
    "-0.0099 \\times 1.8 & -0.0099 \\times -3.4 \\\\ \n",
    "0.0514 \\times 1.8 & 0.0514 \\times -3.4 \n",
    "\\end{pmatrix} \n",
    "$$\n",
    "\n",
    "$$ \n",
    "= \\begin{pmatrix} \n",
    "0.0853 & -0.1611 \\\\ \n",
    "-0.0178 & 0.0337 \\\\ \n",
    "0.0925 & -0.1748 \n",
    "\\end{pmatrix} \n",
    "$$\n",
    "\n",
    "$$ \n",
    "\\frac{\\partial J}{\\partial b^{(1)}} = \\delta^{(1)} \n",
    "$$\n",
    "\n",
    "$$ \n",
    "= \\begin{pmatrix} \n",
    "0.0474 \\\\ \n",
    "-0.0099 \\\\ \n",
    "0.0514 \n",
    "\\end{pmatrix} \n",
    "$$\n",
    "\n",
    "## Nota Final\n",
    "\n",
    "\n",
    "\n",
    "La función de activación sigmoidea, definida como $\\sigma(z) = \\frac{1}{1 + e^{-z}}$, mapea cualquier valor de entrada \\( z \\) a un rango entre 0 y 1. Esto significa que la salida de una neurona con activación sigmoidea siempre estará en ese rango, y nunca alcanzará valores extremos como 5, en el caso de este ejercicio.\n",
    "\n",
    "Sin embargo, esto no impide que la red neuronal aprenda y ajuste sus pesos utilizando el algoritmo de retropropagación (backpropagation). Aunque la salida de la red esté limitada por la función sigmoidea, el mecanismo de retropropagación sigue funcionando correctamente para ajustar los pesos de las neuronas en las capas ocultas y en la capa de salida en función del error calculado.\n",
    "\n",
    "En resumen, aunque la función sigmoidea puede limitar la capacidad de la red para producir ciertos valores específicos en la salida, esto no afecta la capacidad de la red para aprender de los datos y ajustar sus pesos internos mediante el proceso de retropropagación. La red neuronal puede aprender a aproximar la salida deseada dentro de los límites de su función de activación.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CEIA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
