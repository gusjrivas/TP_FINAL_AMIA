{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio teórico\n",
    "\n",
    "Sea una red neuronal de dos capas, la primera de 3 neuronas y la segunda de 1 con los parámetros inicializados con los siguientes valores:\n",
    "$$\n",
    "w^{(1)} =\n",
    "\\begin{pmatrix}\n",
    "0.1 & -0.5 \\\\\n",
    "-0.3 & -0.9 \\\\\n",
    "0.8 & 0.02\n",
    "\\end{pmatrix},\n",
    "b^{(1)} = \\begin{pmatrix}\n",
    "0.1 \\\\\n",
    "0.5 \\\\\n",
    "0.8\n",
    "\\end{pmatrix},\n",
    "w^{(2)} =\n",
    "\\begin{pmatrix}\n",
    "-0.4 & 0.2 & -0.5\n",
    "\\end{pmatrix},\n",
    "b^{(2)} = 0.7\n",
    "$$\n",
    "\n",
    "y donde cada capa calcula su salida vía\n",
    "\n",
    "$$\n",
    "y^{(i)} = \\sigma (w^{(i)} \\cdot x^{(i)}+b^{(i)})\n",
    "$$\n",
    "\n",
    "donde $\\sigma (z) = \\frac{1}{1+e^{-z}}$ es la función sigmoidea .\n",
    "\n",
    "\\\\\n",
    "Dada la observación $x=\\begin{pmatrix}\n",
    "1.8 \\\\\n",
    "-3.4\n",
    "\\end{pmatrix}$, $y=5$ y la función de costo $J(\\theta)=\\frac{1}{2}(\\hat{y}_\\theta-y)^2$, calcular las derivadas de J respecto de cada parámetro $w^{(1)}$, $w^{(2)}$, $b^{(1)}$, $b^{(2)}$.\n",
    "\n",
    "*Nota: Con una sigmoidea a la salida jamás va a poder estimar el 5 \"pedido\", pero eso no afecta al mecanismo de backpropagation!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Respuesta:\n",
    "\n",
    "Una representación de la red neuronal del ejercicio\n",
    "\n",
    "<img src=\"Red_neuronal_ejercicio.png\" alt=\"Red neuronal\" width=\"600\" height=\"400\">\n",
    "\n",
    "\n",
    "### Forward Pass\n",
    "\n",
    "**Calcular la salida de la red neuronal $ \\hat{y} $:**\n",
    "\n",
    "1. **Cálculo de $ z^{(1)} $:**\n",
    "\n",
    "   $$ z^{(1)} = w^{(1)} \\cdot x + b^{(1)} $$\n",
    "\n",
    "   Donde,\n",
    "\n",
    "   $$\n",
    "   w^{(1)} = \\begin{pmatrix}\n",
    "   0.1 & -0.5 \\\\\n",
    "   -0.3 & -0.9 \\\\\n",
    "   0.8 & 0.02\n",
    "   \\end{pmatrix}, \\quad\n",
    "   b^{(1)} = \\begin{pmatrix}\n",
    "   0.1 \\\\\n",
    "   0.5 \\\\\n",
    "   0.8\n",
    "   \\end{pmatrix}, \\quad\n",
    "   x = \\begin{pmatrix}\n",
    "   1.8 \\\\\n",
    "   -3.4\n",
    "   \\end{pmatrix}\n",
    "   $$\n",
    "\n",
    "   Entonces,\n",
    "\n",
    "   $$\n",
    "   z^{(1)} = \\begin{pmatrix}\n",
    "   0.1 & -0.5 \\\\\n",
    "   -0.3 & -0.9 \\\\\n",
    "   0.8 & 0.02\n",
    "   \\end{pmatrix} \\cdot \\begin{pmatrix}\n",
    "   1.8 \\\\\n",
    "   -3.4\n",
    "   \\end{pmatrix} + \\begin{pmatrix}\n",
    "   0.1 \\\\\n",
    "   0.5 \\\\\n",
    "   0.8\n",
    "   \\end{pmatrix}\n",
    "   $$\n",
    "\n",
    "   Resolviendo,\n",
    "\n",
    "   $$\n",
    "   z^{(1)} = \\begin{pmatrix}\n",
    "   0.1 \\cdot 1.8 + (-0.5) \\cdot (-3.4) \\\\\n",
    "   -0.3 \\cdot 1.8 + (-0.9) \\cdot (-3.4) \\\\\n",
    "   0.8 \\cdot 1.8 + 0.02 \\cdot (-3.4)\n",
    "   \\end{pmatrix} + \\begin{pmatrix}\n",
    "   0.1 \\\\\n",
    "   0.5 \\\\\n",
    "   0.8\n",
    "   \\end{pmatrix}\n",
    "   $$\n",
    "\n",
    "   $$\n",
    "   = \\begin{pmatrix}\n",
    "   0.18 + 1.7 \\\\\n",
    "   -0.54 + 3.06 \\\\\n",
    "   1.44 - 0.068\n",
    "   \\end{pmatrix} + \\begin{pmatrix}\n",
    "   0.1 \\\\\n",
    "   0.5 \\\\\n",
    "   0.8\n",
    "   \\end{pmatrix}\n",
    "   $$\n",
    "\n",
    "   $$\n",
    "   = \\begin{pmatrix}\n",
    "   1.98 \\\\\n",
    "   3.02 \\\\\n",
    "   2.172\n",
    "   \\end{pmatrix}\n",
    "   $$\n",
    "\n",
    "2. **Cálculo de $y^{(1)}$ aplicando la función sigmoidea:**\n",
    "\n",
    "   $$ y^{(1)} = \\sigma(z^{(1)}) = \\begin{pmatrix} \\sigma(1.98) \\\\ \\sigma(3.02) \\\\ \\sigma(2.172) \\end{pmatrix} $$\n",
    "\n",
    "   Donde,\n",
    "\n",
    "   $$ \\sigma(z) = \\frac{1}{1 + e^{-z}} $$\n",
    "\n",
    "   Entonces,\n",
    "\n",
    "   $$\n",
    "   \\sigma(1.98) \\approx \\frac{1}{1 + e^{-1.98}} \\approx 0.8797\n",
    "   $$\n",
    "\n",
    "   $$\n",
    "   \\sigma(3.02) \\approx \\frac{1}{1 + e^{-3.02}} \\approx 0.9534\n",
    "   $$\n",
    "\n",
    "   $$\n",
    "   \\sigma(2.172) \\approx \\frac{1}{1 + e^{-2.172}} \\approx 0.8977\n",
    "   $$\n",
    "\n",
    "   Por lo tanto,\n",
    "\n",
    "   $$\n",
    "   y^{(1)} \\approx \\begin{pmatrix} 0.8797 \\\\ 0.9534 \\\\ 0.8977 \\end{pmatrix}\n",
    "   $$\n",
    "\n",
    "3. **Cálculo de $ z^{(2)} $:**\n",
    "\n",
    "   $$ z^{(2)} = w^{(2)} \\cdot y^{(1)} + b^{(2)} $$\n",
    "\n",
    "   Donde,\n",
    "\n",
    "   $$\n",
    "   w^{(2)} = \\begin{pmatrix} -0.4 & 0.2 & -0.5 \\end{pmatrix}, \\quad\n",
    "   b^{(2)} = 0.7\n",
    "   $$\n",
    "\n",
    "   Entonces,\n",
    "\n",
    "   $$\n",
    "   z^{(2)} = \\begin{pmatrix} -0.4 & 0.2 & -0.5 \\end{pmatrix} \\cdot \\begin{pmatrix} 0.8797 \\\\ 0.9534 \\\\ 0.8977 \\end{pmatrix} + 0.7\n",
    "   $$\n",
    "\n",
    "   Resolviendo,\n",
    "\n",
    "   $$\n",
    "   z^{(2)} = -0.4 \\cdot 0.8797 + 0.2 \\cdot 0.9534 - 0.5 \\cdot 0.8977 + 0.7\n",
    "   $$\n",
    "\n",
    "   $$\n",
    "   = -0.35188 + 0.19068 - 0.44885 + 0.7\n",
    "   $$\n",
    "\n",
    "   $$\n",
    "   \\approx 0.08995\n",
    "   $$\n",
    "\n",
    "4. **Cálculo de $ \\hat{y} $ aplicando la función sigmoidea:**\n",
    "\n",
    "   $$ \\hat{y} = \\sigma(z^{(2)}) = \\sigma(0.08995) \\approx \\frac{1}{1 + e^{-0.08995}} \\approx 0.5225 $$\n",
    "\n",
    "### Backward Pass\n",
    "\n",
    "### Calcular la derivada del costo respecto a la salida:\n",
    "\n",
    "**Derivada de la función de costo respecto a $ \\hat{y} $:**\n",
    "\n",
    "$$ \\frac{\\partial J}{\\partial \\hat{y}} = \\hat{y} - y = 0.5225 - 5 = -4.4775 $$\n",
    "\n",
    "### Calcular las derivadas parciales para la capa de salida:\n",
    "\n",
    "**Derivada de $ \\hat{y} $ respecto a $ z^{(2)} $ (regla de la cadena):**\n",
    "\n",
    "$$ \\frac{\\partial \\hat{y}}{\\partial z^{(2)}} = \\sigma'(z^{(2)}) = \\sigma(z^{(2)})(1 - \\sigma(z^{(2)})) \\approx 0.5225 \\cdot (1 - 0.5225) \\approx 0.2495 $$\n",
    "\n",
    "**Combinar las derivadas usando la regla de la cadena para obtener $ \\frac{\\partial J}{\\partial z^{(2)}} $:**\n",
    "\n",
    "$$ \\frac{\\partial J}{\\partial z^{(2)}} = \\frac{\\partial J}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial z^{(2)}} = -4.4775 \\cdot 0.2495 \\approx -1.1174 $$\n",
    "\n",
    "### Calcular los gradientes para $ w^{(2)} $ y $ b^{(2)} $:\n",
    "\n",
    "**Derivada de $ z^{(2)} $ respecto a $ w^{(2)} $:**\n",
    "\n",
    "$$ \\frac{\\partial z^{(2)}}{\\partial w^{(2)}} = y^{(1)} = \\begin{pmatrix} 0.8797 \\\\ 0.9534 \\\\ 0.8977 \\end{pmatrix} $$\n",
    "\n",
    "**Combinar las derivadas usando la regla de la cadena para obtener $ \\frac{\\partial J}{\\partial w^{(2)}} $:**\n",
    "\n",
    "$$ \\frac{\\partial J}{\\partial w^{(2)}} = \\frac{\\partial J}{\\partial z^{(2)}} \\cdot \\frac{\\partial z^{(2)}}{\\partial w^{(2)}} = -1.1174 \\cdot \\begin{pmatrix} 0.8797 \\\\ 0.9534 \\\\ 0.8977 \\end{pmatrix} = \\begin{pmatrix} -0.9834 \\\\ -1.0644 \\\\ -1.0024 \\end{pmatrix} $$\n",
    "\n",
    "**Derivada de $ z^{(2)} $ respecto a $ b^{(2)} $:**\n",
    "\n",
    "$$ \\frac{\\partial z^{(2)}}{\\partial b^{(2)}} = 1 $$\n",
    "\n",
    "**Combinar las derivadas usando la regla de la cadena para obtener $ \\frac{\\partial J}{\\partial b^{(2)}} $:**\n",
    "\n",
    "$$ \\frac{\\partial J}{\\partial b^{(2)}} = \\frac{\\partial J}{\\partial z^{(2)}} \\cdot \\frac{\\partial z^{(2)}}{\\partial b^{(2)}} = -1.1174 $$\n",
    "\n",
    "### Retropropagación a la capa oculta:\n",
    "\n",
    "**Derivada de $ z^{(2)} $ respecto a $ y^{(1)} $:**\n",
    "\n",
    "$$ \\frac{\\partial z^{(2)}}{\\partial y^{(1)}} = w^{(2)} = \\begin{pmatrix} -0.4 & 0.2 & -0.5 \\end{pmatrix} $$\n",
    "\n",
    "**Combinar las derivadas usando la regla de la cadena para obtener $ \\frac{\\partial J}{\\partial y^{(1)}} $:**\n",
    "\n",
    "$$ \\frac{\\partial J}{\\partial y^{(1)}} = \\frac{\\partial J}{\\partial z^{(2)}} \\cdot \\frac{\\partial z^{(2)}}{\\partial y^{(1)}} = -1.1174 \\cdot \\begin{pmatrix} -0.4 \\\\ 0.2 \\\\ -0.5 \\end{pmatrix} = \\begin{pmatrix} 0.44696 \\\\ -0.22348 \\\\ 0.5587 \\end{pmatrix} $$\n",
    "\n",
    "**Derivada de $ y^{(1)} $ respecto a $ z^{(1)} $ (regla de la cadena):**\n",
    "\n",
    "$$ \\frac{\\partial y^{(1)}}{\\partial z^{(1)}} = \\sigma'(z^{(1)}) = \\sigma(z^{(1)})(1 - \\sigma(z^{(1)})) $$\n",
    "\n",
    "Calculamos cada elemento:\n",
    "\n",
    "$$ \\sigma'(1.98) \\approx 0.8797 \\cdot (1 - 0.8797) \\approx 0.1059 $$\n",
    "\n",
    "$$ \\sigma'(3.02) \\approx 0.9534 \\cdot (1 - 0.9534) \\approx 0.0444 $$\n",
    "\n",
    "$$ \\sigma'(2.172) \\approx 0.8977 \\cdot (1 - 0.8977) \\approx 0.0917 $$\n",
    "\n",
    "Entonces,\n",
    "\n",
    "$$ \\frac{\\partial y^{(1)}}{\\partial z^{(1)}} \\approx \\begin{pmatrix} 0.1059 \\\\ 0.0444 \\\\ 0.0917 \\end{pmatrix} $$\n",
    "\n",
    "**Combinar las derivadas usando la regla de la cadena para obtener $ \\frac{\\partial J}{\\partial z^{(1)}} $:**\n",
    "\n",
    "$$ \\frac{\\partial J}{\\partial z^{(1)}} = \\frac{\\partial J}{\\partial y^{(1)}} \\cdot \\frac{\\partial y^{(1)}}{\\partial z^{(1)}} = \\begin{pmatrix} 0.44696 \\\\ -0.22348 \\\\ 0.5587 \\end{pmatrix} \\cdot \\begin{pmatrix} 0.1059 \\\\ 0.0444 \\\\ 0.0917 \\end{pmatrix} = \\begin{pmatrix} 0.0474 \\\\ -0.0099 \\\\ 0.0513 \\end{pmatrix} $$\n",
    "\n",
    "### Calcular los gradientes para $ w^{(1)} $ y $ b^{(1)} $:\n",
    "\n",
    "**Derivada de $ z^{(1)} $ respecto a $ w^{(1)} $:**\n",
    "\n",
    "$$ \\frac{\\partial z^{(1)}}{\\partial w^{(1)}} = x^T = \\begin{pmatrix} 1.8 & -3.4 \\end{pmatrix} $$\n",
    "\n",
    "**Combinar las derivadas usando la regla de la cadena para obtener $ \\frac{\\partial J}{\\partial w^{(1)}} $:**\n",
    "\n",
    "$$ \\frac{\\partial J}{\\partial w^{(1)}} = \\frac{\\partial J}{\\partial z^{(1)}} \\cdot \\frac{\\partial z^{(1)}}{\\partial w^{(1)}} = \\begin{pmatrix} 0.0474 \\\\ -0.0099 \\\\ 0.0513 \\end{pmatrix} \\cdot \\begin{pmatrix} 1.8 & -3.4 \\end{pmatrix} = \\begin{pmatrix} 0.0853 & -0.1611 \\\\ -0.0178 & 0.0337 \\\\ 0.0924 & -0.1744 \\end{pmatrix} $$\n",
    "\n",
    "**Derivada de $ z^{(1)} $ respecto a $ b^{(1)} $:**\n",
    "\n",
    "$$ \\frac{\\partial z^{(1)}}{\\partial b^{(1)}} = 1 $$\n",
    "\n",
    "**Combinar las derivadas usando la regla de la cadena para obtener $ \\frac{\\partial J}{\\partial b^{(1)}} $:**\n",
    "\n",
    "$$ \\frac{\\partial J}{\\partial b^{(1)}} = \\frac{\\partial J}{\\partial z^{(1)}} = \\begin{pmatrix} 0.0474 \\\\ -0.0099 \\\\ 0.0513 \\end{pmatrix} $$\n",
    "\n",
    "### Nota Final:\n",
    "\n",
    "\n",
    "\n",
    "La función de activación sigmoidea, definida como $ \\sigma(z) = \\frac{1}{1 + e^{-z}} $ , mapea cualquier valor de entrada $ z $ a un rango entre 0 y 1. Esto significa que la salida de una neurona con activación sigmoidea siempre estará en ese rango, y nunca alcanzará valores extremos como 5, en el caso de este ejercicio.\n",
    "\n",
    "Sin embargo, esto no impide que la red neuronal aprenda y ajuste sus pesos utilizando el algoritmo de retropropagación (backpropagation). Aunque la salida de la red esté limitada por la función sigmoidea, el mecanismo de retropropagación sigue funcionando correctamente para ajustar los pesos de las neuronas en las capas ocultas y en la capa de salida en función del error calculado.\n",
    "\n",
    "En resumen, aunque la función sigmoidea puede limitar la capacidad de la red para producir ciertos valores específicos en la salida, esto no afecta la capacidad de la red para aprender de los datos y ajustar sus pesos internos mediante el proceso de retropropagación. La red neuronal puede aprender a aproximar la salida deseada dentro de los límites de su función de activación.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
