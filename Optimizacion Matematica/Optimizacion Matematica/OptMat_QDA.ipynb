{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Optimización Matemática - QDA"
      ],
      "metadata": {
        "id": "bpJ7s_SIVu_I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Consigna\n",
        "\n",
        "**Sugerencia:** considerar combinaciones adecuadas de `transpose`, `reshape` y, ocasionalmente, `flatten`. Explorar la dimensionalidad de cada elemento antes de implementar las clases.\n",
        "\n",
        "Debido a la forma cuadrática de QDA, no se puede predecir para *n* observaciones en una sola pasada (utilizar $X \\in \\mathbb{R}^{p \\times n}$ en vez de $x \\in \\mathbb{R}^p$) sin pasar por una matriz de *n x n* en donde se computan todas las interacciones entre observaciones. Se puede acceder al resultado recuperando sólo la diagonal de dicha matriz, pero resulta ineficiente en tiempo y (especialmente) en memoria. Aún así, es *posible* que el modelo funcione más rápido.\n",
        "\n",
        "1. Implementar el modelo `FasterQDA` (se recomienda heredarlo de TensorizedQDA) de manera de eliminar el ciclo for en el método predict.\n",
        "2. Comparar los tiempos de predicción de `FasterQDA` con `TensorizedQDA` y `QDA`.\n",
        "3. Mostrar (puede ser con un print) dónde aparece la mencionada matriz de *n x n*, donde *n* es la cantidad de observaciones a predecir.\n",
        "4.Demostrar que\n",
        "$$\n",
        "diag(A \\cdot B) = \\sum_{cols} A \\odot B^T = np.sum(A \\odot B^T, axis=1)\n",
        "$$ es decir, que se puede \"esquivar\" la matriz de *n x n* usando matrices de *n x p*.\n",
        "5.Utilizar la propiedad antes demostrada para reimplementar la predicción del modelo `FasterQDA` de forma eficiente. ¿Hay cambios en los tiempos de predicción?"
      ],
      "metadata": {
        "id": "I8uFVziOqKzk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modelo"
      ],
      "metadata": {
        "id": "6yEV8WbiWl6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ],
      "metadata": {
        "id": "kgMoAXZ8pRTM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from numpy.linalg import det, inv"
      ],
      "metadata": {
        "id": "teF9O9JJmG7Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### ClassEncoder"
      ],
      "metadata": {
        "id": "7-OfcyzspPS4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ClassEncoder:\n",
        "  def fit(self, y):\n",
        "    self.names = np.unique(y)\n",
        "    self.name_to_class = {name:idx for idx, name in enumerate(self.names)}\n",
        "    self.fmt = y.dtype\n",
        "\n",
        "  def _map_reshape(self, f, arr):\n",
        "    return np.array([f(elem) for elem in arr.flatten()]).reshape(arr.shape)\n",
        "\n",
        "  def transform(self, y):\n",
        "    return self._map_reshape(lambda name: self.name_to_class[name], y)\n",
        "\n",
        "  def fit_transform(self, y):\n",
        "    self.fit(y)\n",
        "    return self.transform(y)\n",
        "\n",
        "  def detransform(self, y_hat):\n",
        "    return self._map_reshape(lambda idx: self.names[idx], y_hat)"
      ],
      "metadata": {
        "id": "sDBLvbTtlwzs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BaseBayesianClassifier"
      ],
      "metadata": {
        "id": "ub6huMv7pNNy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m0KYC8_uSOu4"
      },
      "outputs": [],
      "source": [
        "class BaseBayesianClassifier:\n",
        "  def __init__(self):\n",
        "    self.encoder = ClassEncoder()\n",
        "\n",
        "  def _estimate_a_priori(self, y):\n",
        "    a_priori = np.bincount(y.flatten().astype(int)) / y.size\n",
        "    return np.log(a_priori)\n",
        "\n",
        "  def _fit_params(self, X, y):\n",
        "    # estimate all needed parameters for given model\n",
        "    raise NotImplementedError()\n",
        "\n",
        "  def _predict_log_conditional(self, x, class_idx):\n",
        "    # predict the log(P(x|G=class_idx)), the log of the conditional probability of x given the class\n",
        "    # this should depend on the model used\n",
        "    raise NotImplementedError()\n",
        "\n",
        "  def fit(self, X, y, a_priori=None):\n",
        "    # first encode the classes\n",
        "    y = self.encoder.fit_transform(y)\n",
        "\n",
        "    # if it's needed, estimate a priori probabilities\n",
        "    self.log_a_priori = self._estimate_a_priori(y) if a_priori is None else np.log(a_priori)\n",
        "\n",
        "    # check that a_priori has the correct number of classes\n",
        "    assert len(self.log_a_priori) == len(self.encoder.names), \"A priori probabilities do not match number of classes\"\n",
        "\n",
        "    # now that everything else is in place, estimate all needed parameters for given model\n",
        "    self._fit_params(X, y)\n",
        "\n",
        "  def predict(self, X):\n",
        "    # this is actually an individual prediction encased in a for-loop\n",
        "    m_obs = X.shape[1] # nro. de observaciones (Columnas de X)\n",
        "    y_hat = np.empty(m_obs, dtype=self.encoder.fmt) # 1 x m\n",
        "\n",
        "    for i in range(m_obs):\n",
        "      encoded_y_hat_i = self._predict_one(X[:,i].reshape(-1,1))\n",
        "      # Es la predicción numérica 0, 1 ó 2\n",
        "\n",
        "      y_hat[i] = self.encoder.names[encoded_y_hat_i]\n",
        "      # Es el valor de la predicción en texto (descripción)\n",
        "\n",
        "      # y_hat es el array de predicciones (descripciones) 90 x 1\n",
        "      # Se devuelve 1 x 90\n",
        "\n",
        "    # return prediction as a row vector (matching y)\n",
        "    return y_hat.reshape(1,-1)\n",
        "\n",
        "  def _predict_one(self, x):\n",
        "    # calculate all log posteriori probabilities (actually, +C)\n",
        "    log_posteriori = [ log_a_priori_i + self._predict_log_conditional(x, idx) for idx, log_a_priori_i\n",
        "                  in enumerate(self.log_a_priori) ]\n",
        "\n",
        "    # Log posteriori es un array de las probabilidades de cada clase\n",
        "    # Ej.: [array([[-289.46784276]]), array([[2.3818009]]), array([[-2.34841277]])]\n",
        "    # Devuelve el índice con la probabilidad mayor (0, 1, ó 2)\n",
        "\n",
        "    # return the class that has maximum a posteriori probability\n",
        "    return np.argmax(log_posteriori)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### QDA"
      ],
      "metadata": {
        "id": "X6G16xD5pHrF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class QDA(BaseBayesianClassifier):\n",
        "\n",
        "  def _fit_params(self, X, y):\n",
        "\n",
        "    # estimate each covariance matrix\n",
        "    self.inv_covs = [inv(np.cov(X[:,y.flatten()==idx], bias=True))\n",
        "                      for idx in range(len(self.log_a_priori))]\n",
        "\n",
        "    self.means = [X[:,y.flatten()==idx].mean(axis=1, keepdims=True)\n",
        "                  for idx in range(len(self.log_a_priori))]\n",
        "\n",
        "  def _predict_log_conditional(self, x, class_idx):\n",
        "\n",
        "    # predict the log(P(x|G=class_idx)), the log of the conditional probability of x given the class\n",
        "    # this should depend on the model used\n",
        "    inv_cov = self.inv_covs[class_idx]\n",
        "    unbiased_x =  x - self.means[class_idx]\n",
        "    return 0.5*np.log(det(inv_cov)) -0.5 * unbiased_x.T @ inv_cov @ unbiased_x"
      ],
      "metadata": {
        "id": "IRamFdiGDuSR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TensorizedQDA"
      ],
      "metadata": {
        "id": "PjoCuxRCpEMq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TensorizedQDA(QDA):\n",
        "\n",
        "    def _fit_params(self, X, y):\n",
        "        # ask plain QDA to fit params\n",
        "        super()._fit_params(X,y)\n",
        "\n",
        "        # stack onto new dimension\n",
        "        self.tensor_inv_cov = np.stack(self.inv_covs)\n",
        "        self.tensor_means = np.stack(self.means)\n",
        "\n",
        "    def _predict_log_conditionals(self,x):\n",
        "        unbiased_x = x - self.tensor_means\n",
        "        inner_prod = unbiased_x.transpose(0,2,1) @ self.tensor_inv_cov @ unbiased_x\n",
        "\n",
        "        return 0.5*np.log(det(self.tensor_inv_cov)) - 0.5 * inner_prod.flatten()\n",
        "\n",
        "    def _predict_one(self, x):\n",
        "        # return the class that has maximum a posteriori probability\n",
        "        return np.argmax(self.log_a_priori + self._predict_log_conditionals(x))"
      ],
      "metadata": {
        "id": "fRtC9HEkO5Hu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### FasterQDA"
      ],
      "metadata": {
        "id": "oxnjVLbdqqlz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FasterQDA(TensorizedQDA):\n",
        "\n",
        "  def predict(self, X):\n",
        "    # X es (n,n), n variables, m observaciones\n",
        "    y_hat = np.empty(X.shape[1], dtype=self.encoder.fmt) # m x 1\n",
        "\n",
        "    # unbiased es (3, n, m) (clases, variables, observaciones)\n",
        "    unbiased_X = X - self.tensor_means\n",
        "\n",
        "    # unbiased_X es (3, n, m)\n",
        "    # unbiased_X.transpose es (3, m, n)\n",
        "    # tensor_inv_cov.shape es (3, n, n)\n",
        "    # log_a_priori.shape es (3,)\n",
        "\n",
        "    # inner_prod es (3, m, m)\n",
        "    inner_prod = unbiased_X.transpose(0,2,1) @ self.tensor_inv_cov @ unbiased_X\n",
        "\n",
        "    # print(\"Resultado del producto interno (3, m, m)\")\n",
        "    # print(inner_prod)\n",
        "\n",
        "    # tomar los elementos de la diagonal para reducir la matriz de mxm\n",
        "    inner_prod_d = np.array([np.diagonal(inner_prod[i]) for i in range(inner_prod.shape[0])])\n",
        "\n",
        "    m_a_priori = np.tile(self.log_a_priori, (X.shape[1], 1))\n",
        "    m_log_det = np.tile(np.log(det(self.tensor_inv_cov)), (X.shape[1], 1))\n",
        "    m_result = m_a_priori + (0.5*m_log_det - 0.5*inner_prod_d.T)\n",
        "    encoded_y_hat = np.argmax(m_result, axis=1)\n",
        "    # encoded_y_hat es (m,)\n",
        "\n",
        "    y_hat[:len(encoded_y_hat)] = self.encoder.names[encoded_y_hat]\n",
        "\n",
        "    # (1, m)\n",
        "    return y_hat.reshape(1,-1)\n"
      ],
      "metadata": {
        "id": "vWfhjU_NqrfD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RealFasterQDA\n",
        "\n",
        "\n",
        "Demostrar que\n",
        "$$\n",
        "diag(A \\cdot B) = \\sum_{cols} A \\odot B^T = np.sum(A \\odot B^T, axis=1)\n",
        "$$ es decir, que se puede \"esquivar\" la matriz de *n x n* usando matrices de *n x p*.\n",
        "\n",
        "Sean:\n",
        "\n",
        "**A** una matriz de *n x p*\n",
        "\n",
        "$$\n",
        "\\mathbf{A} =\n",
        "\\begin{pmatrix}\n",
        "a_{11} & ... & a_{1p} \\\\\n",
        "a_{21} & ... & a_{2p} \\\\\n",
        "... & ... & ... \\\\\n",
        "a_{n1} & ... & a_{np} \\\\\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "**B** una matriz de *p x n*\n",
        "\n",
        "$$\n",
        "\\mathbf{B} =\n",
        "\\begin{pmatrix}\n",
        "b_{11} & ... & b_{1n} \\\\\n",
        "b_{21} & ... & b_{2n} \\\\\n",
        "... & ... & ... \\\\\n",
        "b_{p1} & ... & b_{pn} \\\\\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "\n",
        "El producto matricial entre **A** y **B** resulta una matriz de *n x n* de la forma:\n",
        "\n",
        "$$\n",
        "\\\\\n",
        "\\begin{pmatrix}\n",
        "a_{11}b_{11} + a_{12}b_{21} + ... + a_{1p}b_{p1} & ... & a_{11}b_{1n} + a_{12}b_{2n} + ... + a_{1p}b_{pn} \\\\\n",
        "a_{21}b_{11} + a_{22}b_{21} + ... + a_{2p}b_{p1} & ... & a_{21}b_{1n} + a_{22}b_{2n} + ... + a_{2p}b_{pn} \\\\\n",
        "... & ... & ... \\\\\n",
        "a_{n1}b_{11} + a_{n2}b_{21} + ... + a_{np}b_{p1} & ... & a_{n1}b_{1n} + a_{n2}b_{2n} + ... + a_{np}b_{pn} \\\\\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "\n",
        "La diagonal resulta un vector de *n* elementos de la forma:\n",
        "$$\n",
        "\\\\\n",
        "\\begin{pmatrix}\n",
        "a_{11}b_{11} + a_{12}b_{21} + ... + a_{1p}b_{p1} \\\\\n",
        "a_{21}b_{12} + a_{22}b_{22} + ... + a_{2p}b_{p2} \\\\\n",
        "... \\\\\n",
        "a_{n1}b_{1n} + a_{n2}b_{2n} + ... + a_{np}b_{pn} \\\\\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "\n",
        "Por otro lado, **B** traspuesta es la matriz de *n x p* de la forma:\n",
        "\n",
        "$$\n",
        "\\mathbf{B^T} =\n",
        "\\begin{pmatrix}\n",
        "b_{11} & ... & b_{p1} \\\\\n",
        "b_{12} & ... & b_{p2} \\\\\n",
        "... & ... & ... \\\\\n",
        "b_{1n} & ... & b_{pn} \\\\\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "Al hacer el producto elemento a elemento con **A**, quedaría de la forma:\n",
        "\n",
        "\n",
        "$$\n",
        "\\begin{pmatrix}\n",
        "a_{11}b_{11} & ... & a_{1p}b_{p1} \\\\\n",
        "a_{21}b_{12} & ... & a_{2p}b_{p2} \\\\\n",
        "... & ... & ... \\\\\n",
        "a_{n1}b_{1n} & ... & a_{np}b_{pn} \\\\\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "\n",
        "Finalmente, si sumamos los elementos de las filas en una sola columna, resulta el mismo vector diagonal anterior, demostrándose la igualdad:\n",
        "\n",
        "\n",
        "$$\n",
        "\\begin{pmatrix}\n",
        "a_{11}b_{11} + a_{12}b_{21} + ... + a_{1p}b_{p1} \\\\\n",
        "a_{21}b_{12} + a_{22}b_{22} + ... + a_{2p}b_{p2} \\\\\n",
        "... \\\\\n",
        "a_{n1}b_{1n} + a_{n2}b_{2n} + ... + a_{np}b_{pn} \\\\\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wYzVj0w1FnP3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RealFasterQDA(TensorizedQDA):\n",
        "\n",
        "\n",
        "  def predict(self, X):\n",
        "    y_hat = np.empty(X.shape[1], dtype=self.encoder.fmt)\n",
        "\n",
        "    unbiased_X = X - self.tensor_means\n",
        "\n",
        "    #A => (unbiased_X.transpose(0,2,1) @ self.tensor_inv_cov)\n",
        "    #B_t => (np.transpose(unbiased_X, (0,2,1)))\n",
        "    # diag (A@B) = np.sum(A*B_t, axis=1)\n",
        "\n",
        "    inner_prod_d = np.sum((unbiased_X.transpose(0,2,1) @ self.tensor_inv_cov)*(np.transpose(unbiased_X, (0,2,1))), axis=2)\n",
        "\n",
        "    m_a_priori = np.tile(self.log_a_priori, (X.shape[1], 1))\n",
        "    m_log_det = np.tile(np.log(det(self.tensor_inv_cov)), (X.shape[1], 1))\n",
        "    m_result = m_a_priori + (0.5*m_log_det - 0.5*inner_prod_d.T)\n",
        "    encoded_y_hat = np.argmax(m_result, axis=1)\n",
        "\n",
        "    y_hat[:len(encoded_y_hat)] = self.encoder.names[encoded_y_hat]\n",
        "\n",
        "    return y_hat.reshape(1,-1)"
      ],
      "metadata": {
        "id": "ZrokO8pRFLwR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Código para pruebas"
      ],
      "metadata": {
        "id": "KS_zoK-gWkRf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hiperparámetros"
      ],
      "metadata": {
        "id": "nz19b6NJed2A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# hiperparámetros\n",
        "rng_seed = 6543"
      ],
      "metadata": {
        "id": "m05KrhUDINVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DataSets"
      ],
      "metadata": {
        "id": "jBznT0R8pdSU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris, fetch_openml\n",
        "\n",
        "def get_iris_dataset():\n",
        "  data = load_iris()\n",
        "  X_full = data.data\n",
        "  y_full = np.array([data.target_names[y] for y in data.target.reshape(-1,1)])\n",
        "  return X_full, y_full\n",
        "\n",
        "def get_penguins():\n",
        "    # get data\n",
        "    df, tgt = fetch_openml(name=\"penguins\", return_X_y=True, as_frame=True, parser='auto')\n",
        "\n",
        "    # drop non-numeric columns\n",
        "    df.drop(columns=[\"island\",\"sex\"], inplace=True)\n",
        "\n",
        "    # drop rows with missing values\n",
        "    mask = df.isna().sum(axis=1) == 0\n",
        "    df = df[mask]\n",
        "    tgt = tgt[mask]\n",
        "\n",
        "    return df.values, tgt.to_numpy().reshape(-1,1)\n",
        "\n",
        "# showing for iris\n",
        "X_full, y_full = get_iris_dataset()\n",
        "\n",
        "print(f\"X: {X_full.shape}, Y:{y_full.shape}\")"
      ],
      "metadata": {
        "id": "2hkXcoldXOqs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50525a28-5bff-4829-e01e-0bcd234f0402"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X: (150, 4), Y:(150, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# peek data matrix\n",
        "X_full[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jAk-UQCjKecT",
        "outputId": "d2f46aba-877f-4a5a-d19f-2ecc9a984e05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[5.1, 3.5, 1.4, 0.2],\n",
              "       [4.9, 3. , 1.4, 0.2],\n",
              "       [4.7, 3.2, 1.3, 0.2],\n",
              "       [4.6, 3.1, 1.5, 0.2],\n",
              "       [5. , 3.6, 1.4, 0.2]])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# peek target vector\n",
        "y_full[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YdzMURX2KVdO",
        "outputId": "5889c617-787e-4113-864c-695c2c424cc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([['setosa'],\n",
              "       ['setosa'],\n",
              "       ['setosa'],\n",
              "       ['setosa'],\n",
              "       ['setosa']], dtype='<U10')"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Split del DataSet en Train/Test"
      ],
      "metadata": {
        "id": "Kl8UFh1OegbJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# preparing data, train - test validation\n",
        "# 70-30 split\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def split_transpose(X, y, test_sz, random_state):\n",
        "    # split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=random_state)\n",
        "\n",
        "    # transpose so observations are column vectors\n",
        "    return X_train.T, y_train.T, X_test.T, y_test.T\n",
        "\n",
        "def accuracy(y_true, y_pred):\n",
        "  return (y_true == y_pred).mean()\n",
        "\n",
        "train_x, train_y, test_x, test_y = split_transpose(X_full, y_full, 0.4, rng_seed)\n",
        "\n",
        "print(train_x.shape, train_y.shape, test_x.shape, test_y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LKP_QmWCIECs",
        "outputId": "1f7a0e34-a737-4493-b8e8-14facafb9fa2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4, 90) (1, 90) (4, 60) (1, 60)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Entrenamiento y performance"
      ],
      "metadata": {
        "id": "LwgXFPbJemb_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qda = QDA()\n",
        "\n",
        "qda.fit(train_x, train_y)"
      ],
      "metadata": {
        "id": "dGIf2TA5SpoT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_acc = accuracy(train_y, qda.predict(train_x))\n",
        "test_acc = accuracy(test_y, qda.predict(test_x))\n",
        "print(f\"Train (apparent) error is {1-train_acc:.4f} while test error is {1-test_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0Q30DyLWpTL",
        "outputId": "56f13c3d-4c0a-40b7-b491-a663a41b23bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train (apparent) error is 0.0111 while test error is 0.0167\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t_qda = TensorizedQDA()\n",
        "\n",
        "t_qda.fit(train_x, train_y)"
      ],
      "metadata": {
        "id": "kxBgJPlI4yRu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ttrain_acc = accuracy(train_y, t_qda.predict(train_x))\n",
        "ttest_acc = accuracy(test_y, t_qda.predict(test_x))\n",
        "print(f\"Train (apparent) error is {1-ttrain_acc:.4f} while test error is {1-ttest_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ZFoFu2T435-",
        "outputId": "23be8dea-04c4-4b6d-c721-b14f46832baf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train (apparent) error is 0.0111 while test error is 0.0167\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f_qda = FasterQDA()\n",
        "\n",
        "f_qda.fit(train_x, train_y)"
      ],
      "metadata": {
        "id": "nTzwVK6E5fwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ftrain_acc = accuracy(train_y, f_qda.predict(train_x))\n",
        "ftest_acc = accuracy(test_y, f_qda.predict(test_x))\n",
        "print(f\"Train (apparent) error is {1-ftrain_acc:.4f} while test error is {1-ftest_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z6VB-21G5kbI",
        "outputId": "cfc2f90a-65e0-463d-c1dd-574070a17423"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train (apparent) error is 0.0111 while test error is 0.0167\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rf_qda = RealFasterQDA()\n",
        "\n",
        "rf_qda.fit(train_x, train_y)"
      ],
      "metadata": {
        "id": "vYEQ6kncFYCm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rftrain_acc = accuracy(train_y, rf_qda.predict(train_x))\n",
        "rftest_acc = accuracy(test_y, rf_qda.predict(test_x))\n",
        "print(f\"Train (apparent) error is {1-rftrain_acc:.4f} while test error is {1-rftest_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKcTDiEKFc2l",
        "outputId": "45afd5d6-756a-4378-f0d2-b3cb46e05392"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train (apparent) error is 0.0111 while test error is 0.0167\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%timeit\n",
        "\n",
        "qda.predict(test_x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rf2Eey9nk9Qt",
        "outputId": "f4f5769e-36e7-4b3c-808d-63fc27833aa2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6.22 ms ± 652 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%timeit\n",
        "\n",
        "t_qda.predict(test_x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_oGST5-k8m3",
        "outputId": "edd93dfe-463e-4cd4-abb5-c15e1e1b0cf2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.82 ms ± 262 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%timeit\n",
        "\n",
        "f_qda.predict(test_x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G4VMMhCtk4sV",
        "outputId": "3d40d322-3fd9-48ac-ff6c-cb6a30b8c45e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "103 µs ± 24.8 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%timeit\n",
        "\n",
        "rf_qda.predict(test_x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vnZT-HN2fUuW",
        "outputId": "5dc6ee78-717d-450c-d63f-6799c98a82f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "95.4 µs ± 20.3 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%timeit\n",
        "\n",
        "model = QDA()\n",
        "model.fit(train_x, train_y)\n",
        "model.predict(test_x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kjFbVSqfeHUX",
        "outputId": "0254a727-a1d5-4be3-b73a-2f55d2c84a25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21.2 ms ± 7.36 ms per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
          ]
        }
      ]
    }
  ]
}