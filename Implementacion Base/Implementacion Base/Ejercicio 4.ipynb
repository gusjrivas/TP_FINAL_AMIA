{"cells":[{"cell_type":"markdown","metadata":{"id":"RIuUO15QGc0L"},"source":["#Implementación base\n","4. Utilizar otros 2 (dos) valores de *random seed* para obtener distintos splits de train y test, y repetir la comparación del punto anterior ¿Las conclusiones previas se mantienen?"]},{"cell_type":"markdown","source":["Aleatoriedad en la selección de datos: Cambiar la asignación de los datos puede llevar a conjuntos de entrenamiento que, por casualidad, sean más fáciles de ajustar para el modelo QDA. Es posible que el nuevo conjunto de entrenamiento contenga ejemplos que sean más representativos de las distribuciones subyacentes de las clases o que tengan menos ruido, lo que facilita que el modelo aprenda patrones claros y reduzca el error de entrenamiento.\n","\n","Varianza del modelo: La disminución del error de entrenamiento podría ser una consecuencia de una menor varianza en el nuevo conjunto de entrenamiento. Si los datos en el nuevo conjunto de entrenamiento son menos variables o más homogéneos, el modelo puede ajustarse más fácilmente a estos datos, reduciendo así el error de entrenamiento.\n","\n","Distribución equilibrada: La nueva asignación de datos podría haber llevado a un conjunto de entrenamiento que tiene una distribución de clases más equilibrada o mejor representada. Esto puede ayudar al modelo a aprender los parámetros de manera más efectiva, resultando en un error de entrenamiento más bajo.\n","\n","Ruido en los datos: Si el conjunto de entrenamiento original contenía datos ruidosos o atípicos, reasignar los datos puede haber reducido la cantidad de estos datos problemáticos en el conjunto de entrenamiento. Menos ruido puede llevar a un mejor ajuste del modelo y, por lo tanto, a un error de entrenamiento más bajo.\n","\n"],"metadata":{"id":"4YTmrcfOJZcg"}},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":415,"status":"ok","timestamp":1717804791290,"user":{"displayName":"Santi Olaciregui","userId":"08004037901086979441"},"user_tz":180},"id":"T__eZOdyGbdS"},"outputs":[],"source":["# Se importan las librerias necesarias\n","import numpy as np\n","from numpy.linalg import det, inv"]},{"cell_type":"markdown","metadata":{"id":"TaaPO0evcBDD"},"source":["# Modelo"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1717804791751,"user":{"displayName":"Santi Olaciregui","userId":"08004037901086979441"},"user_tz":180},"id":"6tumZFxEH2w7"},"outputs":[],"source":["class ClassEncoder:\n","  \"\"\"\n","  Permite codificar etiquetas categóricas en valores numéricos y decodificarlos\n","  de vuelta a sus etiquetas originales.\n","  \"\"\"\n","\n","  def fit(self, y):\n","    \"\"\"\n","    Ajusta el codificador a las etiquetas proporcionadas.\n","    paso a paso:\n","    np.unique(y) encuentra las etiquetas únicas en y y las almacena en self.names.\n","    Crea un diccionario self.name_to_class que asigna a cada etiqueta única un índice numérico.\n","    Almacena el tipo de datos de y en self.fmt.\n","    \"\"\"\n","    self.names = np.unique(y)\n","    self.name_to_class = {name:idx for idx, name in enumerate(self.names)}\n","    self.fmt = y.dtype\n","    # Q1: por que no hace falta definir un class_to_name para el mapeo inverso?\n","\n","  def _map_reshape(self, f, arr):\n","    \"\"\"\n","    Descripción: Aplica una función f a cada elemento de un array arr y luego lo remodela a su forma original.\n","    paso a paso:\n","    arr.flatten() aplana el array a una dimensión.\n","    Aplica la función f a cada elemento del array aplanado.\n","    Convierte el resultado en un array de NumPy y lo remodela a la forma original de arr.\n","    \"\"\"\n","    return np.array([f(elem) for elem in arr.flatten()]).reshape(arr.shape)\n","    # Q2: por que hace falta un reshape?\n","\n","  def transform(self, y):\n","    \"\"\"\n","    Transforma las etiquetas en números enteros usando el mapeo definido en fit.\n","    paso a paso:\n","    Utiliza _map_reshape para aplicar el diccionario self.name_to_class a cada etiqueta en y,\n","    convirtiéndolas en sus correspondientes valores numéricos.\n","    \"\"\"\n","    return self._map_reshape(lambda name: self.name_to_class[name], y)\n","\n","  def fit_transform(self, y):\n","    \"\"\"\n","    Ajusta el codificador a las etiquetas y luego las transforma en una sola operación.\n","    paso a paso:\n","    Llama a fit(y) para ajustar el codificador.\n","    Luego llama a transform(y) para transformar las etiquetas ajustadas.\n","    \"\"\"\n","    self.fit(y)\n","    return self.transform(y)\n","\n","  def detransform(self, y_hat):\n","    \"\"\"\n","    Convierte los valores numéricos de vuelta a sus etiquetas originales.\n","    paso a paso:\n","    Utiliza _map_reshape para aplicar self.names a cada índice en y_hat,\n","    convirtiéndolos de vuelta a sus etiquetas originales.\n","    \"\"\"\n","    return self._map_reshape(lambda idx: self.names[idx], y_hat)"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1717804791751,"user":{"displayName":"Santi Olaciregui","userId":"08004037901086979441"},"user_tz":180},"id":"pMsyCGFKQGdk"},"outputs":[],"source":["class BaseBayesianClassifier:\n","  \"\"\"\n","  Implementación base para un clasificador Bayesiano.\n","  Contiene métodos para ajustar el modelo y predecir clases basándose\n","  en probabilidades a priori y condicionales.\n","  \"\"\"\n","\n","  def __init__(self):\n","    \"\"\"\n","    Inicializa el clasificador creando un objeto ClassEncoder para codificar\n","    etiquetas categóricas en números enteros.\n","    \"\"\"\n","    self.encoder = ClassEncoder()\n","\n","  def _estimate_a_priori(self, y):\n","    \"\"\"\n","    Estima las probabilidades a priori para cada clase.\n","    paso a paso:\n","    np.bincount(y.flatten().astype(int)) cuenta el número de ocurrencias de cada valor entero en y.\n","    Divide por y.size para obtener las frecuencias relativas (probabilidades).\n","    Devuelve el logaritmo natural de estas probabilidades.\n","    \"\"\"\n","    # Obtener el número de clases en y: 3 en este caso\n","    num_classes = len(np.unique(y))\n","    # Crear un array con probabilidades a priori iguales: 1/3 en este caso\n","    a_priori = np.full(num_classes, 1/num_classes)\n","    # Q3: para que sirve bincount?\n","    return np.log(a_priori)\n","\n","  def _fit_params(self, X, y):\n","    \"\"\"\n","    Método abstracto que debe ser implementado por una subclase para ajustar\n","    los parámetros del modelo específico.\n","    paso a paso:\n","    No tiene funcionalidad en esta clase base, solo establece que las subclases\n","    deben implementar este método.\n","    \"\"\"\n","    # estimate all needed parameters for given model\n","    raise NotImplementedError()\n","\n","  def _predict_log_conditional(self, x, class_idx):\n","    \"\"\"\n","    Método abstracto que debe ser implementado por una subclase para predecir\n","    la probabilidad condicional logarítmica.\n","    paso a paso:\n","    No tiene funcionalidad en esta clase base, solo establece que las subclases\n","    deben implementar este método.\n","    \"\"\"\n","    # predict the log(P(x|G=class_idx)), the log of the conditional probability of x given the class\n","    # this should depend on the model used\n","    raise NotImplementedError()\n","\n","  def fit(self, X, y, a_priori=None):\n","    \"\"\"\n","    Ajusta el clasificador a los datos X y y.\n","    paso a paso:\n","    Codifica las etiquetas y usando ClassEncoder.\n","    Estima las probabilidades a priori si no se proporcionan.\n","    Verifica que las probabilidades a priori coincidan con el número de clases.\n","    Llama al método _fit_params para ajustar los parámetros del modelo específico.\n","    \"\"\"\n","    # first encode the classes\n","    y = self.encoder.fit_transform(y)\n","    # if it's needed, estimate a priori probabilities\n","    self.log_a_priori = self._estimate_a_priori(y) if a_priori is None else np.log(a_priori)\n","    # check that a_priori has the correct number of classes\n","    assert len(self.log_a_priori) == len(self.encoder.names), \"A priori probabilities do not match number of classes\"\n","    # now that everything else is in place, estimate all needed parameters for given model\n","    self._fit_params(X, y)\n","    # Q4: por que el _fit_params va al final? no se puede mover a, por ejemplo, antes de la priori?\n","\n","  def predict(self, X):\n","    \"\"\"\n","    Predice las etiquetas para las observaciones en X.\n","    paso a paso:\n","    Inicializa un array vacío y_hat para almacenar las predicciones.\n","    Itera sobre cada observación en X, predice su clase usando _predict_one,\n","    y almacena el resultado decodificado en y_hat.\n","    Devuelve y_hat como un vector fila.\n","    \"\"\"\n","    # this is actually an individual prediction encased in a for-loop\n","    m_obs = X.shape[1]\n","    y_hat = np.empty(m_obs, dtype=self.encoder.fmt)\n","    for i in range(m_obs):\n","      encoded_y_hat_i = self._predict_one(X[:,i].reshape(-1,1))\n","      y_hat[i] = self.encoder.names[encoded_y_hat_i]\n","    # return prediction as a row vector (matching y)\n","    return y_hat.reshape(1,-1)\n","\n","  def _predict_one(self, x):\n","    \"\"\"\n","    Predice la clase para una única observación x.\n","    paso a paso:\n","    Calcula la probabilidad a posteriori logarítmica para cada clase sumando\n","    la probabilidad a priori logarítmica y la probabilidad condicional logarítmica.\n","    Devuelve el índice de la clase con la máxima probabilidad a posteriori.\n","    \"\"\"\n","    # calculate all log posteriori probabilities (actually, +C)\n","    log_posteriori = [ log_a_priori_i + self._predict_log_conditional(x, idx) for idx, log_a_priori_i\n","                  in enumerate(self.log_a_priori) ]\n","\n","    # return the class that has maximum a posteriori probability\n","    return np.argmax(log_posteriori)"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1717804791752,"user":{"displayName":"Santi Olaciregui","userId":"08004037901086979441"},"user_tz":180},"id":"MELoCHpdUpjf"},"outputs":[],"source":["class LDA(BaseBayesianClassifier):\n","    \"\"\"\n","    Clasifica los datos basándose en modelos Gaussianos con una matriz de\n","    covarianza común para todas las clases.\n","    \"\"\"\n","\n","    def _fit_params(self, X, y):\n","        \"\"\"\n","        Ajusta los parámetros del modelo LDA.\n","        Calcula la matriz de covarianza común y las medias de cada clase.\n","        \"\"\"\n","        # Número de clases\n","        num_classes = len(self.log_a_priori)\n","\n","        # Calcular la media de cada clase\n","        self.means = [X[:, y.flatten() == idx].mean(axis=1, keepdims=True)\n","                      for idx in range(num_classes)]\n","\n","        # Calcular la matriz de covarianza común\n","        # Inicializar matriz de covarianza común\n","        cov = np.zeros((X.shape[0], X.shape[0]))\n","\n","        for idx in range(num_classes):\n","            X_class = X[:, y.flatten() == idx]\n","            cov += np.cov(X_class, bias=True) * X_class.shape[1]\n","\n","        cov /= X.shape[1]  # Dividir por el número total de observaciones\n","        self.inv_cov = inv(cov)\n","\n","    def _predict_log_conditional(self, x, class_idx):\n","        \"\"\"\n","        Predice el logaritmo de la probabilidad condicional de x dado class_idx.\n","        \"\"\"\n","        unbiased_x = x - self.means[class_idx]\n","        return -0.5 * unbiased_x.T @ self.inv_cov @ unbiased_x\n"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"DnX-r6E2I1eJ","executionInfo":{"status":"ok","timestamp":1717804791752,"user_tz":180,"elapsed":4,"user":{"displayName":"Santi Olaciregui","userId":"08004037901086979441"}}},"outputs":[],"source":["class QDA(BaseBayesianClassifier):\n","  \"\"\"\n","  Clasifica los datos basándose en modelos Gaussianos con diferentes matrices\n","  de covarianza para cada clase.\n","  \"\"\"\n","\n","  def _fit_params(self, X, y):\n","    \"\"\"\n","    flatten, se asegura que y sea un array unidimensional, lo cual es necesario\n","    para hacer comparaciones elementales como y == idx. Sin flatten,\n","    y == idx puede no funcionar correctamente si y tiene más de una dimensión.\n","    bias=True en np.cov ajusta el cálculo de la matriz de covarianza dividiendo\n","    por N (número de observaciones) en lugar de N-1. Esto proporciona una\n","    estimación de la covarianza basada en la población en lugar de la muestra.\n","    X es una matriz donde las filas representan características y las columnas\n","    representan observaciones. Calcular la media a lo largo de las columnas\n","    (axis=1) proporciona la media de cada característica para cada clase.\n","    \"\"\"\n","    # estimate each covariance matrix\n","    self.inv_covs = [inv(np.cov(X[:,y.flatten()==idx], bias=True))\n","                      for idx in range(len(self.log_a_priori))]\n","    # Q5: por que hace falta el flatten y no se puede directamente X[:,y==idx]?\n","    # Q6: por que se usa bias=True en vez del default bias=False?\n","    self.means = [X[:,y.flatten()==idx].mean(axis=1, keepdims=True)\n","                  for idx in range(len(self.log_a_priori))]\n","    # Q7: que hace axis=1? por que no axis=0?\n","\n","  def _predict_log_conditional(self, x, class_idx):\n","    \"\"\"\n","    Predice el logaritmo de la probabilidad condicional de x dado class_idx.\n","    paso a paso:\n","    Obtiene la matriz de covarianza inversa para la clase class_idx.\n","    Calcula unbiased_x restando la media de la clase de x.\n","    Calcula el logaritmo de la probabilidad condicional usando la fórmula del\n","    modelo Gaussiano cuadrático.\n","    \"\"\"\n","    # predict the log(P(x|G=class_idx)), the log of the conditional probability of x given the class\n","    # this should depend on the model used\n","    inv_cov = self.inv_covs[class_idx]\n","    unbiased_x =  x - self.means[class_idx]\n","    return 0.5*np.log(det(inv_cov)) -0.5 * unbiased_x.T @ inv_cov @ unbiased_x"]},{"cell_type":"markdown","metadata":{"id":"aZ9vYyWcb2x1"},"source":["# Código para pruebas"]},{"cell_type":"markdown","metadata":{"id":"SBZD2PMFdkVg"},"source":["Seteamos los datos"]},{"cell_type":"markdown","source":["# **LDA**"],"metadata":{"id":"v02KosSIJoVH"}},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1717804791752,"user":{"displayName":"Santi Olaciregui","userId":"08004037901086979441"},"user_tz":180},"id":"Mjyid9dBb-DZ"},"outputs":[],"source":["# hiperparámetros\n","rng_seed = 10000"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1717804791752,"user":{"displayName":"Santi Olaciregui","userId":"08004037901086979441"},"user_tz":180},"id":"451U-AtccHtJ","outputId":"02e002ac-9564-48a7-a08d-f9d4e66501c6"},"outputs":[{"output_type":"stream","name":"stdout","text":["X: (342, 4), Y:(342, 1)\n"]}],"source":["from sklearn.datasets import load_iris, fetch_openml\n","\"\"\"\n","proporciona dos funciones para cargar conjuntos de datos: uno para el conjunto\n","de datos de Iris y otro para un conjunto de datos de pingüinos.\n","\"\"\"\n","\n","def get_iris_dataset():\n","  \"\"\"\n","  Carga el conjunto de datos de Iris.\n","  paso a paso:\n","  Utiliza load_iris de sklearn.datasets para cargar los datos.\n","  Extrae las características (X_full) y las etiquetas (y_full) del conjunto\n","  de datos cargado.\n","  Convierte las etiquetas numéricas a sus nombres correspondientes\n","  (e.g., de 0, 1, 2 a \"setosa\", \"versicolor\", \"virginica\").\n","  Devuelve las características y las etiquetas.\n","  \"\"\"\n","  data = load_iris()\n","  X_full = data.data\n","  y_full = np.array([data.target_names[y] for y in data.target.reshape(-1,1)])\n","  return X_full, y_full\n","\n","def get_penguins():\n","  \"\"\"\n","  Carga el conjunto de datos de pingüinos.\n","  paso a paso:\n","  Utiliza fetch_openml de sklearn.datasets para obtener el conjunto de datos de pingüinos.\n","  Selecciona las características y las etiquetas del conjunto de datos.\n","  Elimina columnas no numéricas (\"island\" y \"sex\").\n","  Elimina filas con valores faltantes.\n","  Devuelve las características y las etiquetas.\n","  \"\"\"\n","  # get data\n","  df, tgt = fetch_openml(name=\"penguins\", return_X_y=True, as_frame=True, parser='auto')\n","  # drop non-numeric columns\n","  df.drop(columns=[\"island\",\"sex\"], inplace=True)\n","  # drop rows with missing values\n","  mask = df.isna().sum(axis=1) == 0\n","  df = df[mask]\n","  tgt = tgt[mask]\n","  return df.values, tgt.to_numpy().reshape(-1,1)\n","\n","# showing for iris\n","X_full, y_full = get_penguins()\n","\n","print(f\"X: {X_full.shape}, Y:{y_full.shape}\")"]},{"cell_type":"markdown","metadata":{"id":"bY8Le0LPGYbU"},"source":["Separamos el dataset en train y test para medir performance"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1717804791752,"user":{"displayName":"Santi Olaciregui","userId":"08004037901086979441"},"user_tz":180},"id":"EVV6Yesadv5Z","outputId":"b95af6b6-e0f9-416f-9b1d-7f9b80b39a40"},"outputs":[{"output_type":"stream","name":"stdout","text":["(4, 239) (1, 239) (4, 103) (1, 103)\n"]}],"source":["# preparing data, train - test validation\n","# 70-30 split\n","from sklearn.model_selection import train_test_split\n","\n","def split_transpose(X, y, test_sz, random_state):\n","  \"\"\"\n","  Descripción: Divide los datos en conjuntos de train y test utilizando una\n","  división de 70-30 (train-test split), y luego transpone las matrices para que\n","  las observaciones sean columnas en lugar de filas.\n","  Parámetros:\n","  X: Matriz de características.\n","  y: Vector de etiquetas.\n","  test_sz: Tamaño del conjunto de test (proporción).\n","  random_state: Semilla aleatoria para reproducibilidad.\n","  Devuelve:\n","  X_train: Matriz de características de train transpuesta.\n","  y_train: Vector de etiquetas de train transpuesto.\n","  X_test: Matriz de características de test transpuesta.\n","  y_test: Vector de etiquetas de test transpuesto.\n","  \"\"\"\n","  # split\n","  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=random_state)\n","  # transpose so observations are column vectors\n","  return X_train.T, y_train.T, X_test.T, y_test.T\n","\n","def accuracy(y_true, y_pred):\n","  \"\"\"\n","  Descripción: Calcula la precisión de las predicciones comparando las etiquetas\n","  verdaderas con las predichas.\n","  Parámetros:\n","  y_true: Vector de etiquetas verdaderas.\n","  y_pred: Vector de etiquetas predichas.\n","  Devuelve:\n","  La precisión de las predicciones.\n","  \"\"\"\n","  return (y_true == y_pred).mean()\n","\n","train_x, train_y, test_x, test_y = split_transpose(X_full, y_full, 0.3, rng_seed)\n","\n","print(train_x.shape, train_y.shape, test_x.shape, test_y.shape)"]},{"cell_type":"markdown","metadata":{"id":"V5TxHukig1cD"},"source":["Entrenamos un LDA y medimos su accuracy"]},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1717804791752,"user":{"displayName":"Santi Olaciregui","userId":"08004037901086979441"},"user_tz":180},"id":"-jdOe1fOg6cF"},"outputs":[],"source":["lda = LDA()\n","lda.fit(train_x, train_y)"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":468,"status":"ok","timestamp":1717804792218,"user":{"displayName":"Santi Olaciregui","userId":"08004037901086979441"},"user_tz":180},"id":"O7126rs2oxnS","outputId":"d047ec51-fc99-46df-bf7b-ad87c1492fb7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Train (apparent) error is 0.0084 while test error is 0.0194\n"]}],"source":["train_acc = accuracy(train_y, lda.predict(train_x))\n","test_acc = accuracy(test_y, lda.predict(test_x))\n","print(f\"Train (apparent) error is {1-train_acc:.4f} while test error is {1-test_acc:.4f}\")"]},{"cell_type":"markdown","source":["# **QDA**"],"metadata":{"id":"jF9p5-twJjB3"}},{"cell_type":"markdown","metadata":{"id":"yI3bwRGyI1eK"},"source":["Entrenamos un QDA y medimos su accuracy"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QTbwDarKI1eK","executionInfo":{"status":"ok","timestamp":1717804792218,"user_tz":180,"elapsed":2,"user":{"displayName":"Santi Olaciregui","userId":"08004037901086979441"}},"outputId":"450a02b3-09c2-4576-ab02-0d95dd783b60"},"outputs":[{"output_type":"stream","name":"stdout","text":["(4, 239) (1, 239) (4, 103) (1, 103)\n"]}],"source":["rng_seed = 100\n","train_x, train_y, test_x, test_y = split_transpose(X_full, y_full, 0.3, rng_seed)\n","print(train_x.shape, train_y.shape, test_x.shape, test_y.shape)"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b1veMphqI1eK","executionInfo":{"status":"ok","timestamp":1717804792219,"user_tz":180,"elapsed":2,"user":{"displayName":"Santi Olaciregui","userId":"08004037901086979441"}},"outputId":"cc3f8b30-7bb7-4b9e-8a51-df83e2a3fbc7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Train (apparent) error is 0.0126 while test error is 0.0194\n"]}],"source":["qda = QDA()\n","qda.fit(train_x, train_y)\n","train_acc = accuracy(train_y, qda.predict(train_x))\n","test_acc = accuracy(test_y, qda.predict(test_x))\n","print(f\"Train (apparent) error is {1-train_acc:.4f} while test error is {1-test_acc:.4f}\")"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}