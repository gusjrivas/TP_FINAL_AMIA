{"cells":[{"cell_type":"markdown","metadata":{"id":"RIuUO15QGc0L"},"source":["#Implementación base\n","3. Implementar el modelo LDA, entrenarlo y testearlo contra los mismos sets que QDA (no múltiples prioris) ¿Se observan diferencias? ¿Podría decirse que alguno de los dos es notoriamente mejor que el otro?\n","\n"]},{"cell_type":"markdown","source":["Si los errores de predicción menores al utilizar LDA en comparación con QDA, hay varias razones posibles que podrían explicar este comportamiento:\n","\n","Menor complejidad del modelo: LDA asume que todas las clases comparten la misma matriz de covarianza, lo que puede resultar en un modelo más simple y menos propenso al sobreajuste, especialmente cuando tienes un conjunto de datos pequeño o ruidoso. QDA, por otro lado, permite que cada clase tenga su propia matriz de covarianza, lo que puede llevar a un modelo más complejo y, en algunos casos, más propenso al sobreajuste.\n","\n","Menos parámetros para estimar: Dado que LDA tiene menos parámetros para estimar (solo una matriz de covarianza común en lugar de una matriz de covarianza para cada clase en QDA), puede requerir menos datos de entrenamiento para estimar con precisión los parámetros del modelo. Si tienes un conjunto de datos de entrenamiento pequeño, LDA podría generalizar mejor que QDA.\n","\n","Mejor manejo del ruido: QDA puede ser más sensible al ruido en los datos debido a que estima una matriz de covarianza para cada clase, lo que puede llevar a una sobreajuste a las características irrelevantes o ruidosas. En contraste, LDA puede ser más robusto al ruido al asumir una matriz de covarianza común para todas las clases.\n","\n","Independencia de las características no garantizada: QDA asume que las características son independientes dentro de cada clase, mientras que LDA no hace esta suposición. Si las características no son completamente independientes dentro de cada clase, QDA puede producir estimaciones de covarianza sesgadas, lo que podría llevar a un rendimiento inferior en comparación con LDA.\n","\n","En resumen, las diferencias en los errores de predicción entre LDA y QDA pueden deberse a las diferencias en la complejidad del modelo, la cantidad de datos de entrenamiento disponibles, la robustez al ruido y la validez de las suposiciones del modelo. En tu caso particular, la combinación de una distribución a priori igual para todas las clases y la naturaleza de los datos puede favorecer el uso de LDA sobre QDA. cuando las distribuciones a priori de las clases son iguales, LDA puede ser más apropiado porque refleja mejor la igualdad de importancia entre las clases al asumir una matriz de covarianza común. Esto puede conducir a un mejor rendimiento general en comparación con QDA, especialmente cuando los datos son limitados o ruidosos."],"metadata":{"id":"Ba-iGtndGZXw"}},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1717803851357,"user":{"displayName":"Santi Olaciregui","userId":"08004037901086979441"},"user_tz":180},"id":"T__eZOdyGbdS"},"outputs":[],"source":["# Se importan las librerias necesarias\n","import numpy as np\n","from numpy.linalg import det, inv"]},{"cell_type":"markdown","metadata":{"id":"TaaPO0evcBDD"},"source":["# Modelo"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1717803851357,"user":{"displayName":"Santi Olaciregui","userId":"08004037901086979441"},"user_tz":180},"id":"6tumZFxEH2w7"},"outputs":[],"source":["class ClassEncoder:\n","  \"\"\"\n","  Permite codificar etiquetas categóricas en valores numéricos y decodificarlos\n","  de vuelta a sus etiquetas originales.\n","  \"\"\"\n","\n","  def fit(self, y):\n","    \"\"\"\n","    Ajusta el codificador a las etiquetas proporcionadas.\n","    paso a paso:\n","    np.unique(y) encuentra las etiquetas únicas en y y las almacena en self.names.\n","    Crea un diccionario self.name_to_class que asigna a cada etiqueta única un índice numérico.\n","    Almacena el tipo de datos de y en self.fmt.\n","    \"\"\"\n","    self.names = np.unique(y)\n","    self.name_to_class = {name:idx for idx, name in enumerate(self.names)}\n","    self.fmt = y.dtype\n","    # Q1: por que no hace falta definir un class_to_name para el mapeo inverso?\n","\n","  def _map_reshape(self, f, arr):\n","    \"\"\"\n","    Descripción: Aplica una función f a cada elemento de un array arr y luego lo remodela a su forma original.\n","    paso a paso:\n","    arr.flatten() aplana el array a una dimensión.\n","    Aplica la función f a cada elemento del array aplanado.\n","    Convierte el resultado en un array de NumPy y lo remodela a la forma original de arr.\n","    \"\"\"\n","    return np.array([f(elem) for elem in arr.flatten()]).reshape(arr.shape)\n","    # Q2: por que hace falta un reshape?\n","\n","  def transform(self, y):\n","    \"\"\"\n","    Transforma las etiquetas en números enteros usando el mapeo definido en fit.\n","    paso a paso:\n","    Utiliza _map_reshape para aplicar el diccionario self.name_to_class a cada etiqueta en y,\n","    convirtiéndolas en sus correspondientes valores numéricos.\n","    \"\"\"\n","    return self._map_reshape(lambda name: self.name_to_class[name], y)\n","\n","  def fit_transform(self, y):\n","    \"\"\"\n","    Ajusta el codificador a las etiquetas y luego las transforma en una sola operación.\n","    paso a paso:\n","    Llama a fit(y) para ajustar el codificador.\n","    Luego llama a transform(y) para transformar las etiquetas ajustadas.\n","    \"\"\"\n","    self.fit(y)\n","    return self.transform(y)\n","\n","  def detransform(self, y_hat):\n","    \"\"\"\n","    Convierte los valores numéricos de vuelta a sus etiquetas originales.\n","    paso a paso:\n","    Utiliza _map_reshape para aplicar self.names a cada índice en y_hat,\n","    convirtiéndolos de vuelta a sus etiquetas originales.\n","    \"\"\"\n","    return self._map_reshape(lambda idx: self.names[idx], y_hat)"]},{"cell_type":"markdown","source":["# **Ejercicio 1**"],"metadata":{"id":"snJXzt21GRAV"}},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1717803851357,"user":{"displayName":"Santi Olaciregui","userId":"08004037901086979441"},"user_tz":180},"id":"pMsyCGFKQGdk"},"outputs":[],"source":["class BaseBayesianClassifier:\n","  \"\"\"\n","  Implementación base para un clasificador Bayesiano.\n","  Contiene métodos para ajustar el modelo y predecir clases basándose\n","  en probabilidades a priori y condicionales.\n","  \"\"\"\n","\n","  def __init__(self):\n","    \"\"\"\n","    Inicializa el clasificador creando un objeto ClassEncoder para codificar\n","    etiquetas categóricas en números enteros.\n","    \"\"\"\n","    self.encoder = ClassEncoder()\n","\n","  def _estimate_a_priori(self, y):\n","    \"\"\"\n","    Estima las probabilidades a priori para cada clase.\n","    paso a paso:\n","    np.bincount(y.flatten().astype(int)) cuenta el número de ocurrencias de cada valor entero en y.\n","    Divide por y.size para obtener las frecuencias relativas (probabilidades).\n","    Devuelve el logaritmo natural de estas probabilidades.\n","    \"\"\"\n","    # Obtener el número de clases en y: 3 en este caso\n","    num_classes = len(np.unique(y))\n","    # Crear un array con probabilidades a priori iguales: 1/3 en este caso\n","    a_priori = np.full(num_classes, 1/num_classes)\n","    # Q3: para que sirve bincount?\n","    return np.log(a_priori)\n","\n","  def _fit_params(self, X, y):\n","    \"\"\"\n","    Método abstracto que debe ser implementado por una subclase para ajustar\n","    los parámetros del modelo específico.\n","    paso a paso:\n","    No tiene funcionalidad en esta clase base, solo establece que las subclases\n","    deben implementar este método.\n","    \"\"\"\n","    # estimate all needed parameters for given model\n","    raise NotImplementedError()\n","\n","  def _predict_log_conditional(self, x, class_idx):\n","    \"\"\"\n","    Método abstracto que debe ser implementado por una subclase para predecir\n","    la probabilidad condicional logarítmica.\n","    paso a paso:\n","    No tiene funcionalidad en esta clase base, solo establece que las subclases\n","    deben implementar este método.\n","    \"\"\"\n","    # predict the log(P(x|G=class_idx)), the log of the conditional probability of x given the class\n","    # this should depend on the model used\n","    raise NotImplementedError()\n","\n","  def fit(self, X, y, a_priori=None):\n","    \"\"\"\n","    Ajusta el clasificador a los datos X y y.\n","    paso a paso:\n","    Codifica las etiquetas y usando ClassEncoder.\n","    Estima las probabilidades a priori si no se proporcionan.\n","    Verifica que las probabilidades a priori coincidan con el número de clases.\n","    Llama al método _fit_params para ajustar los parámetros del modelo específico.\n","    \"\"\"\n","    # first encode the classes\n","    y = self.encoder.fit_transform(y)\n","    # if it's needed, estimate a priori probabilities\n","    self.log_a_priori = self._estimate_a_priori(y) if a_priori is None else np.log(a_priori)\n","    # check that a_priori has the correct number of classes\n","    assert len(self.log_a_priori) == len(self.encoder.names), \"A priori probabilities do not match number of classes\"\n","    # now that everything else is in place, estimate all needed parameters for given model\n","    self._fit_params(X, y)\n","    # Q4: por que el _fit_params va al final? no se puede mover a, por ejemplo, antes de la priori?\n","\n","  def predict(self, X):\n","    \"\"\"\n","    Predice las etiquetas para las observaciones en X.\n","    paso a paso:\n","    Inicializa un array vacío y_hat para almacenar las predicciones.\n","    Itera sobre cada observación en X, predice su clase usando _predict_one,\n","    y almacena el resultado decodificado en y_hat.\n","    Devuelve y_hat como un vector fila.\n","    \"\"\"\n","    # this is actually an individual prediction encased in a for-loop\n","    m_obs = X.shape[1]\n","    y_hat = np.empty(m_obs, dtype=self.encoder.fmt)\n","    for i in range(m_obs):\n","      encoded_y_hat_i = self._predict_one(X[:,i].reshape(-1,1))\n","      y_hat[i] = self.encoder.names[encoded_y_hat_i]\n","    # return prediction as a row vector (matching y)\n","    return y_hat.reshape(1,-1)\n","\n","  def _predict_one(self, x):\n","    \"\"\"\n","    Predice la clase para una única observación x.\n","    paso a paso:\n","    Calcula la probabilidad a posteriori logarítmica para cada clase sumando\n","    la probabilidad a priori logarítmica y la probabilidad condicional logarítmica.\n","    Devuelve el índice de la clase con la máxima probabilidad a posteriori.\n","    \"\"\"\n","    # calculate all log posteriori probabilities (actually, +C)\n","    log_posteriori = [ log_a_priori_i + self._predict_log_conditional(x, idx) for idx, log_a_priori_i\n","                  in enumerate(self.log_a_priori) ]\n","\n","    # return the class that has maximum a posteriori probability\n","    return np.argmax(log_posteriori)"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1717803851357,"user":{"displayName":"Santi Olaciregui","userId":"08004037901086979441"},"user_tz":180},"id":"MELoCHpdUpjf"},"outputs":[],"source":["class LDA(BaseBayesianClassifier):\n","    \"\"\"\n","    Clasifica los datos basándose en modelos Gaussianos con una matriz de\n","    covarianza común para todas las clases.\n","    \"\"\"\n","\n","    def _fit_params(self, X, y):\n","        \"\"\"\n","        Ajusta los parámetros del modelo LDA.\n","        Calcula la matriz de covarianza común y las medias de cada clase.\n","        \"\"\"\n","        # Número de clases\n","        num_classes = len(self.log_a_priori)\n","\n","        # Calcular la media de cada clase\n","        self.means = [X[:, y.flatten() == idx].mean(axis=1, keepdims=True)\n","                      for idx in range(num_classes)]\n","\n","        # Calcular la matriz de covarianza común\n","        # Inicializar matriz de covarianza común\n","        cov = np.zeros((X.shape[0], X.shape[0]))\n","\n","        for idx in range(num_classes):\n","            X_class = X[:, y.flatten() == idx]\n","            cov += np.cov(X_class, bias=True) * X_class.shape[1]\n","\n","        cov /= X.shape[1]  # Dividir por el número total de observaciones\n","        self.inv_cov = inv(cov)\n","\n","    def _predict_log_conditional(self, x, class_idx):\n","        \"\"\"\n","        Predice el logaritmo de la probabilidad condicional de x dado class_idx.\n","        \"\"\"\n","        unbiased_x = x - self.means[class_idx]\n","        return -0.5 * unbiased_x.T @ self.inv_cov @ unbiased_x\n"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1717803851357,"user":{"displayName":"Santi Olaciregui","userId":"08004037901086979441"},"user_tz":180},"id":"N5DRQeR7k2Y7"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"aZ9vYyWcb2x1"},"source":["# Código para pruebas"]},{"cell_type":"markdown","metadata":{"id":"SBZD2PMFdkVg"},"source":["Seteamos los datos"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1717803851357,"user":{"displayName":"Santi Olaciregui","userId":"08004037901086979441"},"user_tz":180},"id":"Mjyid9dBb-DZ"},"outputs":[],"source":["# hiperparámetros\n","rng_seed = 2000"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":43651,"status":"ok","timestamp":1717803895453,"user":{"displayName":"Santi Olaciregui","userId":"08004037901086979441"},"user_tz":180},"id":"451U-AtccHtJ","outputId":"023897e6-c5b4-4550-a9e1-878dec6acf9a"},"outputs":[{"output_type":"stream","name":"stdout","text":["X: (342, 4), Y:(342, 1)\n"]}],"source":["from sklearn.datasets import load_iris, fetch_openml\n","\"\"\"\n","proporciona dos funciones para cargar conjuntos de datos: uno para el conjunto\n","de datos de Iris y otro para un conjunto de datos de pingüinos.\n","\"\"\"\n","\n","def get_iris_dataset():\n","  \"\"\"\n","  Carga el conjunto de datos de Iris.\n","  paso a paso:\n","  Utiliza load_iris de sklearn.datasets para cargar los datos.\n","  Extrae las características (X_full) y las etiquetas (y_full) del conjunto\n","  de datos cargado.\n","  Convierte las etiquetas numéricas a sus nombres correspondientes\n","  (e.g., de 0, 1, 2 a \"setosa\", \"versicolor\", \"virginica\").\n","  Devuelve las características y las etiquetas.\n","  \"\"\"\n","  data = load_iris()\n","  X_full = data.data\n","  y_full = np.array([data.target_names[y] for y in data.target.reshape(-1,1)])\n","  return X_full, y_full\n","\n","def get_penguins():\n","  \"\"\"\n","  Carga el conjunto de datos de pingüinos.\n","  paso a paso:\n","  Utiliza fetch_openml de sklearn.datasets para obtener el conjunto de datos de pingüinos.\n","  Selecciona las características y las etiquetas del conjunto de datos.\n","  Elimina columnas no numéricas (\"island\" y \"sex\").\n","  Elimina filas con valores faltantes.\n","  Devuelve las características y las etiquetas.\n","  \"\"\"\n","  # get data\n","  df, tgt = fetch_openml(name=\"penguins\", return_X_y=True, as_frame=True, parser='auto')\n","  # drop non-numeric columns\n","  df.drop(columns=[\"island\",\"sex\"], inplace=True)\n","  # drop rows with missing values\n","  mask = df.isna().sum(axis=1) == 0\n","  df = df[mask]\n","  tgt = tgt[mask]\n","  return df.values, tgt.to_numpy().reshape(-1,1)\n","\n","# showing for iris\n","X_full, y_full = get_penguins()\n","\n","print(f\"X: {X_full.shape}, Y:{y_full.shape}\")"]},{"cell_type":"markdown","metadata":{"id":"bY8Le0LPGYbU"},"source":["Separamos el dataset en train y test para medir performance"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1717803895453,"user":{"displayName":"Santi Olaciregui","userId":"08004037901086979441"},"user_tz":180},"id":"EVV6Yesadv5Z","outputId":"c884fe42-bfc3-4739-aa64-18278d7f8a3f"},"outputs":[{"output_type":"stream","name":"stdout","text":["(4, 239) (1, 239) (4, 103) (1, 103)\n"]}],"source":["# preparing data, train - test validation\n","# 70-30 split\n","from sklearn.model_selection import train_test_split\n","\n","def split_transpose(X, y, test_sz, random_state):\n","  \"\"\"\n","  Descripción: Divide los datos en conjuntos de train y test utilizando una\n","  división de 70-30 (train-test split), y luego transpone las matrices para que\n","  las observaciones sean columnas en lugar de filas.\n","  Parámetros:\n","  X: Matriz de características.\n","  y: Vector de etiquetas.\n","  test_sz: Tamaño del conjunto de test (proporción).\n","  random_state: Semilla aleatoria para reproducibilidad.\n","  Devuelve:\n","  X_train: Matriz de características de train transpuesta.\n","  y_train: Vector de etiquetas de train transpuesto.\n","  X_test: Matriz de características de test transpuesta.\n","  y_test: Vector de etiquetas de test transpuesto.\n","  \"\"\"\n","  # split\n","  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=random_state)\n","  # transpose so observations are column vectors\n","  return X_train.T, y_train.T, X_test.T, y_test.T\n","\n","def accuracy(y_true, y_pred):\n","  \"\"\"\n","  Descripción: Calcula la precisión de las predicciones comparando las etiquetas\n","  verdaderas con las predichas.\n","  Parámetros:\n","  y_true: Vector de etiquetas verdaderas.\n","  y_pred: Vector de etiquetas predichas.\n","  Devuelve:\n","  La precisión de las predicciones.\n","  \"\"\"\n","  return (y_true == y_pred).mean()\n","\n","train_x, train_y, test_x, test_y = split_transpose(X_full, y_full, 0.3, rng_seed)\n","\n","print(train_x.shape, train_y.shape, test_x.shape, test_y.shape)"]},{"cell_type":"markdown","metadata":{"id":"V5TxHukig1cD"},"source":["Entrenamos un LDA y medimos su accuracy"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1717803895454,"user":{"displayName":"Santi Olaciregui","userId":"08004037901086979441"},"user_tz":180},"id":"-jdOe1fOg6cF"},"outputs":[],"source":["lda = LDA()\n","\n","lda.fit(train_x, train_y)"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1717803895454,"user":{"displayName":"Santi Olaciregui","userId":"08004037901086979441"},"user_tz":180},"id":"O7126rs2oxnS","outputId":"a970a46c-ca85-4953-b9b6-d5d87cdb5852"},"outputs":[{"output_type":"stream","name":"stdout","text":["Train (apparent) error is 0.0084 while test error is 0.0194\n"]}],"source":["train_acc = accuracy(train_y, lda.predict(train_x))\n","test_acc = accuracy(test_y, lda.predict(test_x))\n","print(f\"Train (apparent) error is {1-train_acc:.4f} while test error is {1-test_acc:.4f}\")"]},{"cell_type":"markdown","source":["# **Ejercicio 2**"],"metadata":{"id":"25SXJGeTGMBX"}},{"cell_type":"code","execution_count":10,"metadata":{"id":"Oanjd2mwFUcT","executionInfo":{"status":"ok","timestamp":1717803895454,"user_tz":180,"elapsed":4,"user":{"displayName":"Santi Olaciregui","userId":"08004037901086979441"}}},"outputs":[],"source":["class BaseBayesianClassifier:\n","  \"\"\"\n","  Implementación base para un clasificador Bayesiano.\n","  Contiene métodos para ajustar el modelo y predecir clases basándose\n","  en probabilidades a priori y condicionales.\n","  \"\"\"\n","\n","  def __init__(self):\n","    \"\"\"\n","    Inicializa el clasificador creando un objeto ClassEncoder para codificar\n","    etiquetas categóricas en números enteros.\n","    \"\"\"\n","    self.encoder = ClassEncoder()\n","\n","  def _estimate_a_priori(self, y):\n","    \"\"\"\n","    Estima las probabilidades a priori para cada clase.\n","    paso a paso:\n","    np.bincount(y.flatten().astype(int)) cuenta el número de ocurrencias de cada valor entero en y.\n","    Divide por y.size para obtener las frecuencias relativas (probabilidades).\n","    Devuelve el logaritmo natural de estas probabilidades.\n","    \"\"\"\n","    # Obtener el número de clases en y: 3 en este caso\n","    num_classes = len(np.unique(y))\n","    print(num_classes)\n","    # Crear un array con probabilidades a priori: 0.90, 0.05 y 0.05\n","    a_priori = np.array([0.90, 0.05, 0.05])\n","    print(a_priori)\n","    # Q3: para que sirve bincount?\n","    return np.log(a_priori)\n","\n","  def _fit_params(self, X, y):\n","    \"\"\"\n","    Método abstracto que debe ser implementado por una subclase para ajustar\n","    los parámetros del modelo específico.\n","    paso a paso:\n","    No tiene funcionalidad en esta clase base, solo establece que las subclases\n","    deben implementar este método.\n","    \"\"\"\n","    # estimate all needed parameters for given model\n","    raise NotImplementedError()\n","\n","  def _predict_log_conditional(self, x, class_idx):\n","    \"\"\"\n","    Método abstracto que debe ser implementado por una subclase para predecir\n","    la probabilidad condicional logarítmica.\n","    paso a paso:\n","    No tiene funcionalidad en esta clase base, solo establece que las subclases\n","    deben implementar este método.\n","    \"\"\"\n","    # predict the log(P(x|G=class_idx)), the log of the conditional probability of x given the class\n","    # this should depend on the model used\n","    raise NotImplementedError()\n","\n","  def fit(self, X, y, a_priori=None):\n","    \"\"\"\n","    Ajusta el clasificador a los datos X y y.\n","    paso a paso:\n","    Codifica las etiquetas y usando ClassEncoder.\n","    Estima las probabilidades a priori si no se proporcionan.\n","    Verifica que las probabilidades a priori coincidan con el número de clases.\n","    Llama al método _fit_params para ajustar los parámetros del modelo específico.\n","    \"\"\"\n","    # first encode the classes\n","    y = self.encoder.fit_transform(y)\n","    # if it's needed, estimate a priori probabilities\n","    self.log_a_priori = self._estimate_a_priori(y) if a_priori is None else np.log(a_priori)\n","    # check that a_priori has the correct number of classes\n","    assert len(self.log_a_priori) == len(self.encoder.names), \"A priori probabilities do not match number of classes\"\n","    # now that everything else is in place, estimate all needed parameters for given model\n","    self._fit_params(X, y)\n","    # Q4: por que el _fit_params va al final? no se puede mover a, por ejemplo, antes de la priori?\n","\n","  def predict(self, X):\n","    \"\"\"\n","    Predice las etiquetas para las observaciones en X.\n","    paso a paso:\n","    Inicializa un array vacío y_hat para almacenar las predicciones.\n","    Itera sobre cada observación en X, predice su clase usando _predict_one,\n","    y almacena el resultado decodificado en y_hat.\n","    Devuelve y_hat como un vector fila.\n","    \"\"\"\n","    # this is actually an individual prediction encased in a for-loop\n","    m_obs = X.shape[1]\n","    y_hat = np.empty(m_obs, dtype=self.encoder.fmt)\n","    for i in range(m_obs):\n","      encoded_y_hat_i = self._predict_one(X[:,i].reshape(-1,1))\n","      y_hat[i] = self.encoder.names[encoded_y_hat_i]\n","    # return prediction as a row vector (matching y)\n","    return y_hat.reshape(1,-1)\n","\n","  def _predict_one(self, x):\n","    \"\"\"\n","    Predice la clase para una única observación x.\n","    paso a paso:\n","    Calcula la probabilidad a posteriori logarítmica para cada clase sumando\n","    la probabilidad a priori logarítmica y la probabilidad condicional logarítmica.\n","    Devuelve el índice de la clase con la máxima probabilidad a posteriori.\n","    \"\"\"\n","    # calculate all log posteriori probabilities (actually, +C)\n","    log_posteriori = [ log_a_priori_i + self._predict_log_conditional(x, idx) for idx, log_a_priori_i\n","                  in enumerate(self.log_a_priori) ]\n","\n","    # return the class that has maximum a posteriori probability\n","    return np.argmax(log_posteriori)"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"NqMcWgiYFUcT","executionInfo":{"status":"ok","timestamp":1717803895454,"user_tz":180,"elapsed":4,"user":{"displayName":"Santi Olaciregui","userId":"08004037901086979441"}}},"outputs":[],"source":["class LDA(BaseBayesianClassifier):\n","    \"\"\"\n","    Clasifica los datos basándose en modelos Gaussianos con una matriz de\n","    covarianza común para todas las clases.\n","    \"\"\"\n","\n","    def _fit_params(self, X, y):\n","        \"\"\"\n","        Ajusta los parámetros del modelo LDA.\n","        Calcula la matriz de covarianza común y las medias de cada clase.\n","        \"\"\"\n","        # Número de clases\n","        num_classes = len(self.log_a_priori)\n","\n","        # Calcular la media de cada clase\n","        self.means = [X[:, y.flatten() == idx].mean(axis=1, keepdims=True)\n","                      for idx in range(num_classes)]\n","\n","        # Calcular la matriz de covarianza común\n","        # Inicializar matriz de covarianza común\n","        cov = np.zeros((X.shape[0], X.shape[0]))\n","\n","        for idx in range(num_classes):\n","            X_class = X[:, y.flatten() == idx]\n","            cov += np.cov(X_class, bias=True) * X_class.shape[1]\n","\n","        cov /= X.shape[1]  # Dividir por el número total de observaciones\n","        self.inv_cov = inv(cov)\n","\n","    def _predict_log_conditional(self, x, class_idx):\n","        \"\"\"\n","        Predice el logaritmo de la probabilidad condicional de x dado class_idx.\n","        \"\"\"\n","        unbiased_x = x - self.means[class_idx]\n","        return -0.5 * unbiased_x.T @ self.inv_cov @ unbiased_x"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CHxeS78AFUcT","executionInfo":{"status":"ok","timestamp":1717803895454,"user_tz":180,"elapsed":3,"user":{"displayName":"Santi Olaciregui","userId":"08004037901086979441"}},"outputId":"3b9adb27-d4dc-4d50-a3c2-8293bb2bd1dc"},"outputs":[{"output_type":"stream","name":"stdout","text":["X: (150, 4), Y:(150, 1)\n","(4, 105) (1, 105) (4, 45) (1, 45)\n"]}],"source":["X_full, y_full = get_iris_dataset()\n","\n","print(f\"X: {X_full.shape}, Y:{y_full.shape}\")\n","\n","\n","# preparing data, train - test validation\n","# 70-30 split\n","from sklearn.model_selection import train_test_split\n","\n","def split_transpose(X, y, test_sz, random_state):\n","  \"\"\"\n","  Descripción: Divide los datos en conjuntos de train y test utilizando una\n","  división de 70-30 (train-test split), y luego transpone las matrices para que\n","  las observaciones sean columnas en lugar de filas.\n","  Parámetros:\n","  X: Matriz de características.\n","  y: Vector de etiquetas.\n","  test_sz: Tamaño del conjunto de test (proporción).\n","  random_state: Semilla aleatoria para reproducibilidad.\n","  Devuelve:\n","  X_train: Matriz de características de train transpuesta.\n","  y_train: Vector de etiquetas de train transpuesto.\n","  X_test: Matriz de características de test transpuesta.\n","  y_test: Vector de etiquetas de test transpuesto.\n","  \"\"\"\n","  # split\n","  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=random_state)\n","  # transpose so observations are column vectors\n","  return X_train.T, y_train.T, X_test.T, y_test.T\n","\n","def accuracy(y_true, y_pred):\n","  \"\"\"\n","  Descripción: Calcula la precisión de las predicciones comparando las etiquetas\n","  verdaderas con las predichas.\n","  Parámetros:\n","  y_true: Vector de etiquetas verdaderas.\n","  y_pred: Vector de etiquetas predichas.\n","  Devuelve:\n","  La precisión de las predicciones.\n","  \"\"\"\n","  return (y_true == y_pred).mean()\n","\n","train_x, train_y, test_x, test_y = split_transpose(X_full, y_full, 0.3, rng_seed)\n","\n","print(train_x.shape, train_y.shape, test_x.shape, test_y.shape)"]},{"cell_type":"code","source":["lda = LDA()\n","lda.fit(train_x, train_y)\n","\n","train_acc = accuracy(train_y, lda.predict(train_x))\n","test_acc = accuracy(test_y, lda.predict(test_x))\n","print(f\"Train (apparent) error is {1-train_acc:.4f} while test error is {1-test_acc:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XDC2CjzIGEMt","executionInfo":{"status":"ok","timestamp":1717803975988,"user_tz":180,"elapsed":3,"user":{"displayName":"Santi Olaciregui","userId":"08004037901086979441"}},"outputId":"93d35522-d952-473d-da50-7ebb979e287d"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["3\n","[0.9  0.05 0.05]\n","Train (apparent) error is 0.0095 while test error is 0.0444\n"]}]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}