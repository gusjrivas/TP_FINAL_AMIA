{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Implementación base\n","5. Estimar y comparar los tiempos de predicción de las clases `QDA` y `TensorizedQDA`. De haber diferencias ¿Cuáles pueden ser las causas?"],"metadata":{"id":"RIuUO15QGc0L"}},{"cell_type":"markdown","source":["Optimización de operaciones vectorizadas: La clase TensorizedQDA parece estar optimizada para operaciones vectorizadas, lo que puede mejorar significativamente el rendimiento en comparación con la implementación regular. Al utilizar operaciones vectorizadas en lugar de bucles explícitos, se puede aprovechar la capacidad de cómputo paralelo de la CPU, lo que puede acelerar el procesamiento.\n","\n","Reducción de bucles explícitos: La clase TensorizedQDA podría estar diseñada para minimizar la cantidad de bucles explícitos, lo que generalmente es más eficiente en Python. Los bucles explícitos tienden a ser más lentos que las operaciones vectorizadas debido al sobrecoste de la iteración en Python.\n","\n","Optimización de almacenamiento de datos: Al usar tensores en lugar de matrices y vectores separados, la clase TensorizedQDA puede optimizar el almacenamiento y el acceso a los datos, lo que puede reducir la sobrecarga asociada con la gestión de múltiples estructuras de datos.\n","\n","Reducción de la sobrecarga de llamadas a métodos: La clase TensorizedQDA puede minimizar la cantidad de llamadas a métodos y funciones, lo que puede reducir la sobrecarga asociada con la llamada y retorno de funciones en Python\n","\n","En resumen, la reducción en el tiempo de predicción al utilizar la clase TensorizedQDA en lugar de la clase regular QDA puede deberse a una combinación de optimizaciones de código, incluyendo operaciones vectorizadas, reducción de bucles explícitos y optimización de almacenamiento de datos. Estas optimizaciones pueden mejorar significativamente el rendimiento del modelo y reducir el tiempo de ejecución de predicción."],"metadata":{"id":"KwpbU953J4W-"}},{"cell_type":"code","source":["# Se importan las librerias necesarias\n","import numpy as np\n","from numpy.linalg import det, inv/Users/santiolaciregui/Documents/Especializacion UBA/AMIA/TP_final_AMIA/tp_final_amia_inciso_1.1.ipynb"],"metadata":{"id":"T__eZOdyGbdS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Modelo"],"metadata":{"id":"TaaPO0evcBDD"}},{"cell_type":"code","source":["class ClassEncoder:\n","  \"\"\"\n","  Permite codificar etiquetas categóricas en valores numéricos y decodificarlos\n","  de vuelta a sus etiquetas originales.\n","  \"\"\"\n","\n","  def fit(self, y):\n","    \"\"\"\n","    Ajusta el codificador a las etiquetas proporcionadas.\n","    paso a paso:\n","    np.unique(y) encuentra las etiquetas únicas en y y las almacena en self.names.\n","    Crea un diccionario self.name_to_class que asigna a cada etiqueta única un índice numérico.\n","    Almacena el tipo de datos de y en self.fmt.\n","    \"\"\"\n","    self.names = np.unique(y)\n","    self.name_to_class = {name:idx for idx, name in enumerate(self.names)}\n","    self.fmt = y.dtype\n","    # Q1: por que no hace falta definir un class_to_name para el mapeo inverso?\n","\n","  def _map_reshape(self, f, arr):\n","    \"\"\"\n","    Descripción: Aplica una función f a cada elemento de un array arr y luego lo remodela a su forma original.\n","    paso a paso:\n","    arr.flatten() aplana el array a una dimensión.\n","    Aplica la función f a cada elemento del array aplanado.\n","    Convierte el resultado en un array de NumPy y lo remodela a la forma original de arr.\n","    \"\"\"\n","    return np.array([f(elem) for elem in arr.flatten()]).reshape(arr.shape)\n","    # Q2: por que hace falta un reshape?\n","\n","  def transform(self, y):\n","    \"\"\"\n","    Transforma las etiquetas en números enteros usando el mapeo definido en fit.\n","    paso a paso:\n","    Utiliza _map_reshape para aplicar el diccionario self.name_to_class a cada etiqueta en y,\n","    convirtiéndolas en sus correspondientes valores numéricos.\n","    \"\"\"\n","    return self._map_reshape(lambda name: self.name_to_class[name], y)\n","\n","  def fit_transform(self, y):\n","    \"\"\"\n","    Ajusta el codificador a las etiquetas y luego las transforma en una sola operación.\n","    paso a paso:\n","    Llama a fit(y) para ajustar el codificador.\n","    Luego llama a transform(y) para transformar las etiquetas ajustadas.\n","    \"\"\"\n","    self.fit(y)\n","    return self.transform(y)\n","\n","  def detransform(self, y_hat):\n","    \"\"\"\n","    Convierte los valores numéricos de vuelta a sus etiquetas originales.\n","    paso a paso:\n","    Utiliza _map_reshape para aplicar self.names a cada índice en y_hat,\n","    convirtiéndolos de vuelta a sus etiquetas originales.\n","    \"\"\"\n","    return self._map_reshape(lambda idx: self.names[idx], y_hat)"],"metadata":{"id":"6tumZFxEH2w7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class BaseBayesianClassifier:\n","  \"\"\"\n","  Implementación base para un clasificador Bayesiano.\n","  Contiene métodos para ajustar el modelo y predecir clases basándose\n","  en probabilidades a priori y condicionales.\n","  \"\"\"\n","\n","  def __init__(self):\n","    \"\"\"\n","    Inicializa el clasificador creando un objeto ClassEncoder para codificar\n","    etiquetas categóricas en números enteros.\n","    \"\"\"\n","    self.encoder = ClassEncoder()\n","\n","  def _estimate_a_priori(self, y):\n","    \"\"\"\n","    Estima las probabilidades a priori para cada clase.\n","    paso a paso:\n","    np.bincount(y.flatten().astype(int)) cuenta el número de ocurrencias de cada valor entero en y.\n","    Divide por y.size para obtener las frecuencias relativas (probabilidades).\n","    Devuelve el logaritmo natural de estas probabilidades.\n","    \"\"\"\n","    # Obtener el número de clases en y: 3 en este caso\n","    num_classes = len(np.unique(y))\n","    # Crear un array con probabilidades a priori iguales: 1/3 en este caso\n","    a_priori = np.full(num_classes, 1/num_classes)\n","    # Q3: para que sirve bincount?\n","    return np.log(a_priori)\n","\n","  def _fit_params(self, X, y):\n","    \"\"\"\n","    Método abstracto que debe ser implementado por una subclase para ajustar\n","    los parámetros del modelo específico.\n","    paso a paso:\n","    No tiene funcionalidad en esta clase base, solo establece que las subclases\n","    deben implementar este método.\n","    \"\"\"\n","    # estimate all needed parameters for given model\n","    raise NotImplementedError()\n","\n","  def _predict_log_conditional(self, x, class_idx):\n","    \"\"\"\n","    Método abstracto que debe ser implementado por una subclase para predecir\n","    la probabilidad condicional logarítmica.\n","    paso a paso:\n","    No tiene funcionalidad en esta clase base, solo establece que las subclases\n","    deben implementar este método.\n","    \"\"\"\n","    # predict the log(P(x|G=class_idx)), the log of the conditional probability of x given the class\n","    # this should depend on the model used\n","    raise NotImplementedError()\n","\n","  def fit(self, X, y, a_priori=None):\n","    \"\"\"\n","    Ajusta el clasificador a los datos X y y.\n","    paso a paso:\n","    Codifica las etiquetas y usando ClassEncoder.\n","    Estima las probabilidades a priori si no se proporcionan.\n","    Verifica que las probabilidades a priori coincidan con el número de clases.\n","    Llama al método _fit_params para ajustar los parámetros del modelo específico.\n","    \"\"\"\n","    # first encode the classes\n","    y = self.encoder.fit_transform(y)\n","    # if it's needed, estimate a priori probabilities\n","    self.log_a_priori = self._estimate_a_priori(y) if a_priori is None else np.log(a_priori)\n","    # check that a_priori has the correct number of classes\n","    assert len(self.log_a_priori) == len(self.encoder.names), \"A priori probabilities do not match number of classes\"\n","    # now that everything else is in place, estimate all needed parameters for given model\n","    self._fit_params(X, y)\n","    # Q4: por que el _fit_params va al final? no se puede mover a, por ejemplo, antes de la priori?\n","\n","  def predict(self, X):\n","    \"\"\"\n","    Predice las etiquetas para las observaciones en X.\n","    paso a paso:\n","    Inicializa un array vacío y_hat para almacenar las predicciones.\n","    Itera sobre cada observación en X, predice su clase usando _predict_one,\n","    y almacena el resultado decodificado en y_hat.\n","    Devuelve y_hat como un vector fila.\n","    \"\"\"\n","    # this is actually an individual prediction encased in a for-loop\n","    m_obs = X.shape[1]\n","    y_hat = np.empty(m_obs, dtype=self.encoder.fmt)\n","    for i in range(m_obs):\n","      encoded_y_hat_i = self._predict_one(X[:,i].reshape(-1,1))\n","      y_hat[i] = self.encoder.names[encoded_y_hat_i]\n","    # return prediction as a row vector (matching y)\n","    return y_hat.reshape(1,-1)\n","\n","  def _predict_one(self, x):\n","    \"\"\"\n","    Predice la clase para una única observación x.\n","    paso a paso:\n","    Calcula la probabilidad a posteriori logarítmica para cada clase sumando\n","    la probabilidad a priori logarítmica y la probabilidad condicional logarítmica.\n","    Devuelve el índice de la clase con la máxima probabilidad a posteriori.\n","    \"\"\"\n","    # calculate all log posteriori probabilities (actually, +C)\n","    log_posteriori = [ log_a_priori_i + self._predict_log_conditional(x, idx) for idx, log_a_priori_i\n","                  in enumerate(self.log_a_priori) ]\n","\n","    # return the class that has maximum a posteriori probability\n","    return np.argmax(log_posteriori)"],"metadata":{"id":"pMsyCGFKQGdk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class QDA(BaseBayesianClassifier):\n","  \"\"\"\n","  Clasifica los datos basándose en modelos Gaussianos con diferentes matrices\n","  de covarianza para cada clase.\n","  \"\"\"\n","\n","  def _fit_params(self, X, y):\n","    \"\"\"\n","    flatten, se asegura que y sea un array unidimensional, lo cual es necesario\n","    para hacer comparaciones elementales como y == idx. Sin flatten,\n","    y == idx puede no funcionar correctamente si y tiene más de una dimensión.\n","    bias=True en np.cov ajusta el cálculo de la matriz de covarianza dividiendo\n","    por N (número de observaciones) en lugar de N-1. Esto proporciona una\n","    estimación de la covarianza basada en la población en lugar de la muestra.\n","    X es una matriz donde las filas representan características y las columnas\n","    representan observaciones. Calcular la media a lo largo de las columnas\n","    (axis=1) proporciona la media de cada característica para cada clase.\n","    \"\"\"\n","    # estimate each covariance matrix\n","    self.inv_covs = [inv(np.cov(X[:,y.flatten()==idx], bias=True))\n","                      for idx in range(len(self.log_a_priori))]\n","    # Q5: por que hace falta el flatten y no se puede directamente X[:,y==idx]?\n","    # Q6: por que se usa bias=True en vez del default bias=False?\n","    self.means = [X[:,y.flatten()==idx].mean(axis=1, keepdims=True)\n","                  for idx in range(len(self.log_a_priori))]\n","    # Q7: que hace axis=1? por que no axis=0?\n","\n","  def _predict_log_conditional(self, x, class_idx):\n","    \"\"\"\n","    Predice el logaritmo de la probabilidad condicional de x dado class_idx.\n","    paso a paso:\n","    Obtiene la matriz de covarianza inversa para la clase class_idx.\n","    Calcula unbiased_x restando la media de la clase de x.\n","    Calcula el logaritmo de la probabilidad condicional usando la fórmula del\n","    modelo Gaussiano cuadrático.\n","    \"\"\"\n","    # predict the log(P(x|G=class_idx)), the log of the conditional probability of x given the class\n","    # this should depend on the model used\n","    inv_cov = self.inv_covs[class_idx]\n","    unbiased_x =  x - self.means[class_idx]\n","    return 0.5*np.log(det(inv_cov)) -0.5 * unbiased_x.T @ inv_cov @ unbiased_x"],"metadata":{"id":"jqoDsDSlZBXy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class TensorizedQDA(QDA):\n","\n","    def _fit_params(self, X, y):\n","        # ask plain QDA to fit params\n","        super()._fit_params(X,y)\n","\n","        # stack onto new dimension\n","        self.tensor_inv_cov = np.stack(self.inv_covs)\n","        self.tensor_means = np.stack(self.means)\n","\n","    def _predict_log_conditionals(self,x):\n","        unbiased_x = x - self.tensor_means\n","        inner_prod = unbiased_x.transpose(0,2,1) @ self.tensor_inv_cov @ unbiased_x\n","\n","        return 0.5*np.log(det(self.tensor_inv_cov)) - 0.5 * inner_prod.flatten()\n","\n","    def _predict_one(self, x):\n","        # return the class that has maximum a posteriori probability\n","        return np.argmax(self.log_a_priori + self._predict_log_conditionals(x))"],"metadata":{"id":"kpsJ3BWJ1UYN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Código para pruebas"],"metadata":{"id":"aZ9vYyWcb2x1"}},{"cell_type":"markdown","source":["Seteamos los datos"],"metadata":{"id":"SBZD2PMFdkVg"}},{"cell_type":"code","source":["# hiperparámetros\n","rng_seed = 2000"],"metadata":{"id":"Mjyid9dBb-DZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.datasets import load_iris, fetch_openml\n","\"\"\"\n","proporciona dos funciones para cargar conjuntos de datos: uno para el conjunto\n","de datos de Iris y otro para un conjunto de datos de pingüinos.\n","\"\"\"\n","\n","def get_iris_dataset():\n","  \"\"\"\n","  Carga el conjunto de datos de Iris.\n","  paso a paso:\n","  Utiliza load_iris de sklearn.datasets para cargar los datos.\n","  Extrae las características (X_full) y las etiquetas (y_full) del conjunto\n","  de datos cargado.\n","  Convierte las etiquetas numéricas a sus nombres correspondientes\n","  (e.g., de 0, 1, 2 a \"setosa\", \"versicolor\", \"virginica\").\n","  Devuelve las características y las etiquetas.\n","  \"\"\"\n","  data = load_iris()\n","  X_full = data.data\n","  y_full = np.array([data.target_names[y] for y in data.target.reshape(-1,1)])\n","  return X_full, y_full\n","\n","def get_penguins():\n","  \"\"\"\n","  Carga el conjunto de datos de pingüinos.\n","  paso a paso:\n","  Utiliza fetch_openml de sklearn.datasets para obtener el conjunto de datos de pingüinos.\n","  Selecciona las características y las etiquetas del conjunto de datos.\n","  Elimina columnas no numéricas (\"island\" y \"sex\").\n","  Elimina filas con valores faltantes.\n","  Devuelve las características y las etiquetas.\n","  \"\"\"\n","  # get data\n","  df, tgt = fetch_openml(name=\"penguins\", return_X_y=True, as_frame=True, parser='auto')\n","  # drop non-numeric columns\n","  df.drop(columns=[\"island\",\"sex\"], inplace=True)\n","  # drop rows with missing values\n","  mask = df.isna().sum(axis=1) == 0\n","  df = df[mask]\n","  tgt = tgt[mask]\n","  return df.values, tgt.to_numpy().reshape(-1,1)\n","\n","# showing for iris\n","X_full, y_full = get_iris_dataset()\n","\n","print(f\"X: {X_full.shape}, Y:{y_full.shape}\")"],"metadata":{"id":"451U-AtccHtJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1717617115746,"user_tz":180,"elapsed":2,"user":{"displayName":"Fabricio Lopretto","userId":"02987429057712364816"}},"outputId":"e44a226b-2307-4f41-9056-832bec46afbc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["X: (150, 4), Y:(150, 1)\n"]}]},{"cell_type":"code","source":["# peek data matrix\n","X_full[:5]"],"metadata":{"id":"M7JEi3MadbV3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# peek target vector\n","y_full[:5]"],"metadata":{"id":"ullZwwF-deHr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Separamos el dataset en train y test para medir performance"],"metadata":{"id":"bY8Le0LPGYbU"}},{"cell_type":"code","source":["# preparing data, train - test validation\n","# 70-30 split\n","from sklearn.model_selection import train_test_split\n","\n","def split_transpose(X, y, test_sz, random_state):\n","  \"\"\"\n","  Descripción: Divide los datos en conjuntos de entrenamiento y prueba utilizando una\n","  división de 70-30 (train-test split), y luego transpone las matrices para que\n","  las observaciones sean columnas en lugar de filas.\n","  Parámetros:\n","  X: Matriz de características.\n","  y: Vector de etiquetas.\n","  test_sz: Tamaño del conjunto de prueba (proporción).\n","  random_state: Semilla aleatoria para reproducibilidad.\n","  Devuelve:\n","  X_train: Matriz de características de entrenamiento transpuesta.\n","  y_train: Vector de etiquetas de entrenamiento transpuesto.\n","  X_test: Matriz de características de prueba transpuesta.\n","  y_test: Vector de etiquetas de prueba transpuesto.\n","  \"\"\"\n","  # split\n","  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=random_state)\n","  # transpose so observations are column vectors\n","  return X_train.T, y_train.T, X_test.T, y_test.T\n","\n","def accuracy(y_true, y_pred):\n","  \"\"\"\n","  Descripción: Calcula la precisión de las predicciones comparando las etiquetas verdaderas con las predichas.\n","  Parámetros:\n","  y_true: Vector de etiquetas verdaderas.\n","  y_pred: Vector de etiquetas predichas.\n","  Devuelve:\n","  La precisión de las predicciones.\n","  \"\"\"\n","  return (y_true == y_pred).mean()\n","\n","train_x, train_y, test_x, test_y = split_transpose(X_full, y_full, 0.3, rng_seed)\n","\n","print(train_x.shape, train_y.shape, test_x.shape, test_y.shape)"],"metadata":{"id":"EVV6Yesadv5Z","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1717617120471,"user_tz":180,"elapsed":404,"user":{"displayName":"Fabricio Lopretto","userId":"02987429057712364816"}},"outputId":"35fa1a7d-e9bd-45b9-f5de-975d5a403ff7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(4, 90) (1, 90) (4, 60) (1, 60)\n"]}]},{"cell_type":"markdown","source":["Entrenamos un QDA y medimos su accuracy"],"metadata":{"id":"V5TxHukig1cD"}},{"cell_type":"code","source":["#qda = QDA()\n","qda = TensorizedQDA()\n","\n","qda.fit(train_x, train_y)"],"metadata":{"id":"-jdOe1fOg6cF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_acc = accuracy(train_y, qda.predict(train_x))\n","test_acc = accuracy(test_y, qda.predict(test_x))\n","print(f\"Train (apparent) error is {1-train_acc:.4f} while test error is {1-test_acc:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q0sumlC51tIX","executionInfo":{"status":"ok","timestamp":1717617176402,"user_tz":180,"elapsed":409,"user":{"displayName":"Fabricio Lopretto","userId":"02987429057712364816"}},"outputId":"a49d3215-19fd-4ea5-e48b-7f08b7a266da"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Train (apparent) error is 0.0111 while test error is 0.0833\n"]}]},{"cell_type":"markdown","source":["Tiempo ciclo completo de QDA/TQDA"],"metadata":{"id":"P1ExgSoP7U-e"}},{"cell_type":"code","source":["%%timeit\n","\n","qda.predict(test_x)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r_wOqpXr1wiH","executionInfo":{"status":"ok","timestamp":1717617290661,"user_tz":180,"elapsed":14636,"user":{"displayName":"Fabricio Lopretto","userId":"02987429057712364816"}},"outputId":"63c4fb6c-b27f-4d91-9021-70f07a654ea5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1.8 ms ± 237 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"]}]},{"cell_type":"markdown","source":["Tiempo inferencia (predicción)"],"metadata":{"id":"as-PWqCE7bm5"}},{"cell_type":"code","source":["%%timeit\n","\n","#model = QDA()\n","model = TensorizedQDA()\n","model.fit(train_x, train_y)\n","model.predict(test_x)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z5ari18W12r2","executionInfo":{"status":"ok","timestamp":1717618990985,"user_tz":180,"elapsed":3043,"user":{"displayName":"Fabricio Lopretto","userId":"02987429057712364816"}},"outputId":"7212734e-2ff7-4e47-e6eb-55e9fff4c421"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["3.19 ms ± 94.3 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"]}]}]}