{"cells":[{"cell_type":"markdown","metadata":{"id":"RIuUO15QGc0L"},"source":["## Implementación base\n","1. Entrenar un modelo QDA sobre el dataset *iris* utilizando las distribuciones *a priori* a continuación ¿Se observan diferencias?¿Por qué cree? _Pista: comparar con las distribuciones del dataset completo, **sin splitear**_.\n","    1. Uniforme (cada clase tiene probabilidad 1/3)\n","    2. Una clase con probabilidad 0.9, las demás 0.05 (probar las 3 combinaciones)\n","2. Repetir el punto anterior para el dataset *penguin*.\n","\n","Cuando se cambian las probabilidades a priori, el modelo ajustado (QDA en este caso) cambia la manera en que pondera la probabilidad de cada clase al hacer predicciones.\n","\n","En el conjunto de entrenamiento, esto puede llevar a que las observaciones se clasifiquen de manera diferente debido a que el modelo ahora prioriza una clase distinta (por ejemplo, al pasar de priorizar la clase 1 a la clase 2).\n","\n","Dado que el modelo ha sido entrenado específicamente en estos datos, cualquier cambio en las probabilidades a priori afectará directamente la clasificación de las observaciones en el conjunto de entrenamiento, y por ende, el error de predicción en este conjunto.\n","\n","El conjunto de prueba es independiente del conjunto de entrenamiento. Su propósito es evaluar la capacidad del modelo de generalizar a datos nuevos.\n","Si las probabilidades a priori originales reflejan correctamente la distribución de clases en la población general, cambiar estas probabilidades no debería mejorar la capacidad del modelo para generalizar.\n","El error de predicción en el conjunto de prueba refleja la capacidad del modelo para predecir correctamente en datos no vistos. Si el cambio en las probabilidades a priori no mejora esta capacidad (porque las nuevas probabilidades no se alinean mejor con la distribución real de clases en los datos de prueba), el error no se verá afectado.\n","Esto sugiere que el cambio de las probabilidades a priori no mejora la capacidad del modelo para generalizar, sino que únicamente optimiza su desempeño en el conjunto de datos con el que fue entrenado.\n","\n","Resumiendo, el error de predicción en el conjunto de entrenamiento se modifica debido a que las probabilidades a priori afectan directamente cómo el modelo clasifica las observaciones en este conjunto. Sin embargo, el error en el conjunto de prueba podría no se modificarse porque este conjunto está diseñado para evaluar la capacidad de generalización del modelo. Si las nuevas probabilidades a priori no reflejan mejor la distribución real de clases en la población general, no habrá un cambio en la capacidad de generalización del modelo, y por tanto, el error de predicción en el conjunto de prueba permanecerá constante."]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1717802215817,"user":{"displayName":"Santi Olaciregui","userId":"08004037901086979441"},"user_tz":180},"id":"T__eZOdyGbdS"},"outputs":[],"source":["# Se importan las librerias necesarias\n","import numpy as np\n","from numpy.linalg import det, inv"]},{"cell_type":"markdown","metadata":{"id":"TaaPO0evcBDD"},"source":["# Modelo"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1717802215818,"user":{"displayName":"Santi Olaciregui","userId":"08004037901086979441"},"user_tz":180},"id":"6tumZFxEH2w7"},"outputs":[],"source":["class ClassEncoder:\n","  \"\"\"\n","  Permite codificar etiquetas categóricas en valores numéricos y decodificarlos\n","  de vuelta a sus etiquetas originales.\n","  \"\"\"\n","\n","  def fit(self, y):\n","    \"\"\"\n","    Ajusta el codificador a las etiquetas proporcionadas.\n","    paso a paso:\n","    np.unique(y) encuentra las etiquetas únicas en y y las almacena en self.names.\n","    Crea un diccionario self.name_to_class que asigna a cada etiqueta única un índice numérico.\n","    Almacena el tipo de datos de y en self.fmt.\n","    \"\"\"\n","    self.names = np.unique(y)\n","    self.name_to_class = {name:idx for idx, name in enumerate(self.names)}\n","    self.fmt = y.dtype\n","    # Q1: por que no hace falta definir un class_to_name para el mapeo inverso?\n","\n","  def _map_reshape(self, f, arr):\n","    \"\"\"\n","    Descripción: Aplica una función f a cada elemento de un array arr y luego lo remodela a su forma original.\n","    paso a paso:\n","    arr.flatten() aplana el array a una dimensión.\n","    Aplica la función f a cada elemento del array aplanado.\n","    Convierte el resultado en un array de NumPy y lo remodela a la forma original de arr.\n","    \"\"\"\n","    return np.array([f(elem) for elem in arr.flatten()]).reshape(arr.shape)\n","    # Q2: por que hace falta un reshape?\n","\n","  def transform(self, y):\n","    \"\"\"\n","    Transforma las etiquetas en números enteros usando el mapeo definido en fit.\n","    paso a paso:\n","    Utiliza _map_reshape para aplicar el diccionario self.name_to_class a cada etiqueta en y,\n","    convirtiéndolas en sus correspondientes valores numéricos.\n","    \"\"\"\n","    return self._map_reshape(lambda name: self.name_to_class[name], y)\n","\n","  def fit_transform(self, y):\n","    \"\"\"\n","    Ajusta el codificador a las etiquetas y luego las transforma en una sola operación.\n","    paso a paso:\n","    Llama a fit(y) para ajustar el codificador.\n","    Luego llama a transform(y) para transformar las etiquetas ajustadas.\n","    \"\"\"\n","    self.fit(y)\n","    return self.transform(y)\n","\n","  def detransform(self, y_hat):\n","    \"\"\"\n","    Convierte los valores numéricos de vuelta a sus etiquetas originales.\n","    paso a paso:\n","    Utiliza _map_reshape para aplicar self.names a cada índice en y_hat,\n","    convirtiéndolos de vuelta a sus etiquetas originales.\n","    \"\"\"\n","    return self._map_reshape(lambda idx: self.names[idx], y_hat)"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1717802215818,"user":{"displayName":"Santi Olaciregui","userId":"08004037901086979441"},"user_tz":180},"id":"pMsyCGFKQGdk"},"outputs":[],"source":["class BaseBayesianClassifier:\n","  \"\"\"\n","  Implementación base para un clasificador Bayesiano.\n","  Contiene métodos para ajustar el modelo y predecir clases basándose\n","  en probabilidades a priori y condicionales.\n","  \"\"\"\n","\n","  def __init__(self):\n","    \"\"\"\n","    Inicializa el clasificador creando un objeto ClassEncoder para codificar\n","    etiquetas categóricas en números enteros.\n","    \"\"\"\n","    self.encoder = ClassEncoder()\n","\n","  def _estimate_a_priori(self, y):\n","    \"\"\"\n","    Estima las probabilidades a priori para cada clase.\n","    paso a paso:\n","    np.bincount(y.flatten().astype(int)) cuenta el número de ocurrencias de cada valor entero en y.\n","    Divide por y.size para obtener las frecuencias relativas (probabilidades).\n","    Devuelve el logaritmo natural de estas probabilidades.\n","    \"\"\"\n","    # Obtener el número de clases en y: 3 en este caso\n","    num_classes = len(np.unique(y))\n","    # Crear un array con probabilidades a priori iguales: 1/3 en este caso\n","    a_priori = np.full(num_classes, 1/num_classes)\n","    # Q3: para que sirve bincount?\n","    return np.log(a_priori)\n","\n","  def _fit_params(self, X, y):\n","    \"\"\"\n","    Método abstracto que debe ser implementado por una subclase para ajustar\n","    los parámetros del modelo específico.\n","    paso a paso:\n","    No tiene funcionalidad en esta clase base, solo establece que las subclases\n","    deben implementar este método.\n","    \"\"\"\n","    # estimate all needed parameters for given model\n","    raise NotImplementedError()\n","\n","  def _predict_log_conditional(self, x, class_idx):\n","    \"\"\"\n","    Método abstracto que debe ser implementado por una subclase para predecir\n","    la probabilidad condicional logarítmica.\n","    paso a paso:\n","    No tiene funcionalidad en esta clase base, solo establece que las subclases\n","    deben implementar este método.\n","    \"\"\"\n","    # predict the log(P(x|G=class_idx)), the log of the conditional probability of x given the class\n","    # this should depend on the model used\n","    raise NotImplementedError()\n","\n","  def fit(self, X, y, a_priori=None):\n","    \"\"\"\n","    Ajusta el clasificador a los datos X y y.\n","    paso a paso:\n","    Codifica las etiquetas y usando ClassEncoder.\n","    Estima las probabilidades a priori si no se proporcionan.\n","    Verifica que las probabilidades a priori coincidan con el número de clases.\n","    Llama al método _fit_params para ajustar los parámetros del modelo específico.\n","    \"\"\"\n","    # first encode the classes\n","    y = self.encoder.fit_transform(y)\n","    # if it's needed, estimate a priori probabilities\n","    self.log_a_priori = self._estimate_a_priori(y) if a_priori is None else np.log(a_priori)\n","    # check that a_priori has the correct number of classes\n","    assert len(self.log_a_priori) == len(self.encoder.names), \"A priori probabilities do not match number of classes\"\n","    # now that everything else is in place, estimate all needed parameters for given model\n","    self._fit_params(X, y)\n","    # Q4: por que el _fit_params va al final? no se puede mover a, por ejemplo, antes de la priori?\n","\n","  def predict(self, X):\n","    \"\"\"\n","    Predice las etiquetas para las observaciones en X.\n","    paso a paso:\n","    Inicializa un array vacío y_hat para almacenar las predicciones.\n","    Itera sobre cada observación en X, predice su clase usando _predict_one,\n","    y almacena el resultado decodificado en y_hat.\n","    Devuelve y_hat como un vector fila.\n","    \"\"\"\n","    # this is actually an individual prediction encased in a for-loop\n","    m_obs = X.shape[1]\n","    y_hat = np.empty(m_obs, dtype=self.encoder.fmt)\n","    for i in range(m_obs):\n","      encoded_y_hat_i = self._predict_one(X[:,i].reshape(-1,1))\n","      y_hat[i] = self.encoder.names[encoded_y_hat_i]\n","    # return prediction as a row vector (matching y)\n","    return y_hat.reshape(1,-1)\n","\n","  def _predict_one(self, x):\n","    \"\"\"\n","    Predice la clase para una única observación x.\n","    paso a paso:\n","    Calcula la probabilidad a posteriori logarítmica para cada clase sumando\n","    la probabilidad a priori logarítmica y la probabilidad condicional logarítmica.\n","    Devuelve el índice de la clase con la máxima probabilidad a posteriori.\n","    \"\"\"\n","    # calculate all log posteriori probabilities (actually, +C)\n","    log_posteriori = [ log_a_priori_i + self._predict_log_conditional(x, idx) for idx, log_a_priori_i\n","                  in enumerate(self.log_a_priori) ]\n","\n","    # return the class that has maximum a posteriori probability\n","    return np.argmax(log_posteriori)"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1717802215818,"user":{"displayName":"Santi Olaciregui","userId":"08004037901086979441"},"user_tz":180},"id":"jqoDsDSlZBXy"},"outputs":[],"source":["class QDA(BaseBayesianClassifier):\n","  \"\"\"\n","  Clasifica los datos basándose en modelos Gaussianos con diferentes matrices\n","  de covarianza para cada clase.\n","  \"\"\"\n","\n","  def _fit_params(self, X, y):\n","    \"\"\"\n","    flatten, se asegura que y sea un array unidimensional, lo cual es necesario\n","    para hacer comparaciones elementales como y == idx. Sin flatten,\n","    y == idx puede no funcionar correctamente si y tiene más de una dimensión.\n","    bias=True en np.cov ajusta el cálculo de la matriz de covarianza dividiendo\n","    por N (número de observaciones) en lugar de N-1. Esto proporciona una\n","    estimación de la covarianza basada en la población en lugar de la muestra.\n","    X es una matriz donde las filas representan características y las columnas\n","    representan observaciones. Calcular la media a lo largo de las columnas\n","    (axis=1) proporciona la media de cada característica para cada clase.\n","    \"\"\"\n","    # estimate each covariance matrix\n","    self.inv_covs = [inv(np.cov(X[:,y.flatten()==idx], bias=True))\n","                      for idx in range(len(self.log_a_priori))]\n","    # Q5: por que hace falta el flatten y no se puede directamente X[:,y==idx]?\n","    # Q6: por que se usa bias=True en vez del default bias=False?\n","    self.means = [X[:,y.flatten()==idx].mean(axis=1, keepdims=True)\n","                  for idx in range(len(self.log_a_priori))]\n","    # Q7: que hace axis=1? por que no axis=0?\n","\n","  def _predict_log_conditional(self, x, class_idx):\n","    \"\"\"\n","    Predice el logaritmo de la probabilidad condicional de x dado class_idx.\n","    paso a paso:\n","    Obtiene la matriz de covarianza inversa para la clase class_idx.\n","    Calcula unbiased_x restando la media de la clase de x.\n","    Calcula el logaritmo de la probabilidad condicional usando la fórmula del\n","    modelo Gaussiano cuadrático.\n","    \"\"\"\n","    # predict the log(P(x|G=class_idx)), the log of the conditional probability of x given the class\n","    # this should depend on the model used\n","    inv_cov = self.inv_covs[class_idx]\n","    unbiased_x =  x - self.means[class_idx]\n","    return 0.5*np.log(det(inv_cov)) -0.5 * unbiased_x.T @ inv_cov @ unbiased_x"]},{"cell_type":"markdown","metadata":{"id":"aZ9vYyWcb2x1"},"source":["# Código para pruebas"]},{"cell_type":"markdown","metadata":{"id":"SBZD2PMFdkVg"},"source":["Seteamos los datos"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1717802215818,"user":{"displayName":"Santi Olaciregui","userId":"08004037901086979441"},"user_tz":180},"id":"Mjyid9dBb-DZ"},"outputs":[],"source":["# hiperparámetros\n","rng_seed = 2000"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6188,"status":"ok","timestamp":1717802222002,"user":{"displayName":"Santi Olaciregui","userId":"08004037901086979441"},"user_tz":180},"id":"451U-AtccHtJ","outputId":"98227940-947d-4934-ac81-fac53f16492b"},"outputs":[{"output_type":"stream","name":"stdout","text":["X: (342, 4), Y:(342, 1)\n"]}],"source":["from sklearn.datasets import load_iris, fetch_openml\n","\"\"\"\n","proporciona dos funciones para cargar conjuntos de datos: uno para el conjunto\n","de datos de Iris y otro para un conjunto de datos de pingüinos.\n","\"\"\"\n","\n","def get_iris_dataset():\n","  \"\"\"\n","  Carga el conjunto de datos de Iris.\n","  paso a paso:\n","  Utiliza load_iris de sklearn.datasets para cargar los datos.\n","  Extrae las características (X_full) y las etiquetas (y_full) del conjunto\n","  de datos cargado.\n","  Convierte las etiquetas numéricas a sus nombres correspondientes\n","  (e.g., de 0, 1, 2 a \"setosa\", \"versicolor\", \"virginica\").\n","  Devuelve las características y las etiquetas.\n","  \"\"\"\n","  data = load_iris()\n","  X_full = data.data\n","  y_full = np.array([data.target_names[y] for y in data.target.reshape(-1,1)])\n","  return X_full, y_full\n","\n","def get_penguins():\n","  \"\"\"\n","  Carga el conjunto de datos de pingüinos.\n","  paso a paso:\n","  Utiliza fetch_openml de sklearn.datasets para obtener el conjunto de datos de pingüinos.\n","  Selecciona las características y las etiquetas del conjunto de datos.\n","  Elimina columnas no numéricas (\"island\" y \"sex\").\n","  Elimina filas con valores faltantes.\n","  Devuelve las características y las etiquetas.\n","  \"\"\"\n","  # get data\n","  df, tgt = fetch_openml(name=\"penguins\", return_X_y=True, as_frame=True, parser='auto')\n","  # drop non-numeric columns\n","  df.drop(columns=[\"island\",\"sex\"], inplace=True)\n","  # drop rows with missing values\n","  mask = df.isna().sum(axis=1) == 0\n","  df = df[mask]\n","  tgt = tgt[mask]\n","  return df.values, tgt.to_numpy().reshape(-1,1)\n","\n","# showing for iris\n","X_full, y_full = get_penguins()\n","\n","print(f\"X: {X_full.shape}, Y:{y_full.shape}\")"]},{"cell_type":"markdown","metadata":{"id":"bY8Le0LPGYbU"},"source":["Separamos el dataset en train y test para medir performance"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":657,"status":"ok","timestamp":1717802222655,"user":{"displayName":"Santi Olaciregui","userId":"08004037901086979441"},"user_tz":180},"id":"EVV6Yesadv5Z","outputId":"60fded29-663c-4e7d-b654-6eead807ff08"},"outputs":[{"output_type":"stream","name":"stdout","text":["(4, 239) (1, 239) (4, 103) (1, 103)\n"]}],"source":["# preparing data, train - test validation\n","# 70-30 split\n","from sklearn.model_selection import train_test_split\n","\n","def split_transpose(X, y, test_sz, random_state):\n","  \"\"\"\n","  Descripción: Divide los datos en conjuntos de train y test utilizando una\n","  división de 70-30 (train-test split), y luego transpone las matrices para que\n","  las observaciones sean columnas en lugar de filas.\n","  Parámetros:\n","  X: Matriz de características.\n","  y: Vector de etiquetas.\n","  test_sz: Tamaño del conjunto de test (proporción).\n","  random_state: Semilla aleatoria para reproducibilidad.\n","  Devuelve:\n","  X_train: Matriz de características de train transpuesta.\n","  y_train: Vector de etiquetas de train transpuesto.\n","  X_test: Matriz de características de test transpuesta.\n","  y_test: Vector de etiquetas de test transpuesto.\n","  \"\"\"\n","  # split\n","  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=random_state)\n","  # transpose so observations are column vectors\n","  return X_train.T, y_train.T, X_test.T, y_test.T\n","\n","def accuracy(y_true, y_pred):\n","  \"\"\"\n","  Descripción: Calcula la precisión de las predicciones comparando las etiquetas\n","  verdaderas con las predichas.\n","  Parámetros:\n","  y_true: Vector de etiquetas verdaderas.\n","  y_pred: Vector de etiquetas predichas.\n","  Devuelve:\n","  La precisión de las predicciones.\n","  \"\"\"\n","  return (y_true == y_pred).mean()\n","\n","train_x, train_y, test_x, test_y = split_transpose(X_full, y_full, 0.3, rng_seed)\n","\n","print(train_x.shape, train_y.shape, test_x.shape, test_y.shape)"]},{"cell_type":"markdown","metadata":{"id":"V5TxHukig1cD"},"source":["Entrenamos un QDA y medimos su accuracy"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1717802222655,"user":{"displayName":"Santi Olaciregui","userId":"08004037901086979441"},"user_tz":180},"id":"-jdOe1fOg6cF"},"outputs":[],"source":["qda = QDA()\n","\n","qda.fit(train_x, train_y)"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1717802222655,"user":{"displayName":"Santi Olaciregui","userId":"08004037901086979441"},"user_tz":180},"id":"O7126rs2oxnS","outputId":"99047c8a-790e-4993-e0d9-ab14e0fc7f75"},"outputs":[{"output_type":"stream","name":"stdout","text":["Train (apparent) error is 0.0084 while test error is 0.0194\n"]}],"source":["train_acc = accuracy(train_y, qda.predict(train_x))\n","test_acc = accuracy(test_y, qda.predict(test_x))\n","print(f\"Train (apparent) error is {1-train_acc:.4f} while test error is {1-test_acc:.4f}\")"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"ZaSOkpCp_O6v","executionInfo":{"status":"ok","timestamp":1717802222655,"user_tz":180,"elapsed":4,"user":{"displayName":"Santi Olaciregui","userId":"08004037901086979441"}}},"outputs":[],"source":["class BaseBayesianClassifier:\n","  \"\"\"\n","  Implementación base para un clasificador Bayesiano.\n","  Contiene métodos para ajustar el modelo y predecir clases basándose\n","  en probabilidades a priori y condicionales.\n","  \"\"\"\n","\n","  def __init__(self):\n","    \"\"\"\n","    Inicializa el clasificador creando un objeto ClassEncoder para codificar\n","    etiquetas categóricas en números enteros.\n","    \"\"\"\n","    self.encoder = ClassEncoder()\n","\n","  def _estimate_a_priori(self, y):\n","    \"\"\"\n","    Estima las probabilidades a priori para cada clase.\n","    paso a paso:\n","    np.bincount(y.flatten().astype(int)) cuenta el número de ocurrencias de cada valor entero en y.\n","    Divide por y.size para obtener las frecuencias relativas (probabilidades).\n","    Devuelve el logaritmo natural de estas probabilidades.\n","    \"\"\"\n","    # Obtener el número de clases en y: 3 en este caso\n","    num_classes = len(np.unique(y))\n","    print(num_classes)\n","    # Crear un array con probabilidades a priori: 0.90, 0.05 y 0.05\n","    a_priori = np.array([0.05, 0.05, 0.90])\n","    print(a_priori)\n","    # Q3: para que sirve bincount?\n","    return np.log(a_priori)\n","\n","  def _fit_params(self, X, y):\n","    \"\"\"\n","    Método abstracto que debe ser implementado por una subclase para ajustar\n","    los parámetros del modelo específico.\n","    paso a paso:\n","    No tiene funcionalidad en esta clase base, solo establece que las subclases\n","    deben implementar este método.\n","    \"\"\"\n","    # estimate all needed parameters for given model\n","    raise NotImplementedError()\n","\n","  def _predict_log_conditional(self, x, class_idx):\n","    \"\"\"\n","    Método abstracto que debe ser implementado por una subclase para predecir\n","    la probabilidad condicional logarítmica.\n","    paso a paso:\n","    No tiene funcionalidad en esta clase base, solo establece que las subclases\n","    deben implementar este método.\n","    \"\"\"\n","    # predict the log(P(x|G=class_idx)), the log of the conditional probability of x given the class\n","    # this should depend on the model used\n","    raise NotImplementedError()\n","\n","  def fit(self, X, y, a_priori=None):\n","    \"\"\"\n","    Ajusta el clasificador a los datos X y y.\n","    paso a paso:\n","    Codifica las etiquetas y usando ClassEncoder.\n","    Estima las probabilidades a priori si no se proporcionan.\n","    Verifica que las probabilidades a priori coincidan con el número de clases.\n","    Llama al método _fit_params para ajustar los parámetros del modelo específico.\n","    \"\"\"\n","    # first encode the classes\n","    y = self.encoder.fit_transform(y)\n","    # if it's needed, estimate a priori probabilities\n","    self.log_a_priori = self._estimate_a_priori(y) if a_priori is None else np.log(a_priori)\n","    # check that a_priori has the correct number of classes\n","    assert len(self.log_a_priori) == len(self.encoder.names), \"A priori probabilities do not match number of classes\"\n","    # now that everything else is in place, estimate all needed parameters for given model\n","    self._fit_params(X, y)\n","    # Q4: por que el _fit_params va al final? no se puede mover a, por ejemplo, antes de la priori?\n","\n","  def predict(self, X):\n","    \"\"\"\n","    Predice las etiquetas para las observaciones en X.\n","    paso a paso:\n","    Inicializa un array vacío y_hat para almacenar las predicciones.\n","    Itera sobre cada observación en X, predice su clase usando _predict_one,\n","    y almacena el resultado decodificado en y_hat.\n","    Devuelve y_hat como un vector fila.\n","    \"\"\"\n","    # this is actually an individual prediction encased in a for-loop\n","    m_obs = X.shape[1]\n","    y_hat = np.empty(m_obs, dtype=self.encoder.fmt)\n","    for i in range(m_obs):\n","      encoded_y_hat_i = self._predict_one(X[:,i].reshape(-1,1))\n","      y_hat[i] = self.encoder.names[encoded_y_hat_i]\n","    # return prediction as a row vector (matching y)\n","    return y_hat.reshape(1,-1)\n","\n","  def _predict_one(self, x):\n","    \"\"\"\n","    Predice la clase para una única observación x.\n","    paso a paso:\n","    Calcula la probabilidad a posteriori logarítmica para cada clase sumando\n","    la probabilidad a priori logarítmica y la probabilidad condicional logarítmica.\n","    Devuelve el índice de la clase con la máxima probabilidad a posteriori.\n","    \"\"\"\n","    # calculate all log posteriori probabilities (actually, +C)\n","    log_posteriori = [ log_a_priori_i + self._predict_log_conditional(x, idx) for idx, log_a_priori_i\n","                  in enumerate(self.log_a_priori) ]\n","\n","    # return the class that has maximum a posteriori probability\n","    return np.argmax(log_posteriori)"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"0HiUZUwh_O6v","executionInfo":{"status":"ok","timestamp":1717802222655,"user_tz":180,"elapsed":4,"user":{"displayName":"Santi Olaciregui","userId":"08004037901086979441"}}},"outputs":[],"source":["class QDA(BaseBayesianClassifier):\n","  \"\"\"\n","  Clasifica los datos basándose en modelos Gaussianos con diferentes matrices\n","  de covarianza para cada clase.\n","  \"\"\"\n","\n","  def _fit_params(self, X, y):\n","    \"\"\"\n","    flatten, se asegura que y sea un array unidimensional, lo cual es necesario\n","    para hacer comparaciones elementales como y == idx. Sin flatten,\n","    y == idx puede no funcionar correctamente si y tiene más de una dimensión.\n","    bias=True en np.cov ajusta el cálculo de la matriz de covarianza dividiendo\n","    por N (número de observaciones) en lugar de N-1. Esto proporciona una\n","    estimación de la covarianza basada en la población en lugar de la muestra.\n","    X es una matriz donde las filas representan características y las columnas\n","    representan observaciones. Calcular la media a lo largo de las columnas\n","    (axis=1) proporciona la media de cada característica para cada clase.\n","    \"\"\"\n","    # estimate each covariance matrix\n","    self.inv_covs = [inv(np.cov(X[:,y.flatten()==idx], bias=True))\n","                      for idx in range(len(self.log_a_priori))]\n","    # Q5: por que hace falta el flatten y no se puede directamente X[:,y==idx]?\n","    # Q6: por que se usa bias=True en vez del default bias=False?\n","    self.means = [X[:,y.flatten()==idx].mean(axis=1, keepdims=True)\n","                  for idx in range(len(self.log_a_priori))]\n","    # Q7: que hace axis=1? por que no axis=0?\n","\n","  def _predict_log_conditional(self, x, class_idx):\n","    \"\"\"\n","    Predice el logaritmo de la probabilidad condicional de x dado class_idx.\n","    paso a paso:\n","    Obtiene la matriz de covarianza inversa para la clase class_idx.\n","    Calcula unbiased_x restando la media de la clase de x.\n","    Calcula el logaritmo de la probabilidad condicional usando la fórmula del\n","    modelo Gaussiano cuadrático.\n","    \"\"\"\n","    # predict the log(P(x|G=class_idx)), the log of the conditional probability of x given the class\n","    # this should depend on the model used\n","    inv_cov = self.inv_covs[class_idx]\n","    unbiased_x =  x - self.means[class_idx]\n","    return 0.5*np.log(det(inv_cov)) -0.5 * unbiased_x.T @ inv_cov @ unbiased_x"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d32kQqfY_O6v","executionInfo":{"status":"ok","timestamp":1717802222655,"user_tz":180,"elapsed":4,"user":{"displayName":"Santi Olaciregui","userId":"08004037901086979441"}},"outputId":"ea492538-c997-43f4-8694-f9482f93c435"},"outputs":[{"output_type":"stream","name":"stdout","text":["3\n","[0.05 0.05 0.9 ]\n","Train (apparent) error is 0.0084 while test error is 0.0194\n"]}],"source":["qda = QDA()\n","qda.fit(train_x, train_y)\n","\n","train_acc = accuracy(train_y, qda.predict(train_x))\n","test_acc = accuracy(test_y, qda.predict(test_x))\n","print(f\"Train (apparent) error is {1-train_acc:.4f} while test error is {1-test_acc:.4f}\")"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.8"}},"nbformat":4,"nbformat_minor":0}