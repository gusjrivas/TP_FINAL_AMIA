{"cells":[{"cell_type":"markdown","metadata":{"id":"RIuUO15QGc0L"},"source":["Implementaci√≥n base\n","1. Entrenar un modelo QDA sobre el dataset *iris* utilizando las distribuciones *a priori* a continuaci√≥n ¬øSe observan diferencias?¬øPor qu√© cree? _Pista: comparar con las distribuciones del dataset completo, **sin splitear**_.\n","    1. Uniforme (cada clase tiene probabilidad 1/3)\n","    2. Una clase con probabilidad 0.9, las dem√°s 0.05 (probar las 3 combinaciones).\n","\n","\n","Modificar las probabilidades a priori para las clases afectar√° los errores de predicci√≥n en los datos de entrenamiento y de prueba. Esto se debe a como funcionan los clasificadores Bayesianos, que combinan las probabilidades a priori con las probabilidades condicionales para calcular las probabilidades a posteriori. Viendo la expresi√≥n de la clasificaci√≥n Bayesiana:\n","\n","Se basa en el Teorema de Bayes:\n","\n","ùëÉ(ùê∫=ùëî‚à£ùëã=ùë•)=ùëÉ(ùëã=ùë•‚à£ùê∫=ùëî)ùëÉ(ùê∫=ùëî)/ùëÉ(ùëã=ùë•)\n","\n","Donde:\n","\n","ùëÉ(ùê∫=ùëî‚à£ùëã=ùë•) es la probabilidad a posteriori de la clase ùëî dado el dato ùë•.\n","\n","ùëÉ(ùëã=ùë•‚à£ùê∫=ùëî) es la probabilidad condicional de ùë• dado que pertenece a la clase ùëî.\n","\n","ùëÉ(ùê∫=ùëî) es la probabilidad a priori de la clase ùëî.\n","\n","ùëÉ(ùëã=ùë• es la probabilidad marginal de ùë•.\n","\n","En un clasificador Bayesiano, la decisi√≥n se toma generalmente seleccionando la clase con la mayor probabilidad a posteriori. Esto implica que las probabilidades a priori\n","ùëÉ(ùê∫=ùëî) juegan un papel crucial en la decisi√≥n final.\n","\n","Impacto de las Probabilidades A Priori en los Errores de Predicci√≥n\n","Si inicialmente teeneos probabilidades a priori\n","[0.90,0.05,0.05] para las clases 1, 2 y 3, respectivamente, estamos asumiendo que la mayor√≠a de los datos pertenecen a la clase 1. Esto influye en las predicciones del modelo, haci√©ndolo m√°s inclinado a predecir la clase 1. Si la verdadera distribuci√≥n de las clases es similar a esta, los errores de predicci√≥n en los datos de entrenamiento y en los datos de prueba (Ep) ser√°n bajos.\n","\n","Sin embargo, si cambias las probabilidades a priori a\n","[0.05,0.90,0.05, el modelo ahora asumir√° que la mayor√≠a de los datos pertenecen a la clase 2. Esto cambiar√° significativamente las decisiones del modelo, haci√©ndolo m√°s inclinado a predecir la clase 2. Si la verdadera distribuci√≥n de las clases no coincide con esta nueva suposici√≥n, los errores de predicci√≥n aumentar√°n.\n","\n","El cambio en los errores de predicci√≥n ocurre porque las probabilidades a priori modifican las probabilidades a posteriori que usa el clasificador para tomar decisiones. Si las probabilidades a priori no reflejan la verdadera distribuci√≥n de las clases, el clasificador tender√° a hacer predicciones incorrectas m√°s frecuentemente.\n","\n","Conclusi√≥n\n","Las probabilidades a priori influyen directamente en la probabilidad a posteriori calculada por el clasificador, y por ende, en las predicciones que hace el modelo. Si las probabilidades a priori est√°n en l√≠nea con la verdadera distribuci√≥n de las clases, los errores de predicci√≥n ser√°n menores. Si no lo est√°n, los errores de predicci√≥n aumentar√°n. Por lo tanto, es crucial estimar o definir correctamente las probabilidades a priori para lograr una buena performance del clasificador Bayesiano."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T__eZOdyGbdS"},"outputs":[],"source":["# Se importan las librerias necesarias\n","import numpy as np\n","from numpy.linalg import det, inv"]},{"cell_type":"markdown","metadata":{"id":"TaaPO0evcBDD"},"source":["# Modelo"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6tumZFxEH2w7"},"outputs":[],"source":["class ClassEncoder:\n","  \"\"\"\n","  Permite codificar etiquetas categ√≥ricas en valores num√©ricos y decodificarlos\n","  de vuelta a sus etiquetas originales.\n","  \"\"\"\n","\n","  def fit(self, y):\n","    \"\"\"\n","    Ajusta el codificador a las etiquetas proporcionadas.\n","    paso a paso:\n","    np.unique(y) encuentra las etiquetas √∫nicas en y y las almacena en self.names.\n","    Crea un diccionario self.name_to_class que asigna a cada etiqueta √∫nica un √≠ndice num√©rico.\n","    Almacena el tipo de datos de y en self.fmt.\n","    \"\"\"\n","    self.names = np.unique(y)\n","    self.name_to_class = {name:idx for idx, name in enumerate(self.names)}\n","    self.fmt = y.dtype\n","    # Q1: por que no hace falta definir un class_to_name para el mapeo inverso?\n","\n","  def _map_reshape(self, f, arr):\n","    \"\"\"\n","    Descripci√≥n: Aplica una funci√≥n f a cada elemento de un array arr y luego lo remodela a su forma original.\n","    paso a paso:\n","    arr.flatten() aplana el array a una dimensi√≥n.\n","    Aplica la funci√≥n f a cada elemento del array aplanado.\n","    Convierte el resultado en un array de NumPy y lo remodela a la forma original de arr.\n","    \"\"\"\n","    return np.array([f(elem) for elem in arr.flatten()]).reshape(arr.shape)\n","    # Q2: por que hace falta un reshape?\n","\n","  def transform(self, y):\n","    \"\"\"\n","    Transforma las etiquetas en n√∫meros enteros usando el mapeo definido en fit.\n","    paso a paso:\n","    Utiliza _map_reshape para aplicar el diccionario self.name_to_class a cada etiqueta en y,\n","    convirti√©ndolas en sus correspondientes valores num√©ricos.\n","    \"\"\"\n","    return self._map_reshape(lambda name: self.name_to_class[name], y)\n","\n","  def fit_transform(self, y):\n","    \"\"\"\n","    Ajusta el codificador a las etiquetas y luego las transforma en una sola operaci√≥n.\n","    paso a paso:\n","    Llama a fit(y) para ajustar el codificador.\n","    Luego llama a transform(y) para transformar las etiquetas ajustadas.\n","    \"\"\"\n","    self.fit(y)\n","    return self.transform(y)\n","\n","  def detransform(self, y_hat):\n","    \"\"\"\n","    Convierte los valores num√©ricos de vuelta a sus etiquetas originales.\n","    paso a paso:\n","    Utiliza _map_reshape para aplicar self.names a cada √≠ndice en y_hat,\n","    convirti√©ndolos de vuelta a sus etiquetas originales.\n","    \"\"\"\n","    return self._map_reshape(lambda idx: self.names[idx], y_hat)"]},{"cell_type":"markdown","source":["# **Ejercicio 1.1**"],"metadata":{"id":"OFEqHZ8t-hX8"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"pMsyCGFKQGdk"},"outputs":[],"source":["class BaseBayesianClassifier:\n","  \"\"\"\n","  Implementaci√≥n base para un clasificador Bayesiano.\n","  Contiene m√©todos para ajustar el modelo y predecir clases bas√°ndose\n","  en probabilidades a priori y condicionales.\n","  \"\"\"\n","\n","  def __init__(self):\n","    \"\"\"\n","    Inicializa el clasificador creando un objeto ClassEncoder para codificar\n","    etiquetas categ√≥ricas en n√∫meros enteros.\n","    \"\"\"\n","    self.encoder = ClassEncoder()\n","\n","  def _estimate_a_priori(self, y):\n","    \"\"\"\n","    Estima las probabilidades a priori para cada clase.\n","    paso a paso:\n","    np.bincount(y.flatten().astype(int)) cuenta el n√∫mero de ocurrencias de cada valor entero en y.\n","    Divide por y.size para obtener las frecuencias relativas (probabilidades).\n","    Devuelve el logaritmo natural de estas probabilidades.\n","    \"\"\"\n","    # Obtener el n√∫mero de clases en y: 3 en este caso\n","    num_classes = len(np.unique(y))\n","    # Crear un array con probabilidades a priori iguales: 1/3 en este caso\n","    a_priori = np.full(num_classes, 1/num_classes)\n","    # Q3: para que sirve bincount?\n","    return np.log(a_priori)\n","\n","  def _fit_params(self, X, y):\n","    \"\"\"\n","    M√©todo abstracto que debe ser implementado por una subclase para ajustar\n","    los par√°metros del modelo espec√≠fico.\n","    paso a paso:\n","    No tiene funcionalidad en esta clase base, solo establece que las subclases\n","    deben implementar este m√©todo.\n","    \"\"\"\n","    # estimate all needed parameters for given model\n","    raise NotImplementedError()\n","\n","  def _predict_log_conditional(self, x, class_idx):\n","    \"\"\"\n","    M√©todo abstracto que debe ser implementado por una subclase para predecir\n","    la probabilidad condicional logar√≠tmica.\n","    paso a paso:\n","    No tiene funcionalidad en esta clase base, solo establece que las subclases\n","    deben implementar este m√©todo.\n","    \"\"\"\n","    # predict the log(P(x|G=class_idx)), the log of the conditional probability of x given the class\n","    # this should depend on the model used\n","    raise NotImplementedError()\n","\n","  def fit(self, X, y, a_priori=None):\n","    \"\"\"\n","    Ajusta el clasificador a los datos X y y.\n","    paso a paso:\n","    Codifica las etiquetas y usando ClassEncoder.\n","    Estima las probabilidades a priori si no se proporcionan.\n","    Verifica que las probabilidades a priori coincidan con el n√∫mero de clases.\n","    Llama al m√©todo _fit_params para ajustar los par√°metros del modelo espec√≠fico.\n","    \"\"\"\n","    # first encode the classes\n","    y = self.encoder.fit_transform(y)\n","    # if it's needed, estimate a priori probabilities\n","    self.log_a_priori = self._estimate_a_priori(y) if a_priori is None else np.log(a_priori)\n","    # check that a_priori has the correct number of classes\n","    assert len(self.log_a_priori) == len(self.encoder.names), \"A priori probabilities do not match number of classes\"\n","    # now that everything else is in place, estimate all needed parameters for given model\n","    self._fit_params(X, y)\n","    # Q4: por que el _fit_params va al final? no se puede mover a, por ejemplo, antes de la priori?\n","\n","  def predict(self, X):\n","    \"\"\"\n","    Predice las etiquetas para las observaciones en X.\n","    paso a paso:\n","    Inicializa un array vac√≠o y_hat para almacenar las predicciones.\n","    Itera sobre cada observaci√≥n en X, predice su clase usando _predict_one,\n","    y almacena el resultado decodificado en y_hat.\n","    Devuelve y_hat como un vector fila.\n","    \"\"\"\n","    # this is actually an individual prediction encased in a for-loop\n","    m_obs = X.shape[1]\n","    y_hat = np.empty(m_obs, dtype=self.encoder.fmt)\n","    for i in range(m_obs):\n","      encoded_y_hat_i = self._predict_one(X[:,i].reshape(-1,1))\n","      y_hat[i] = self.encoder.names[encoded_y_hat_i]\n","    # return prediction as a row vector (matching y)\n","    return y_hat.reshape(1,-1)\n","\n","  def _predict_one(self, x):\n","    \"\"\"\n","    Predice la clase para una √∫nica observaci√≥n x.\n","    paso a paso:\n","    Calcula la probabilidad a posteriori logar√≠tmica para cada clase sumando\n","    la probabilidad a priori logar√≠tmica y la probabilidad condicional logar√≠tmica.\n","    Devuelve el √≠ndice de la clase con la m√°xima probabilidad a posteriori.\n","    \"\"\"\n","    # calculate all log posteriori probabilities (actually, +C)\n","    log_posteriori = [ log_a_priori_i + self._predict_log_conditional(x, idx) for idx, log_a_priori_i\n","                  in enumerate(self.log_a_priori) ]\n","\n","    # return the class that has maximum a posteriori probability\n","    return np.argmax(log_posteriori)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jqoDsDSlZBXy"},"outputs":[],"source":["class QDA(BaseBayesianClassifier):\n","  \"\"\"\n","  Clasifica los datos bas√°ndose en modelos Gaussianos con diferentes matrices\n","  de covarianza para cada clase.\n","  \"\"\"\n","\n","  def _fit_params(self, X, y):\n","    \"\"\"\n","    flatten, se asegura que y sea un array unidimensional, lo cual es necesario\n","    para hacer comparaciones elementales como y == idx. Sin flatten,\n","    y == idx puede no funcionar correctamente si y tiene m√°s de una dimensi√≥n.\n","    bias=True en np.cov ajusta el c√°lculo de la matriz de covarianza dividiendo\n","    por N (n√∫mero de observaciones) en lugar de N-1. Esto proporciona una\n","    estimaci√≥n de la covarianza basada en la poblaci√≥n en lugar de la muestra.\n","    X es una matriz donde las filas representan caracter√≠sticas y las columnas\n","    representan observaciones. Calcular la media a lo largo de las columnas\n","    (axis=1) proporciona la media de cada caracter√≠stica para cada clase.\n","    \"\"\"\n","    # estimate each covariance matrix\n","    self.inv_covs = [inv(np.cov(X[:,y.flatten()==idx], bias=True))\n","                      for idx in range(len(self.log_a_priori))]\n","    # Q5: por que hace falta el flatten y no se puede directamente X[:,y==idx]?\n","    # Q6: por que se usa bias=True en vez del default bias=False?\n","    self.means = [X[:,y.flatten()==idx].mean(axis=1, keepdims=True)\n","                  for idx in range(len(self.log_a_priori))]\n","    # Q7: que hace axis=1? por que no axis=0?\n","\n","  def _predict_log_conditional(self, x, class_idx):\n","    \"\"\"\n","    Predice el logaritmo de la probabilidad condicional de x dado class_idx.\n","    paso a paso:\n","    Obtiene la matriz de covarianza inversa para la clase class_idx.\n","    Calcula unbiased_x restando la media de la clase de x.\n","    Calcula el logaritmo de la probabilidad condicional usando la f√≥rmula del\n","    modelo Gaussiano cuadr√°tico.\n","    \"\"\"\n","    # predict the log(P(x|G=class_idx)), the log of the conditional probability of x given the class\n","    # this should depend on the model used\n","    inv_cov = self.inv_covs[class_idx]\n","    unbiased_x =  x - self.means[class_idx]\n","    return 0.5*np.log(det(inv_cov)) -0.5 * unbiased_x.T @ inv_cov @ unbiased_x"]},{"cell_type":"markdown","metadata":{"id":"aZ9vYyWcb2x1"},"source":["# C√≥digo para pruebas"]},{"cell_type":"markdown","metadata":{"id":"SBZD2PMFdkVg"},"source":["Seteamos los datos"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mjyid9dBb-DZ"},"outputs":[],"source":["# hiperpar√°metros\n","rng_seed = 2000"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1571,"status":"ok","timestamp":1717801844360,"user":{"displayName":"Santi Olaciregui","userId":"08004037901086979441"},"user_tz":180},"id":"451U-AtccHtJ","outputId":"05517db4-b2ab-4dc1-bad6-dd7a83de6f57"},"outputs":[{"output_type":"stream","name":"stdout","text":["X: (150, 4), Y:(150, 1)\n"]}],"source":["from sklearn.datasets import load_iris, fetch_openml\n","\"\"\"\n","proporciona dos funciones para cargar conjuntos de datos: uno para el conjunto\n","de datos de Iris y otro para un conjunto de datos de ping√ºinos.\n","\"\"\"\n","\n","def get_iris_dataset():\n","  \"\"\"\n","  Carga el conjunto de datos de Iris.\n","  paso a paso:\n","  Utiliza load_iris de sklearn.datasets para cargar los datos.\n","  Extrae las caracter√≠sticas (X_full) y las etiquetas (y_full) del conjunto\n","  de datos cargado.\n","  Convierte las etiquetas num√©ricas a sus nombres correspondientes\n","  (e.g., de 0, 1, 2 a \"setosa\", \"versicolor\", \"virginica\").\n","  Devuelve las caracter√≠sticas y las etiquetas.\n","  \"\"\"\n","  data = load_iris()\n","  X_full = data.data\n","  y_full = np.array([data.target_names[y] for y in data.target.reshape(-1,1)])\n","  return X_full, y_full\n","\n","def get_penguins():\n","  \"\"\"\n","  Carga el conjunto de datos de ping√ºinos.\n","  paso a paso:\n","  Utiliza fetch_openml de sklearn.datasets para obtener el conjunto de datos de ping√ºinos.\n","  Selecciona las caracter√≠sticas y las etiquetas del conjunto de datos.\n","  Elimina columnas no num√©ricas (\"island\" y \"sex\").\n","  Elimina filas con valores faltantes.\n","  Devuelve las caracter√≠sticas y las etiquetas.\n","  \"\"\"\n","  # get data\n","  df, tgt = fetch_openml(name=\"penguins\", return_X_y=True, as_frame=True, parser='auto')\n","  # drop non-numeric columns\n","  df.drop(columns=[\"island\",\"sex\"], inplace=True)\n","  # drop rows with missing values\n","  mask = df.isna().sum(axis=1) == 0\n","  df = df[mask]\n","  tgt = tgt[mask]\n","  return df.values, tgt.to_numpy().reshape(-1,1)\n","\n","# showing for iris\n","X_full, y_full = get_iris_dataset()\n","\n","print(f\"X: {X_full.shape}, Y:{y_full.shape}\")"]},{"cell_type":"markdown","metadata":{"id":"bY8Le0LPGYbU"},"source":["Separamos el dataset en train y test para medir performance"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1717801844360,"user":{"displayName":"Santi Olaciregui","userId":"08004037901086979441"},"user_tz":180},"id":"EVV6Yesadv5Z","outputId":"ca9e2ffd-6d03-4dc6-9be5-30bb7eea35d4"},"outputs":[{"output_type":"stream","name":"stdout","text":["(4, 105) (1, 105) (4, 45) (1, 45)\n"]}],"source":["# preparing data, train - test validation\n","# 70-30 split\n","from sklearn.model_selection import train_test_split\n","\n","def split_transpose(X, y, test_sz, random_state):\n","  \"\"\"\n","  Descripci√≥n: Divide los datos en conjuntos de train y test utilizando una\n","  divisi√≥n de 70-30 (train-test split), y luego transpone las matrices para que\n","  las observaciones sean columnas en lugar de filas.\n","  Par√°metros:\n","  X: Matriz de caracter√≠sticas.\n","  y: Vector de etiquetas.\n","  test_sz: Tama√±o del conjunto de test (proporci√≥n).\n","  random_state: Semilla aleatoria para reproducibilidad.\n","  Devuelve:\n","  X_train: Matriz de caracter√≠sticas de train transpuesta.\n","  y_train: Vector de etiquetas de train transpuesto.\n","  X_test: Matriz de caracter√≠sticas de test transpuesta.\n","  y_test: Vector de etiquetas de test transpuesto.\n","  \"\"\"\n","  # split\n","  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=random_state)\n","  # transpose so observations are column vectors\n","  return X_train.T, y_train.T, X_test.T, y_test.T\n","\n","def accuracy(y_true, y_pred):\n","  \"\"\"\n","  Descripci√≥n: Calcula la precisi√≥n de las predicciones comparando las etiquetas\n","  verdaderas con las predichas.\n","  Par√°metros:\n","  y_true: Vector de etiquetas verdaderas.\n","  y_pred: Vector de etiquetas predichas.\n","  Devuelve:\n","  La precisi√≥n de las predicciones.\n","  \"\"\"\n","  return (y_true == y_pred).mean()\n","\n","train_x, train_y, test_x, test_y = split_transpose(X_full, y_full, 0.3, rng_seed)\n","\n","print(train_x.shape, train_y.shape, test_x.shape, test_y.shape)"]},{"cell_type":"markdown","metadata":{"id":"V5TxHukig1cD"},"source":["Entrenamos un QDA y medimos su accuracy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-jdOe1fOg6cF"},"outputs":[],"source":["qda = QDA()\n","\n","qda.fit(train_x, train_y)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":506,"status":"ok","timestamp":1717801844864,"user":{"displayName":"Santi Olaciregui","userId":"08004037901086979441"},"user_tz":180},"id":"O7126rs2oxnS","outputId":"8c495fc1-da46-40e0-89ad-50fa88b67fa7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Train (apparent) error is 0.0190 while test error is 0.0667\n"]}],"source":["train_acc = accuracy(train_y, qda.predict(train_x))\n","test_acc = accuracy(test_y, qda.predict(test_x))\n","print(f\"Train (apparent) error is {1-train_acc:.4f} while test error is {1-test_acc:.4f}\")"]},{"cell_type":"markdown","source":["# **Ejercicio 1.2**"],"metadata":{"id":"oY4Fj9QA-EhH"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"hbqFZahD98IM"},"outputs":[],"source":["class BaseBayesianClassifier:\n","  \"\"\"\n","  Implementaci√≥n base para un clasificador Bayesiano.\n","  Contiene m√©todos para ajustar el modelo y predecir clases bas√°ndose\n","  en probabilidades a priori y condicionales.\n","  \"\"\"\n","\n","  def __init__(self):\n","    \"\"\"\n","    Inicializa el clasificador creando un objeto ClassEncoder para codificar\n","    etiquetas categ√≥ricas en n√∫meros enteros.\n","    \"\"\"\n","    self.encoder = ClassEncoder()\n","\n","  def _estimate_a_priori(self, y):\n","    \"\"\"\n","    Estima las probabilidades a priori para cada clase.\n","    paso a paso:\n","    np.bincount(y.flatten().astype(int)) cuenta el n√∫mero de ocurrencias de cada valor entero en y.\n","    Divide por y.size para obtener las frecuencias relativas (probabilidades).\n","    Devuelve el logaritmo natural de estas probabilidades.\n","    \"\"\"\n","    # Obtener el n√∫mero de clases en y: 3 en este caso\n","    num_classes = len(np.unique(y))\n","    # Crear un array con probabilidades a priori: 0.90, 0.05 y 0.05\n","    a_priori = np.array([0.05, 0.05, 0.90])\n","    print(a_priori)\n","    # Q3: para que sirve bincount?\n","    return np.log(a_priori)\n","\n","  def _fit_params(self, X, y):\n","    \"\"\"\n","    M√©todo abstracto que debe ser implementado por una subclase para ajustar\n","    los par√°metros del modelo espec√≠fico.\n","    paso a paso:\n","    No tiene funcionalidad en esta clase base, solo establece que las subclases\n","    deben implementar este m√©todo.\n","    \"\"\"\n","    # estimate all needed parameters for given model\n","    raise NotImplementedError()\n","\n","  def _predict_log_conditional(self, x, class_idx):\n","    \"\"\"\n","    M√©todo abstracto que debe ser implementado por una subclase para predecir\n","    la probabilidad condicional logar√≠tmica.\n","    paso a paso:\n","    No tiene funcionalidad en esta clase base, solo establece que las subclases\n","    deben implementar este m√©todo.\n","    \"\"\"\n","    # predict the log(P(x|G=class_idx)), the log of the conditional probability of x given the class\n","    # this should depend on the model used\n","    raise NotImplementedError()\n","\n","  def fit(self, X, y, a_priori=None):\n","    \"\"\"\n","    Ajusta el clasificador a los datos X y y.\n","    paso a paso:\n","    Codifica las etiquetas y usando ClassEncoder.\n","    Estima las probabilidades a priori si no se proporcionan.\n","    Verifica que las probabilidades a priori coincidan con el n√∫mero de clases.\n","    Llama al m√©todo _fit_params para ajustar los par√°metros del modelo espec√≠fico.\n","    \"\"\"\n","    # first encode the classes\n","    y = self.encoder.fit_transform(y)\n","    # if it's needed, estimate a priori probabilities\n","    self.log_a_priori = self._estimate_a_priori(y) if a_priori is None else np.log(a_priori)\n","    # check that a_priori has the correct number of classes\n","    assert len(self.log_a_priori) == len(self.encoder.names), \"A priori probabilities do not match number of classes\"\n","    # now that everything else is in place, estimate all needed parameters for given model\n","    self._fit_params(X, y)\n","    # Q4: por que el _fit_params va al final? no se puede mover a, por ejemplo, antes de la priori?\n","\n","  def predict(self, X):\n","    \"\"\"\n","    Predice las etiquetas para las observaciones en X.\n","    paso a paso:\n","    Inicializa un array vac√≠o y_hat para almacenar las predicciones.\n","    Itera sobre cada observaci√≥n en X, predice su clase usando _predict_one,\n","    y almacena el resultado decodificado en y_hat.\n","    Devuelve y_hat como un vector fila.\n","    \"\"\"\n","    # this is actually an individual prediction encased in a for-loop\n","    m_obs = X.shape[1]\n","    y_hat = np.empty(m_obs, dtype=self.encoder.fmt)\n","    for i in range(m_obs):\n","      encoded_y_hat_i = self._predict_one(X[:,i].reshape(-1,1))\n","      y_hat[i] = self.encoder.names[encoded_y_hat_i]\n","    # return prediction as a row vector (matching y)\n","    return y_hat.reshape(1,-1)\n","\n","  def _predict_one(self, x):\n","    \"\"\"\n","    Predice la clase para una √∫nica observaci√≥n x.\n","    paso a paso:\n","    Calcula la probabilidad a posteriori logar√≠tmica para cada clase sumando\n","    la probabilidad a priori logar√≠tmica y la probabilidad condicional logar√≠tmica.\n","    Devuelve el √≠ndice de la clase con la m√°xima probabilidad a posteriori.\n","    \"\"\"\n","    # calculate all log posteriori probabilities (actually, +C)\n","    log_posteriori = [ log_a_priori_i + self._predict_log_conditional(x, idx) for idx, log_a_priori_i\n","                  in enumerate(self.log_a_priori) ]\n","\n","    # return the class that has maximum a posteriori probability\n","    return np.argmax(log_posteriori)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5rEspKtU98IM"},"outputs":[],"source":["class QDA(BaseBayesianClassifier):\n","  \"\"\"\n","  Clasifica los datos bas√°ndose en modelos Gaussianos con diferentes matrices\n","  de covarianza para cada clase.\n","  \"\"\"\n","\n","  def _fit_params(self, X, y):\n","    \"\"\"\n","    flatten, se asegura que y sea un array unidimensional, lo cual es necesario\n","    para hacer comparaciones elementales como y == idx. Sin flatten,\n","    y == idx puede no funcionar correctamente si y tiene m√°s de una dimensi√≥n.\n","    bias=True en np.cov ajusta el c√°lculo de la matriz de covarianza dividiendo\n","    por N (n√∫mero de observaciones) en lugar de N-1. Esto proporciona una\n","    estimaci√≥n de la covarianza basada en la poblaci√≥n en lugar de la muestra.\n","    X es una matriz donde las filas representan caracter√≠sticas y las columnas\n","    representan observaciones. Calcular la media a lo largo de las columnas\n","    (axis=1) proporciona la media de cada caracter√≠stica para cada clase.\n","    \"\"\"\n","    # estimate each covariance matrix\n","    self.inv_covs = [inv(np.cov(X[:,y.flatten()==idx], bias=True))\n","                      for idx in range(len(self.log_a_priori))]\n","    # Q5: por que hace falta el flatten y no se puede directamente X[:,y==idx]?\n","    # Q6: por que se usa bias=True en vez del default bias=False?\n","    self.means = [X[:,y.flatten()==idx].mean(axis=1, keepdims=True)\n","                  for idx in range(len(self.log_a_priori))]\n","    # Q7: que hace axis=1? por que no axis=0?\n","\n","  def _predict_log_conditional(self, x, class_idx):\n","    \"\"\"\n","    Predice el logaritmo de la probabilidad condicional de x dado class_idx.\n","    paso a paso:\n","    Obtiene la matriz de covarianza inversa para la clase class_idx.\n","    Calcula unbiased_x restando la media de la clase de x.\n","    Calcula el logaritmo de la probabilidad condicional usando la f√≥rmula del\n","    modelo Gaussiano cuadr√°tico.\n","    \"\"\"\n","    # predict the log(P(x|G=class_idx)), the log of the conditional probability of x given the class\n","    # this should depend on the model used\n","    inv_cov = self.inv_covs[class_idx]\n","    unbiased_x =  x - self.means[class_idx]\n","    return 0.5*np.log(det(inv_cov)) -0.5 * unbiased_x.T @ inv_cov @ unbiased_x"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Oygbi91p98IM","executionInfo":{"status":"ok","timestamp":1717801844865,"user_tz":180,"elapsed":3,"user":{"displayName":"Santi Olaciregui","userId":"08004037901086979441"}},"outputId":"ce5f218a-c87a-4e47-b34e-e72078ed3ada"},"outputs":[{"output_type":"stream","name":"stdout","text":["[0.05 0.05 0.9 ]\n","Train (apparent) error is 0.0190 while test error is 0.1111\n"]}],"source":["qda = QDA()\n","qda.fit(train_x, train_y)\n","\n","train_acc = accuracy(train_y, qda.predict(train_x))\n","test_acc = accuracy(test_y, qda.predict(test_x))\n","print(f\"Train (apparent) error is {1-train_acc:.4f} while test error is {1-test_acc:.4f}\")"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.8"}},"nbformat":4,"nbformat_minor":0}