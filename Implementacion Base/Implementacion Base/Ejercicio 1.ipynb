{"cells":[{"cell_type":"markdown","metadata":{"id":"RIuUO15QGc0L"},"source":["Implementación base\n","1. Entrenar un modelo QDA sobre el dataset *iris* utilizando las distribuciones *a priori* a continuación ¿Se observan diferencias?¿Por qué cree? _Pista: comparar con las distribuciones del dataset completo, **sin splitear**_.\n","    1. Uniforme (cada clase tiene probabilidad 1/3)\n","    2. Una clase con probabilidad 0.9, las demás 0.05 (probar las 3 combinaciones).\n","\n","\n","Modificar las probabilidades a priori para las clases afectará los errores de predicción en los datos de entrenamiento y de prueba. Esto se debe a como funcionan los clasificadores Bayesianos, que combinan las probabilidades a priori con las probabilidades condicionales para calcular las probabilidades a posteriori. Viendo la expresión de la clasificación Bayesiana:\n","\n","Se basa en el Teorema de Bayes:\n","\n","𝑃(𝐺=𝑔∣𝑋=𝑥)=𝑃(𝑋=𝑥∣𝐺=𝑔)𝑃(𝐺=𝑔)/𝑃(𝑋=𝑥)\n","\n","Donde:\n","\n","𝑃(𝐺=𝑔∣𝑋=𝑥) es la probabilidad a posteriori de la clase 𝑔 dado el dato 𝑥.\n","\n","𝑃(𝑋=𝑥∣𝐺=𝑔) es la probabilidad condicional de 𝑥 dado que pertenece a la clase 𝑔.\n","\n","𝑃(𝐺=𝑔) es la probabilidad a priori de la clase 𝑔.\n","\n","𝑃(𝑋=𝑥 es la probabilidad marginal de 𝑥.\n","\n","En un clasificador Bayesiano, la decisión se toma generalmente seleccionando la clase con la mayor probabilidad a posteriori. Esto implica que las probabilidades a priori\n","𝑃(𝐺=𝑔) juegan un papel crucial en la decisión final.\n","\n","Impacto de las Probabilidades A Priori en los Errores de Predicción\n","Si inicialmente teeneos probabilidades a priori\n","[0.90,0.05,0.05] para las clases 1, 2 y 3, respectivamente, estamos asumiendo que la mayoría de los datos pertenecen a la clase 1. Esto influye en las predicciones del modelo, haciéndolo más inclinado a predecir la clase 1. Si la verdadera distribución de las clases es similar a esta, los errores de predicción en los datos de entrenamiento y en los datos de prueba (Ep) serán bajos.\n","\n","Sin embargo, si cambias las probabilidades a priori a\n","[0.05,0.90,0.05, el modelo ahora asumirá que la mayoría de los datos pertenecen a la clase 2. Esto cambiará significativamente las decisiones del modelo, haciéndolo más inclinado a predecir la clase 2. Si la verdadera distribución de las clases no coincide con esta nueva suposición, los errores de predicción aumentarán.\n","\n","El cambio en los errores de predicción ocurre porque las probabilidades a priori modifican las probabilidades a posteriori que usa el clasificador para tomar decisiones. Si las probabilidades a priori no reflejan la verdadera distribución de las clases, el clasificador tenderá a hacer predicciones incorrectas más frecuentemente.\n","\n","Conclusión\n","Las probabilidades a priori influyen directamente en la probabilidad a posteriori calculada por el clasificador, y por ende, en las predicciones que hace el modelo. Si las probabilidades a priori están en línea con la verdadera distribución de las clases, los errores de predicción serán menores. Si no lo están, los errores de predicción aumentarán. Por lo tanto, es crucial estimar o definir correctamente las probabilidades a priori para lograr una buena performance del clasificador Bayesiano."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T__eZOdyGbdS"},"outputs":[],"source":["# Se importan las librerias necesarias\n","import numpy as np\n","from numpy.linalg import det, inv"]},{"cell_type":"markdown","metadata":{"id":"TaaPO0evcBDD"},"source":["# Modelo"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6tumZFxEH2w7"},"outputs":[],"source":["class ClassEncoder:\n","  \"\"\"\n","  Permite codificar etiquetas categóricas en valores numéricos y decodificarlos\n","  de vuelta a sus etiquetas originales.\n","  \"\"\"\n","\n","  def fit(self, y):\n","    \"\"\"\n","    Ajusta el codificador a las etiquetas proporcionadas.\n","    paso a paso:\n","    np.unique(y) encuentra las etiquetas únicas en y y las almacena en self.names.\n","    Crea un diccionario self.name_to_class que asigna a cada etiqueta única un índice numérico.\n","    Almacena el tipo de datos de y en self.fmt.\n","    \"\"\"\n","    self.names = np.unique(y)\n","    self.name_to_class = {name:idx for idx, name in enumerate(self.names)}\n","    self.fmt = y.dtype\n","    # Q1: por que no hace falta definir un class_to_name para el mapeo inverso?\n","\n","  def _map_reshape(self, f, arr):\n","    \"\"\"\n","    Descripción: Aplica una función f a cada elemento de un array arr y luego lo remodela a su forma original.\n","    paso a paso:\n","    arr.flatten() aplana el array a una dimensión.\n","    Aplica la función f a cada elemento del array aplanado.\n","    Convierte el resultado en un array de NumPy y lo remodela a la forma original de arr.\n","    \"\"\"\n","    return np.array([f(elem) for elem in arr.flatten()]).reshape(arr.shape)\n","    # Q2: por que hace falta un reshape?\n","\n","  def transform(self, y):\n","    \"\"\"\n","    Transforma las etiquetas en números enteros usando el mapeo definido en fit.\n","    paso a paso:\n","    Utiliza _map_reshape para aplicar el diccionario self.name_to_class a cada etiqueta en y,\n","    convirtiéndolas en sus correspondientes valores numéricos.\n","    \"\"\"\n","    return self._map_reshape(lambda name: self.name_to_class[name], y)\n","\n","  def fit_transform(self, y):\n","    \"\"\"\n","    Ajusta el codificador a las etiquetas y luego las transforma en una sola operación.\n","    paso a paso:\n","    Llama a fit(y) para ajustar el codificador.\n","    Luego llama a transform(y) para transformar las etiquetas ajustadas.\n","    \"\"\"\n","    self.fit(y)\n","    return self.transform(y)\n","\n","  def detransform(self, y_hat):\n","    \"\"\"\n","    Convierte los valores numéricos de vuelta a sus etiquetas originales.\n","    paso a paso:\n","    Utiliza _map_reshape para aplicar self.names a cada índice en y_hat,\n","    convirtiéndolos de vuelta a sus etiquetas originales.\n","    \"\"\"\n","    return self._map_reshape(lambda idx: self.names[idx], y_hat)"]},{"cell_type":"markdown","source":["# **Ejercicio 1.1**"],"metadata":{"id":"OFEqHZ8t-hX8"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"pMsyCGFKQGdk"},"outputs":[],"source":["class BaseBayesianClassifier:\n","  \"\"\"\n","  Implementación base para un clasificador Bayesiano.\n","  Contiene métodos para ajustar el modelo y predecir clases basándose\n","  en probabilidades a priori y condicionales.\n","  \"\"\"\n","\n","  def __init__(self):\n","    \"\"\"\n","    Inicializa el clasificador creando un objeto ClassEncoder para codificar\n","    etiquetas categóricas en números enteros.\n","    \"\"\"\n","    self.encoder = ClassEncoder()\n","\n","  def _estimate_a_priori(self, y):\n","    \"\"\"\n","    Estima las probabilidades a priori para cada clase.\n","    paso a paso:\n","    np.bincount(y.flatten().astype(int)) cuenta el número de ocurrencias de cada valor entero en y.\n","    Divide por y.size para obtener las frecuencias relativas (probabilidades).\n","    Devuelve el logaritmo natural de estas probabilidades.\n","    \"\"\"\n","    # Obtener el número de clases en y: 3 en este caso\n","    num_classes = len(np.unique(y))\n","    # Crear un array con probabilidades a priori iguales: 1/3 en este caso\n","    a_priori = np.full(num_classes, 1/num_classes)\n","    # Q3: para que sirve bincount?\n","    return np.log(a_priori)\n","\n","  def _fit_params(self, X, y):\n","    \"\"\"\n","    Método abstracto que debe ser implementado por una subclase para ajustar\n","    los parámetros del modelo específico.\n","    paso a paso:\n","    No tiene funcionalidad en esta clase base, solo establece que las subclases\n","    deben implementar este método.\n","    \"\"\"\n","    # estimate all needed parameters for given model\n","    raise NotImplementedError()\n","\n","  def _predict_log_conditional(self, x, class_idx):\n","    \"\"\"\n","    Método abstracto que debe ser implementado por una subclase para predecir\n","    la probabilidad condicional logarítmica.\n","    paso a paso:\n","    No tiene funcionalidad en esta clase base, solo establece que las subclases\n","    deben implementar este método.\n","    \"\"\"\n","    # predict the log(P(x|G=class_idx)), the log of the conditional probability of x given the class\n","    # this should depend on the model used\n","    raise NotImplementedError()\n","\n","  def fit(self, X, y, a_priori=None):\n","    \"\"\"\n","    Ajusta el clasificador a los datos X y y.\n","    paso a paso:\n","    Codifica las etiquetas y usando ClassEncoder.\n","    Estima las probabilidades a priori si no se proporcionan.\n","    Verifica que las probabilidades a priori coincidan con el número de clases.\n","    Llama al método _fit_params para ajustar los parámetros del modelo específico.\n","    \"\"\"\n","    # first encode the classes\n","    y = self.encoder.fit_transform(y)\n","    # if it's needed, estimate a priori probabilities\n","    self.log_a_priori = self._estimate_a_priori(y) if a_priori is None else np.log(a_priori)\n","    # check that a_priori has the correct number of classes\n","    assert len(self.log_a_priori) == len(self.encoder.names), \"A priori probabilities do not match number of classes\"\n","    # now that everything else is in place, estimate all needed parameters for given model\n","    self._fit_params(X, y)\n","    # Q4: por que el _fit_params va al final? no se puede mover a, por ejemplo, antes de la priori?\n","\n","  def predict(self, X):\n","    \"\"\"\n","    Predice las etiquetas para las observaciones en X.\n","    paso a paso:\n","    Inicializa un array vacío y_hat para almacenar las predicciones.\n","    Itera sobre cada observación en X, predice su clase usando _predict_one,\n","    y almacena el resultado decodificado en y_hat.\n","    Devuelve y_hat como un vector fila.\n","    \"\"\"\n","    # this is actually an individual prediction encased in a for-loop\n","    m_obs = X.shape[1]\n","    y_hat = np.empty(m_obs, dtype=self.encoder.fmt)\n","    for i in range(m_obs):\n","      encoded_y_hat_i = self._predict_one(X[:,i].reshape(-1,1))\n","      y_hat[i] = self.encoder.names[encoded_y_hat_i]\n","    # return prediction as a row vector (matching y)\n","    return y_hat.reshape(1,-1)\n","\n","  def _predict_one(self, x):\n","    \"\"\"\n","    Predice la clase para una única observación x.\n","    paso a paso:\n","    Calcula la probabilidad a posteriori logarítmica para cada clase sumando\n","    la probabilidad a priori logarítmica y la probabilidad condicional logarítmica.\n","    Devuelve el índice de la clase con la máxima probabilidad a posteriori.\n","    \"\"\"\n","    # calculate all log posteriori probabilities (actually, +C)\n","    log_posteriori = [ log_a_priori_i + self._predict_log_conditional(x, idx) for idx, log_a_priori_i\n","                  in enumerate(self.log_a_priori) ]\n","\n","    # return the class that has maximum a posteriori probability\n","    return np.argmax(log_posteriori)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jqoDsDSlZBXy"},"outputs":[],"source":["class QDA(BaseBayesianClassifier):\n","  \"\"\"\n","  Clasifica los datos basándose en modelos Gaussianos con diferentes matrices\n","  de covarianza para cada clase.\n","  \"\"\"\n","\n","  def _fit_params(self, X, y):\n","    \"\"\"\n","    flatten, se asegura que y sea un array unidimensional, lo cual es necesario\n","    para hacer comparaciones elementales como y == idx. Sin flatten,\n","    y == idx puede no funcionar correctamente si y tiene más de una dimensión.\n","    bias=True en np.cov ajusta el cálculo de la matriz de covarianza dividiendo\n","    por N (número de observaciones) en lugar de N-1. Esto proporciona una\n","    estimación de la covarianza basada en la población en lugar de la muestra.\n","    X es una matriz donde las filas representan características y las columnas\n","    representan observaciones. Calcular la media a lo largo de las columnas\n","    (axis=1) proporciona la media de cada característica para cada clase.\n","    \"\"\"\n","    # estimate each covariance matrix\n","    self.inv_covs = [inv(np.cov(X[:,y.flatten()==idx], bias=True))\n","                      for idx in range(len(self.log_a_priori))]\n","    # Q5: por que hace falta el flatten y no se puede directamente X[:,y==idx]?\n","    # Q6: por que se usa bias=True en vez del default bias=False?\n","    self.means = [X[:,y.flatten()==idx].mean(axis=1, keepdims=True)\n","                  for idx in range(len(self.log_a_priori))]\n","    # Q7: que hace axis=1? por que no axis=0?\n","\n","  def _predict_log_conditional(self, x, class_idx):\n","    \"\"\"\n","    Predice el logaritmo de la probabilidad condicional de x dado class_idx.\n","    paso a paso:\n","    Obtiene la matriz de covarianza inversa para la clase class_idx.\n","    Calcula unbiased_x restando la media de la clase de x.\n","    Calcula el logaritmo de la probabilidad condicional usando la fórmula del\n","    modelo Gaussiano cuadrático.\n","    \"\"\"\n","    # predict the log(P(x|G=class_idx)), the log of the conditional probability of x given the class\n","    # this should depend on the model used\n","    inv_cov = self.inv_covs[class_idx]\n","    unbiased_x =  x - self.means[class_idx]\n","    return 0.5*np.log(det(inv_cov)) -0.5 * unbiased_x.T @ inv_cov @ unbiased_x"]},{"cell_type":"markdown","metadata":{"id":"aZ9vYyWcb2x1"},"source":["# Código para pruebas"]},{"cell_type":"markdown","metadata":{"id":"SBZD2PMFdkVg"},"source":["Seteamos los datos"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mjyid9dBb-DZ"},"outputs":[],"source":["# hiperparámetros\n","rng_seed = 2000"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1571,"status":"ok","timestamp":1717801844360,"user":{"displayName":"Santi Olaciregui","userId":"08004037901086979441"},"user_tz":180},"id":"451U-AtccHtJ","outputId":"05517db4-b2ab-4dc1-bad6-dd7a83de6f57"},"outputs":[{"output_type":"stream","name":"stdout","text":["X: (150, 4), Y:(150, 1)\n"]}],"source":["from sklearn.datasets import load_iris, fetch_openml\n","\"\"\"\n","proporciona dos funciones para cargar conjuntos de datos: uno para el conjunto\n","de datos de Iris y otro para un conjunto de datos de pingüinos.\n","\"\"\"\n","\n","def get_iris_dataset():\n","  \"\"\"\n","  Carga el conjunto de datos de Iris.\n","  paso a paso:\n","  Utiliza load_iris de sklearn.datasets para cargar los datos.\n","  Extrae las características (X_full) y las etiquetas (y_full) del conjunto\n","  de datos cargado.\n","  Convierte las etiquetas numéricas a sus nombres correspondientes\n","  (e.g., de 0, 1, 2 a \"setosa\", \"versicolor\", \"virginica\").\n","  Devuelve las características y las etiquetas.\n","  \"\"\"\n","  data = load_iris()\n","  X_full = data.data\n","  y_full = np.array([data.target_names[y] for y in data.target.reshape(-1,1)])\n","  return X_full, y_full\n","\n","def get_penguins():\n","  \"\"\"\n","  Carga el conjunto de datos de pingüinos.\n","  paso a paso:\n","  Utiliza fetch_openml de sklearn.datasets para obtener el conjunto de datos de pingüinos.\n","  Selecciona las características y las etiquetas del conjunto de datos.\n","  Elimina columnas no numéricas (\"island\" y \"sex\").\n","  Elimina filas con valores faltantes.\n","  Devuelve las características y las etiquetas.\n","  \"\"\"\n","  # get data\n","  df, tgt = fetch_openml(name=\"penguins\", return_X_y=True, as_frame=True, parser='auto')\n","  # drop non-numeric columns\n","  df.drop(columns=[\"island\",\"sex\"], inplace=True)\n","  # drop rows with missing values\n","  mask = df.isna().sum(axis=1) == 0\n","  df = df[mask]\n","  tgt = tgt[mask]\n","  return df.values, tgt.to_numpy().reshape(-1,1)\n","\n","# showing for iris\n","X_full, y_full = get_iris_dataset()\n","\n","print(f\"X: {X_full.shape}, Y:{y_full.shape}\")"]},{"cell_type":"markdown","metadata":{"id":"bY8Le0LPGYbU"},"source":["Separamos el dataset en train y test para medir performance"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1717801844360,"user":{"displayName":"Santi Olaciregui","userId":"08004037901086979441"},"user_tz":180},"id":"EVV6Yesadv5Z","outputId":"ca9e2ffd-6d03-4dc6-9be5-30bb7eea35d4"},"outputs":[{"output_type":"stream","name":"stdout","text":["(4, 105) (1, 105) (4, 45) (1, 45)\n"]}],"source":["# preparing data, train - test validation\n","# 70-30 split\n","from sklearn.model_selection import train_test_split\n","\n","def split_transpose(X, y, test_sz, random_state):\n","  \"\"\"\n","  Descripción: Divide los datos en conjuntos de train y test utilizando una\n","  división de 70-30 (train-test split), y luego transpone las matrices para que\n","  las observaciones sean columnas en lugar de filas.\n","  Parámetros:\n","  X: Matriz de características.\n","  y: Vector de etiquetas.\n","  test_sz: Tamaño del conjunto de test (proporción).\n","  random_state: Semilla aleatoria para reproducibilidad.\n","  Devuelve:\n","  X_train: Matriz de características de train transpuesta.\n","  y_train: Vector de etiquetas de train transpuesto.\n","  X_test: Matriz de características de test transpuesta.\n","  y_test: Vector de etiquetas de test transpuesto.\n","  \"\"\"\n","  # split\n","  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=random_state)\n","  # transpose so observations are column vectors\n","  return X_train.T, y_train.T, X_test.T, y_test.T\n","\n","def accuracy(y_true, y_pred):\n","  \"\"\"\n","  Descripción: Calcula la precisión de las predicciones comparando las etiquetas\n","  verdaderas con las predichas.\n","  Parámetros:\n","  y_true: Vector de etiquetas verdaderas.\n","  y_pred: Vector de etiquetas predichas.\n","  Devuelve:\n","  La precisión de las predicciones.\n","  \"\"\"\n","  return (y_true == y_pred).mean()\n","\n","train_x, train_y, test_x, test_y = split_transpose(X_full, y_full, 0.3, rng_seed)\n","\n","print(train_x.shape, train_y.shape, test_x.shape, test_y.shape)"]},{"cell_type":"markdown","metadata":{"id":"V5TxHukig1cD"},"source":["Entrenamos un QDA y medimos su accuracy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-jdOe1fOg6cF"},"outputs":[],"source":["qda = QDA()\n","\n","qda.fit(train_x, train_y)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":506,"status":"ok","timestamp":1717801844864,"user":{"displayName":"Santi Olaciregui","userId":"08004037901086979441"},"user_tz":180},"id":"O7126rs2oxnS","outputId":"8c495fc1-da46-40e0-89ad-50fa88b67fa7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Train (apparent) error is 0.0190 while test error is 0.0667\n"]}],"source":["train_acc = accuracy(train_y, qda.predict(train_x))\n","test_acc = accuracy(test_y, qda.predict(test_x))\n","print(f\"Train (apparent) error is {1-train_acc:.4f} while test error is {1-test_acc:.4f}\")"]},{"cell_type":"markdown","source":["# **Ejercicio 1.2**"],"metadata":{"id":"oY4Fj9QA-EhH"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"hbqFZahD98IM"},"outputs":[],"source":["class BaseBayesianClassifier:\n","  \"\"\"\n","  Implementación base para un clasificador Bayesiano.\n","  Contiene métodos para ajustar el modelo y predecir clases basándose\n","  en probabilidades a priori y condicionales.\n","  \"\"\"\n","\n","  def __init__(self):\n","    \"\"\"\n","    Inicializa el clasificador creando un objeto ClassEncoder para codificar\n","    etiquetas categóricas en números enteros.\n","    \"\"\"\n","    self.encoder = ClassEncoder()\n","\n","  def _estimate_a_priori(self, y):\n","    \"\"\"\n","    Estima las probabilidades a priori para cada clase.\n","    paso a paso:\n","    np.bincount(y.flatten().astype(int)) cuenta el número de ocurrencias de cada valor entero en y.\n","    Divide por y.size para obtener las frecuencias relativas (probabilidades).\n","    Devuelve el logaritmo natural de estas probabilidades.\n","    \"\"\"\n","    # Obtener el número de clases en y: 3 en este caso\n","    num_classes = len(np.unique(y))\n","    # Crear un array con probabilidades a priori: 0.90, 0.05 y 0.05\n","    a_priori = np.array([0.05, 0.05, 0.90])\n","    print(a_priori)\n","    # Q3: para que sirve bincount?\n","    return np.log(a_priori)\n","\n","  def _fit_params(self, X, y):\n","    \"\"\"\n","    Método abstracto que debe ser implementado por una subclase para ajustar\n","    los parámetros del modelo específico.\n","    paso a paso:\n","    No tiene funcionalidad en esta clase base, solo establece que las subclases\n","    deben implementar este método.\n","    \"\"\"\n","    # estimate all needed parameters for given model\n","    raise NotImplementedError()\n","\n","  def _predict_log_conditional(self, x, class_idx):\n","    \"\"\"\n","    Método abstracto que debe ser implementado por una subclase para predecir\n","    la probabilidad condicional logarítmica.\n","    paso a paso:\n","    No tiene funcionalidad en esta clase base, solo establece que las subclases\n","    deben implementar este método.\n","    \"\"\"\n","    # predict the log(P(x|G=class_idx)), the log of the conditional probability of x given the class\n","    # this should depend on the model used\n","    raise NotImplementedError()\n","\n","  def fit(self, X, y, a_priori=None):\n","    \"\"\"\n","    Ajusta el clasificador a los datos X y y.\n","    paso a paso:\n","    Codifica las etiquetas y usando ClassEncoder.\n","    Estima las probabilidades a priori si no se proporcionan.\n","    Verifica que las probabilidades a priori coincidan con el número de clases.\n","    Llama al método _fit_params para ajustar los parámetros del modelo específico.\n","    \"\"\"\n","    # first encode the classes\n","    y = self.encoder.fit_transform(y)\n","    # if it's needed, estimate a priori probabilities\n","    self.log_a_priori = self._estimate_a_priori(y) if a_priori is None else np.log(a_priori)\n","    # check that a_priori has the correct number of classes\n","    assert len(self.log_a_priori) == len(self.encoder.names), \"A priori probabilities do not match number of classes\"\n","    # now that everything else is in place, estimate all needed parameters for given model\n","    self._fit_params(X, y)\n","    # Q4: por que el _fit_params va al final? no se puede mover a, por ejemplo, antes de la priori?\n","\n","  def predict(self, X):\n","    \"\"\"\n","    Predice las etiquetas para las observaciones en X.\n","    paso a paso:\n","    Inicializa un array vacío y_hat para almacenar las predicciones.\n","    Itera sobre cada observación en X, predice su clase usando _predict_one,\n","    y almacena el resultado decodificado en y_hat.\n","    Devuelve y_hat como un vector fila.\n","    \"\"\"\n","    # this is actually an individual prediction encased in a for-loop\n","    m_obs = X.shape[1]\n","    y_hat = np.empty(m_obs, dtype=self.encoder.fmt)\n","    for i in range(m_obs):\n","      encoded_y_hat_i = self._predict_one(X[:,i].reshape(-1,1))\n","      y_hat[i] = self.encoder.names[encoded_y_hat_i]\n","    # return prediction as a row vector (matching y)\n","    return y_hat.reshape(1,-1)\n","\n","  def _predict_one(self, x):\n","    \"\"\"\n","    Predice la clase para una única observación x.\n","    paso a paso:\n","    Calcula la probabilidad a posteriori logarítmica para cada clase sumando\n","    la probabilidad a priori logarítmica y la probabilidad condicional logarítmica.\n","    Devuelve el índice de la clase con la máxima probabilidad a posteriori.\n","    \"\"\"\n","    # calculate all log posteriori probabilities (actually, +C)\n","    log_posteriori = [ log_a_priori_i + self._predict_log_conditional(x, idx) for idx, log_a_priori_i\n","                  in enumerate(self.log_a_priori) ]\n","\n","    # return the class that has maximum a posteriori probability\n","    return np.argmax(log_posteriori)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5rEspKtU98IM"},"outputs":[],"source":["class QDA(BaseBayesianClassifier):\n","  \"\"\"\n","  Clasifica los datos basándose en modelos Gaussianos con diferentes matrices\n","  de covarianza para cada clase.\n","  \"\"\"\n","\n","  def _fit_params(self, X, y):\n","    \"\"\"\n","    flatten, se asegura que y sea un array unidimensional, lo cual es necesario\n","    para hacer comparaciones elementales como y == idx. Sin flatten,\n","    y == idx puede no funcionar correctamente si y tiene más de una dimensión.\n","    bias=True en np.cov ajusta el cálculo de la matriz de covarianza dividiendo\n","    por N (número de observaciones) en lugar de N-1. Esto proporciona una\n","    estimación de la covarianza basada en la población en lugar de la muestra.\n","    X es una matriz donde las filas representan características y las columnas\n","    representan observaciones. Calcular la media a lo largo de las columnas\n","    (axis=1) proporciona la media de cada característica para cada clase.\n","    \"\"\"\n","    # estimate each covariance matrix\n","    self.inv_covs = [inv(np.cov(X[:,y.flatten()==idx], bias=True))\n","                      for idx in range(len(self.log_a_priori))]\n","    # Q5: por que hace falta el flatten y no se puede directamente X[:,y==idx]?\n","    # Q6: por que se usa bias=True en vez del default bias=False?\n","    self.means = [X[:,y.flatten()==idx].mean(axis=1, keepdims=True)\n","                  for idx in range(len(self.log_a_priori))]\n","    # Q7: que hace axis=1? por que no axis=0?\n","\n","  def _predict_log_conditional(self, x, class_idx):\n","    \"\"\"\n","    Predice el logaritmo de la probabilidad condicional de x dado class_idx.\n","    paso a paso:\n","    Obtiene la matriz de covarianza inversa para la clase class_idx.\n","    Calcula unbiased_x restando la media de la clase de x.\n","    Calcula el logaritmo de la probabilidad condicional usando la fórmula del\n","    modelo Gaussiano cuadrático.\n","    \"\"\"\n","    # predict the log(P(x|G=class_idx)), the log of the conditional probability of x given the class\n","    # this should depend on the model used\n","    inv_cov = self.inv_covs[class_idx]\n","    unbiased_x =  x - self.means[class_idx]\n","    return 0.5*np.log(det(inv_cov)) -0.5 * unbiased_x.T @ inv_cov @ unbiased_x"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Oygbi91p98IM","executionInfo":{"status":"ok","timestamp":1717801844865,"user_tz":180,"elapsed":3,"user":{"displayName":"Santi Olaciregui","userId":"08004037901086979441"}},"outputId":"ce5f218a-c87a-4e47-b34e-e72078ed3ada"},"outputs":[{"output_type":"stream","name":"stdout","text":["[0.05 0.05 0.9 ]\n","Train (apparent) error is 0.0190 while test error is 0.1111\n"]}],"source":["qda = QDA()\n","qda.fit(train_x, train_y)\n","\n","train_acc = accuracy(train_y, qda.predict(train_x))\n","test_acc = accuracy(test_y, qda.predict(test_x))\n","print(f\"Train (apparent) error is {1-train_acc:.4f} while test error is {1-test_acc:.4f}\")"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.8"}},"nbformat":4,"nbformat_minor":0}